import argparse
import random
import pandas as pd
import warnings
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import re
import os
import json
from llm_as_a_judge import judge
from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer
from model.model_minimind import MiniMindConfig, MiniMindForCausalLM
from model.model_lora import *
from model.model_adapter import *
from trainer.train_projection_head import projectionhead, ClsTunedModel
from trainer.train_prompt_tuning import PromptTuningModel
from trainer.trainer_utils import setup_seed
from torch.utils.data import Dataset
from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction
warnings.filterwarnings('ignore')

def init_model(args):
    tokenizer = AutoTokenizer.from_pretrained(args.load_from) 
    if 'model' in args.load_from:
        model = MiniMindForCausalLM(MiniMindConfig(
            hidden_size=args.hidden_size,
            num_hidden_layers=args.num_hidden_layers,
            use_moe=bool(args.use_moe),
            inference_rope_scaling=args.inference_rope_scaling
        ))
        moe_suffix = '_moe' if args.use_moe else ''
        ckp = f'./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth'
        
        # å¦‚æœä½¿ç”¨projection headï¼Œéœ€è¦åŠ è½½å¸¦<cls> embeddingçš„æ¨¡å‹ï¼ˆvocab_size + 1ï¼‰
        if args.projectionhead:
            # æ‰©å±•embeddingå±‚ä»¥å®¹çº³<cls> token
            original_vocab_size = len(tokenizer)
            model.resize_token_embeddings(original_vocab_size + 1)
            print(f'æ¨¡å‹embeddingå·²æ‰©å±•ä»¥å®¹çº³<cls> token: {original_vocab_size} -> {original_vocab_size + 1}')
        
        model.load_state_dict(torch.load(ckp, map_location=args.device), strict=True)
        
        if args.lora_weight != 'None':
            apply_lora(model, rank=args.rank)
            load_lora(model, f'./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth')
        
        if args.adapter_weight != 'None':
            apply_adapter(model, middle_features=args.middle_features)
            load_adapter(model, f'./{args.save_dir}/adapter/{args.adapter_weight}_{args.hidden_size}.pth')
        
    else:
        model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)
    
    print(f'MiniMindæ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)')
    return model.eval().to(args.device), tokenizer

def main():
    parser = argparse.ArgumentParser(description="MiniMindæ¨¡å‹æ¨ç†ä¸å¯¹è¯")
    parser.add_argument('--load_from', default='model', type=str, help="æ¨¡å‹åŠ è½½è·¯å¾„ï¼ˆmodel=åŸç”Ÿtorchæƒé‡ï¼Œå…¶ä»–è·¯å¾„=transformersæ ¼å¼ï¼‰")
    parser.add_argument('--save_dir', default='out', type=str, help="æ¨¡å‹æƒé‡ç›®å½•")
    parser.add_argument('--weight', default='full_sft', type=str, help="æƒé‡åç§°å‰ç¼€ï¼ˆpretrain, full_sft, rlhf, reason, ppo_actor, grpo, spoï¼‰")
    parser.add_argument('--lora_weight', default='None', type=str, help="LoRAæƒé‡åç§°ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨ï¼Œå¯é€‰ï¼šlora_identity, lora_medicalï¼‰")
    parser.add_argument('--adapter_weight', default='None', type=str, help="Adapteræƒé‡åç§°ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨ï¼Œå¯é€‰ï¼šadapter_identity, adapter_medicalï¼‰")
    parser.add_argument('--hidden_size', default=512, type=int, help="éšè—å±‚ç»´åº¦ï¼ˆ512=Small-26M, 640=MoE-145M, 768=Base-104Mï¼‰")
    parser.add_argument('--num_hidden_layers', default=8, type=int, help="éšè—å±‚æ•°é‡ï¼ˆSmall/MoE=8, Base=16ï¼‰")
    parser.add_argument('--use_moe', default=0, type=int, choices=[0, 1], help="æ˜¯å¦ä½¿ç”¨MoEæ¶æ„ï¼ˆ0=å¦ï¼Œ1=æ˜¯ï¼‰")
    parser.add_argument('--inference_rope_scaling', default=False, action='store_true', help="å¯ç”¨RoPEä½ç½®ç¼–ç å¤–æ¨ï¼ˆ4å€ï¼Œä»…è§£å†³ä½ç½®ç¼–ç é—®é¢˜ï¼‰")
    parser.add_argument('--max_new_tokens', default=8192, type=int, help="æœ€å¤§ç”Ÿæˆé•¿åº¦ï¼ˆæ³¨æ„ï¼šå¹¶éæ¨¡å‹å®é™…é•¿æ–‡æœ¬èƒ½åŠ›ï¼‰")
    parser.add_argument('--temperature', default=0.85, type=float, help="ç”Ÿæˆæ¸©åº¦ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆ0-1ï¼Œè¶Šå¤§è¶Šéšæœºï¼‰")
    parser.add_argument('--top_p', default=0.85, type=float, help="nucleusé‡‡æ ·é˜ˆå€¼ï¼ˆ0-1ï¼‰")
    parser.add_argument('--historys', default=0, type=int, help="æºå¸¦å†å²å¯¹è¯è½®æ•°ï¼ˆéœ€ä¸ºå¶æ•°ï¼Œ0è¡¨ç¤ºä¸æºå¸¦å†å²ï¼‰")
    parser.add_argument('--device', default='cuda' if torch.cuda.is_available() else 'cpu', type=str, help="è¿è¡Œè®¾å¤‡")
    parser.add_argument('--prompt_tuning', default=0, type=int, help="ä½¿ç”¨prompt tuningè¿›è¡Œåˆ†ç±»")
    parser.add_argument('--projectionhead', default=False, action='store_true', help="ä½¿ç”¨projection headè¿›è¡Œåˆ†ç±»")
    parser.add_argument('--projectionhead_cls_tuning', default=False, action='store_true', help="ä½¿ç”¨tuned cls embedding+projection headè¿›è¡Œåˆ†ç±»")
    parser.add_argument('--cls_tuning_weight', default='cls_tuning_projection_head_classifier', type=str, help="cls tuningæƒé‡åç§°ï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨ï¼Œå¯é€‰ï¼šfull_sft, rlhf, reason, ppo_actor, grpo, spoï¼‰")
    parser.add_argument('--rank', default=8, type=int, help="loraçš„ç§©")
    parser.add_argument("--middle_features", type=int, default=8, help="ä¸­é—´å±‚ç‰¹å¾ç»´åº¦")
    parser.add_argument('--llm_as_a_judge', default=False, action='store_true', help="ä½¿ç”¨llmä½œä¸ºæ³•å®˜è¿›è¡Œåˆ†ç±»å‡†ç¡®æ€§è¯„ä¼°")
    parser.add_argument('--deepseek_api_key', default='None', type=str, help="deepseek api keyï¼ˆNoneè¡¨ç¤ºä¸ä½¿ç”¨ï¼‰")
    # TODO: add other llms for llm_as_a_judge
    args = parser.parse_args()
    if args.llm_as_a_judge:
        if args.deepseek_api_key != 'None':
            os.environ["DEEPSEEK_API_KEY"] = args.deepseek_api_key
        else:
            raise ValueError('deepseek_api_key is required when llm_as_a_judge is True')
    # prompts = [
    #     'ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ',
    #     'ä¸ºä»€ä¹ˆå¤©ç©ºæ˜¯è“è‰²çš„',
    #     'è¯·ç”¨Pythonå†™ä¸€ä¸ªè®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—çš„å‡½æ•°',
    #     'è§£é‡Šä¸€ä¸‹"å…‰åˆä½œç”¨"çš„åŸºæœ¬è¿‡ç¨‹',
    #     'å¦‚æœæ˜å¤©ä¸‹é›¨ï¼Œæˆ‘åº”è¯¥å¦‚ä½•å‡ºé—¨',
    #     'æ¯”è¾ƒä¸€ä¸‹çŒ«å’Œç‹—ä½œä¸ºå® ç‰©çš„ä¼˜ç¼ºç‚¹',
    #     'è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ',
    #     'æ¨èä¸€äº›ä¸­å›½çš„ç¾é£Ÿ'
    # ]
    # ç”¨äºè®¡ç®—perplexityï¼ˆä»…åœ¨ä¸ä½¿ç”¨projection headæ—¶ï¼‰
    total_nll = 0.0  # æ€»è´Ÿå¯¹æ•°ä¼¼ç„¶
    total_tokens = 0  # æ€»tokenæ•°
    conversation = []
    model, tokenizer = init_model(args)
    moe_suffix = '_moe' if args.use_moe else ''

    # å¦‚æœä½¿ç”¨projection headï¼Œéœ€è¦åŠ è½½å®ƒå¹¶æ„å»ºç±»åˆ«æ˜ å°„
    label2id = None
    id2label = None

    # input_mode = int(input('[0] è‡ªåŠ¨æµ‹è¯•\n[1] æ‰‹åŠ¨è¾“å…¥\n'))
    input_mode=0
    streamer = TextStreamer(tokenizer, skip_prompt=True, skip_special_tokens=True)
    # è¯»å–jsonlæµ‹è¯•æ–‡ä»¶
    import json
    test_prompts = []
    test_labels = []
    categories = set()
    
    with open('dataset/bbc_news_test.jsonl', 'r', encoding='utf-8') as f:
        for line in f:
            data = json.loads(line.strip())
            conversations = data['conversations']
            # æå–userçš„contentä½œä¸ºpromptï¼Œassistantçš„contentä½œä¸ºlabel
            if len(conversations) >= 2:
                prompt = conversations[0]['content']  # userçš„content
                label = conversations[1]['content']   # assistantçš„content
                test_prompts.append(prompt)
                test_labels.append(label)
                categories.add(label)
    
    categories = sorted(list(categories))
    if args.projectionhead:
        # åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„label2idæ˜ å°„ï¼ˆç¡®ä¿ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        
        label2id_path = f'./{args.save_dir}/{args.weight}_label2id_{args.hidden_size}{moe_suffix}.json'
        
        if os.path.exists(label2id_path):
            with open(label2id_path, 'r', encoding='utf-8') as f:
                label2id = json.load(f)
            # å°†å­—ç¬¦ä¸²é”®è½¬æ¢ä¸ºæ•´æ•°ï¼ˆJSONä¿å­˜æ—¶é”®ä¼šè¢«è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
            label2id = {label: int(idx) for label, idx in label2id.items()}
            id2label = {idx: label for label, idx in label2id.items()}
            num_classes = len(label2id)
            print(f'å·²åŠ è½½è®­ç»ƒæ—¶çš„label2idæ˜ å°„: {label2id_path}')
            print(f'ç±»åˆ«æ˜ å°„: {label2id}')
        else:
            raise ValueError(f'æœªæ‰¾åˆ°è®­ç»ƒæ—¶çš„label2idæ˜ å°„: {label2id_path}')
        # åˆ›å»ºå¹¶åŠ è½½projection head
        projection_head = projectionhead(args.hidden_size, num_classes).to(args.device)
        ckp_head = f'./{args.save_dir}/{args.weight}_head_{args.hidden_size}{moe_suffix}.pth'
        projection_head.load_state_dict(torch.load(ckp_head, map_location=args.device))
        projection_head.eval()
        print(f'å·²åŠ è½½projection headï¼Œç±»åˆ«æ•°: {num_classes}')
    if args.projectionhead_cls_tuning:
        # åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„cls embedding
        cls_embedding_path = f'./{args.save_dir}/{args.cls_tuning_weight}_cls_{args.hidden_size}{moe_suffix}.pth'
        cls_embedding = torch.load(cls_embedding_path, map_location=args.device)
        cls_embedding = cls_embedding.to(args.device)
        print(f'å·²åŠ è½½cls embedding')
        model = ClsTunedModel(model, cls_embedding).to(args.device)
                # åŠ è½½è®­ç»ƒæ—¶ä¿å­˜çš„label2idæ˜ å°„ï¼ˆç¡®ä¿ä¸è®­ç»ƒæ—¶ä¸€è‡´ï¼‰
        label2id_path = f'./{args.save_dir}/{args.cls_tuning_weight}_label2id_{args.hidden_size}{moe_suffix}.json'
        
        if os.path.exists(label2id_path):
            with open(label2id_path, 'r', encoding='utf-8') as f:
                label2id = json.load(f)
            # å°†å­—ç¬¦ä¸²é”®è½¬æ¢ä¸ºæ•´æ•°ï¼ˆJSONä¿å­˜æ—¶é”®ä¼šè¢«è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼‰
            label2id = {label: int(idx) for label, idx in label2id.items()}
            id2label = {idx: label for label, idx in label2id.items()}
            num_classes = len(label2id)
            print(f'å·²åŠ è½½è®­ç»ƒæ—¶çš„label2idæ˜ å°„: {label2id_path}')
            print(f'ç±»åˆ«æ˜ å°„: {label2id}')
        else:
            raise ValueError(f'æœªæ‰¾åˆ°è®­ç»ƒæ—¶çš„label2idæ˜ å°„: {label2id_path}')
        # åˆ›å»ºå¹¶åŠ è½½projection head
        projection_head = projectionhead(args.hidden_size, num_classes).to(args.device)
        ckp_head = f'./{args.save_dir}/{args.cls_tuning_weight}_head_{args.hidden_size}{moe_suffix}.pth'
        projection_head.load_state_dict(torch.load(ckp_head, map_location=args.device))
        projection_head.eval()
        print(f'å·²åŠ è½½projection headï¼Œç±»åˆ«æ•°: {num_classes}')
    if args.prompt_tuning:
        # åŠ è½½ virtual_embeddingï¼ˆè®­ç»ƒæ—¶ä¿å­˜ä¸º half precisionï¼‰
        virtual_embedding = torch.load(f'./{args.save_dir}/{args.weight}_virtual_embedding_{args.prompt_tuning}{moe_suffix}.pth', map_location=args.device)
        # ç¡®ä¿ virtual_embedding çš„å½¢çŠ¶æ­£ç¡®
        if len(virtual_embedding.shape) != 2 or virtual_embedding.shape[0] != args.prompt_tuning:
            raise ValueError(f'virtual_embedding çš„å½¢çŠ¶é”™è¯¯ï¼šæœŸæœ› ({args.prompt_tuning}, {args.hidden_size})ï¼Œå¾—åˆ° {virtual_embedding.shape}')
        # è½¬æ¢ä¸º float32 ä»¥åŒ¹é…æ¨¡å‹æƒé‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        virtual_embedding = virtual_embedding.float().to(args.device)
        # å¦‚æœç”¨PromptTuningModelåŒ…è£…çš„è¯ï¼Œä¸‹é¢çš„ä»£ç å°±ä¸å…¼å®¹äº†ï¼Œè€Œä¸”è¿˜å¾—è‡ªå·±å†™generateæ–¹æ³•
        # HACK: æŠŠå‡ ä¸ªvirtual embeddingå‡è£…æˆçœŸå®å­˜åœ¨çš„token(<placeholder>)çš„embeddingï¼Œç„¶åç»™è¯­æ–™æœ€å‰é¢åŠ ä¸Šplaceholderï¼Œè¿™æ ·å°±èƒ½è®©æ¨¡å‹åœ¨ç”Ÿæˆæ—¶ä½¿ç”¨placeholder tokenï¼Œæ‰¾åˆ°å¯¹åº”çš„virtual embedding
        tokenizer.add_tokens([f'<placeholder_{i}>' for i in range(args.prompt_tuning)])
        model.resize_token_embeddings(len(tokenizer))
        
        # å°† virtual_embedding èµ‹å€¼ç»™æ–°æ·»åŠ çš„ placeholder tokens
        # æ³¨æ„ï¼šéœ€è¦ç¡®ä¿æƒé‡ç±»å‹åŒ¹é…ï¼ˆmodel æƒé‡å¯èƒ½æ˜¯ float32 æˆ– bfloat16ï¼‰
        with torch.no_grad():
            model.model.embed_tokens.weight[-args.prompt_tuning:, :] = virtual_embedding.to(model.model.embed_tokens.weight.dtype)
        
        print(f'å·²åŠ è½½prompt tuningï¼Œè™šæ‹Ÿtokenæ•°: {args.prompt_tuning}')
    class dataset(Dataset):
        def __init__(self, prompts, labels):
            self.prompt = prompts
            self.label = labels
        def __len__(self):
            return len(self.prompt)
    # for prompt in ['ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ']:
    #     setup_seed(2026) # or setup_seed(random.randint(0, 2048))
    #     if input_mode == 0: print(f'ğŸ‘¶: {prompt}')
    #     conversation = []
    #     conversation.append({"role": "user", "content": prompt})

    #     templates = {"conversation": conversation, "tokenize": False, "add_generation_prompt": True}
    #     inputs = tokenizer.apply_chat_template(**templates)
    #     inputs = tokenizer(inputs, return_tensors="pt", truncation=True).to(args.device)

    #     print('ğŸ¤–ï¸: ', end='')
    #     generated_ids = model.generate(
    #         inputs=inputs["input_ids"], attention_mask=inputs["attention_mask"],
    #         max_new_tokens=args.max_new_tokens, do_sample=True, streamer=streamer,
    #         pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,
    #         top_p=args.top_p, temperature=args.temperature, repetition_penalty=1.0
    #     )
    #     response = tokenizer.decode(generated_ids[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
    test_dataset = dataset(test_prompts, test_labels)
    prompts = test_dataset.prompt
    labels = test_dataset.label
    
    # è·å–æ‰€æœ‰ç±»åˆ«å¹¶è½¬æ¢ä¸ºtoken_id
    category_token_ids = {}
    for cat in categories:
        # å°†ç±»åˆ«åç§°è½¬æ¢ä¸ºtoken_idï¼ˆå–ç¬¬ä¸€ä¸ªtokenï¼‰
        tokens = tokenizer.encode(cat, add_special_tokens=False)
        category_token_ids[cat] = tokens[0] if tokens else None
    llm_as_a_judge_correct = 0
    correct = 0  # åŸºäºlogitsçš„å‡†ç¡®ç‡
    total = 0
    correct_gen = 0  # åŸºäºç”Ÿæˆçš„å‡†ç¡®ç‡
    total_gen = 0
    bleu_scores = []  # å­˜å‚¨æ‰€æœ‰BLEUåˆ†æ•°
    if input_mode == 0:
        # è‡ªåŠ¨æµ‹è¯•æ¨¡å¼
        for idx, prompt in enumerate(prompts):
            # prompt='ä½ æœ‰ä»€ä¹ˆç‰¹é•¿ï¼Ÿ'
            # print(f'ğŸ‘¶: {prompt}')
            # print(f'ğŸ‘¶: {prompt[:100]}...' if len(prompt) > 100 else f'ğŸ‘¶: {prompt}')
            conversation = conversation[-args.historys:] if args.historys else []
            conversation.append({"role": "user", "content": prompt})
            true_category = labels[idx] if idx < len(labels) else None
            if args.projectionhead:
                cls_token_id=len(tokenizer)
                input_ids = torch.cat([torch.tensor([cls_token_id],device=args.device), torch.tensor(tokenizer.encode(prompt,add_special_tokens=False),dtype=torch.long,device=args.device)]).unsqueeze(0)
                attention_mask = (input_ids != tokenizer.pad_token_id).long()
                outputs = model.model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    use_cache=False
                )
                logits=outputs[0][:,0,:]
                cls=projection_head(logits)
                predicted_category = torch.argmax(cls, dim=-1).item()
                if predicted_category == label2id[true_category]:
                    correct += 1
                total += 1
                continue
            if args.projectionhead_cls_tuning:
                placeholder_id=1
                input_ids = torch.cat([torch.tensor([placeholder_id],device=args.device), torch.tensor(tokenizer.encode(prompt,add_special_tokens=False),dtype=torch.long,device=args.device)]).unsqueeze(0)
                attention_mask = (input_ids != tokenizer.pad_token_id).long()
                outputs = model(
                    input_ids=input_ids,
                    attention_mask=attention_mask,
                    use_cache=False
                )
                hidden_states = outputs.hidden_states
                logits=hidden_states[:,0,:]
                cls=projection_head(logits)
                predicted_category = torch.argmax(cls, dim=-1).item()
                if predicted_category == label2id[true_category]:
                    correct += 1
                total += 1
                continue

            # è®¡ç®—perplexityï¼šä½¿ç”¨ground truthï¼ˆçœŸå®ç­”æ¡ˆï¼‰ä½œä¸ºä¸Šä¸‹æ–‡
            if true_category:
                # æ„å»ºå®Œæ•´å¯¹è¯åºåˆ—ï¼ˆprompt + çœŸå®çš„assistantå›å¤ï¼‰
                full_conversation = conversation + [{"role": "assistant", "content": true_category}]
                full_templates = {"conversation": full_conversation, "tokenize": False, "add_generation_prompt": False}
                full_text = tokenizer.apply_chat_template(**full_templates)
                if args.prompt_tuning:
                    placeholder=[f'<placeholder_{i}>' for i in range(args.prompt_tuning)]
                    full_text = ''.join(placeholder)+full_text
                full_inputs = tokenizer(full_text, return_tensors="pt", truncation=True, max_length=8192).to(args.device)
                full_input_ids = full_inputs['input_ids'][0]  # [seq_len]
                full_attention_mask = full_inputs['attention_mask'][0]  # [seq_len]
                
                # æŸ¥æ‰¾assistantå›å¤çš„èµ·å§‹ä½ç½®ï¼ˆé€šè¿‡æŸ¥æ‰¾<|im_start|>assistantæ ‡è®°ï¼‰
                bos_id = tokenizer(f'{tokenizer.bos_token}assistant', add_special_tokens=False).input_ids
                assistant_start_pos = None
                for i in range(len(full_input_ids) - len(bos_id) + 1):
                    if full_input_ids[i:i+len(bos_id)].tolist() == bos_id:
                        assistant_start_pos = i + len(bos_id)  # assistantå†…å®¹çš„èµ·å§‹ä½ç½®
                        break
                
                if assistant_start_pos is not None and assistant_start_pos < len(full_input_ids):
                    # å‰å‘ä¼ æ’­è·å–æ‰€æœ‰ä½ç½®çš„logits
                    with torch.no_grad():
                        full_logits = model(input_ids=full_input_ids.unsqueeze(0), 
                                           attention_mask=full_attention_mask.unsqueeze(0)).logits[0]  # [seq_len, vocab_size]
                    
                    # æå–assistantå›å¤éƒ¨åˆ†çš„tokenï¼ˆä»assistantå†…å®¹å¼€å§‹åˆ°åºåˆ—ç»“æŸï¼‰
                    assistant_tokens = full_input_ids[assistant_start_pos:]  # [assistant_len]
                    assistant_mask = full_attention_mask[assistant_start_pos:]  # [assistant_len]
                    
                    # å¦‚æœé‡åˆ°eos_tokenï¼Œåªè®¡ç®—åˆ°eos_tokenä¹‹å‰ï¼ˆåŒ…å«eos_tokenï¼‰
                    if tokenizer.eos_token_id in assistant_tokens:
                        eos_idx = (assistant_tokens == tokenizer.eos_token_id).nonzero(as_tuple=True)[0]
                        if len(eos_idx) > 0:
                            eos_pos = eos_idx[0].item() + 1  # åŒ…å«eos_token
                            assistant_tokens = assistant_tokens[:eos_pos]
                            assistant_mask = assistant_mask[:eos_pos]
                    
                    if len(assistant_tokens) > 0:
                        # è®¡ç®—assistantå›å¤éƒ¨åˆ†çš„perplexity
                        # logits[i] é¢„æµ‹ input_ids[i+1]
                        # æ‰€ä»¥ full_logits[assistant_start_pos-1] é¢„æµ‹ assistant_tokens[0]
                        #     full_logits[assistant_start_pos] é¢„æµ‹ assistant_tokens[1]
                        #     ...
                        logits_start_idx = assistant_start_pos - 1  # ç”¨äºé¢„æµ‹assistantç¬¬ä¸€ä¸ªtokençš„logitsä½ç½®
                        logits_end_idx = logits_start_idx + len(assistant_tokens)  # æœ€åä¸€ä¸ªé¢„æµ‹ä½ç½®
                        
                        # è·å–assistantå›å¤éƒ¨åˆ†çš„logits
                        assistant_logits = full_logits[logits_start_idx:logits_end_idx, :]  # [assistant_len, vocab_size]
                        assistant_labels = assistant_tokens  # [assistant_len]
                        
                        # è®¡ç®—æ¯ä¸ªtokençš„è´Ÿå¯¹æ•°ä¼¼ç„¶
                        log_probs = torch.log_softmax(assistant_logits[:,:len(tokenizer)-args.prompt_tuning], dim=-1)  # [assistant_len, vocab_size]
                        nll = -log_probs.gather(1, assistant_labels.unsqueeze(1)).squeeze(1)  # [assistant_len]
                        
                        # åªè®¡ç®—æœ‰æ•ˆä½ç½®ï¼ˆépaddingï¼‰
                        valid_nll = nll * assistant_mask.float()
                        total_nll += valid_nll.sum().item()
                        total_tokens += assistant_mask.sum().item()
            
            # ç”¨äºåˆ†ç±»é¢„æµ‹ï¼šåªéœ€è¦promptéƒ¨åˆ†
            templates = {"conversation": conversation, "tokenize": False, "add_generation_prompt": True}
            inputs = tokenizer.apply_chat_template(**templates)
            if args.prompt_tuning:
                placeholder=[f'<placeholder_{i}>' for i in range(args.prompt_tuning)]
                inputs = ''.join(placeholder)+inputs
            # print(f'inputs: {inputs}')
            # print(f'inputs: {inputs}')
            # print(true_category)
            # raise Exception('stop')
            inputs = tokenizer(inputs, return_tensors="pt", truncation=True).to(args.device)
            logits = model(input_ids=inputs['input_ids'],attention_mask=inputs['attention_mask']).logits
            
            # å–æœ€åä¸€ä¸ªä½ç½®çš„logitsç”¨äºåˆ†ç±»
            last_logits = logits[0, -1, :len(tokenizer)-args.prompt_tuning]  # [vocab_size]
            last_prob = torch.softmax(last_logits, dim=-1)
            # è·å–æ¯ä¸ªç±»åˆ«å¯¹åº”çš„åˆ†æ•°
            category_scores = {}
            for cat, token_id in category_token_ids.items():
                if token_id is not None:
                    category_scores[cat] = last_prob[token_id].item()
            
            # é€‰æ‹©åˆ†æ•°æœ€é«˜çš„ç±»åˆ«ä½œä¸ºé¢„æµ‹
            predicted_category = max(category_scores, key=category_scores.get)
            
            # print(f'é¢„æµ‹ç±»åˆ«: {predicted_category} (åˆ†æ•°: {category_scores[predicted_category]:.2f})')
            if true_category:
                # print(f'çœŸå®ç±»åˆ«: {true_category}')
                if predicted_category == true_category:
                    correct += 1
                #     print('âœ“ æ­£ç¡®')
                # else:
                #     print('âœ— é”™è¯¯')
                total += 1
            
            # æ–¹æ³•2ï¼šåŸºäºç”Ÿæˆçš„å‡†ç¡®ç‡ï¼ˆä½¿ç”¨æ­£åˆ™åŒ¹é…ï¼‰
            if true_category:
                # print(f'attention_mask: {inputs['attention_mask']}')
                # è®©æ¨¡å‹ç”Ÿæˆå®Œæ•´ç­”æ¡ˆ
                with torch.no_grad():
                    generated_ids = model.generate(
                        repetition_penalty=1.0,
                        input_ids=inputs['input_ids'],
                        attention_mask=inputs['attention_mask'],
                        max_new_tokens=50,  # é™åˆ¶ç”Ÿæˆé•¿åº¦
                        do_sample=True,  # ä½¿ç”¨é‡‡æ ·
                        temperature=args.temperature,
                        top_p=args.top_p,
                        pad_token_id=tokenizer.pad_token_id,
                        eos_token_id=tokenizer.eos_token_id,
                        bad_words_ids=[tokenizer.convert_tokens_to_ids([f'<placeholder_{i}>']) for i in range(args.prompt_tuning)] if args.prompt_tuning else None
                    )
                
                # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆä¸åŒ…æ‹¬promptï¼‰
                prompt_length = inputs['input_ids'].shape[1]
                generated_tokens = generated_ids[0, prompt_length:]
                generated_text = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()
                # print(f'generated_text: {generated_text}')
                # raise Exception('stop')
                # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ï¼Œæ£€æŸ¥ç”Ÿæˆçš„æ–‡æœ¬ä¸­æ˜¯å¦åŒ…å«çœŸå®ç±»åˆ«
                # æ„å»ºæ­£åˆ™è¡¨è¾¾å¼ï¼šåŒ¹é…ç±»åˆ«åç§°ï¼ˆä¸åŒºåˆ†å¤§å°å†™ï¼Œå•è¯è¾¹ç•Œï¼‰
                if args.llm_as_a_judge:
                    result = judge(prompt, generated_text, true_category, categories, true_category)
                    if result['correctness']:
                        llm_as_a_judge_correct += 1
                pattern = r'\b' + re.escape(true_category) + r'\b'
                if re.search(pattern, generated_text, re.IGNORECASE):
                    correct_gen += 1
                total_gen += 1
                
                # è®¡ç®—BLEUåˆ†æ•°
                # å°†å‚è€ƒç­”æ¡ˆå’Œç”Ÿæˆç­”æ¡ˆè½¬æ¢ä¸ºtokenåˆ—è¡¨ï¼ˆæŒ‰å•è¯åˆ†å‰²ï¼‰
                reference = [true_category.lower().split()]  # BLEUéœ€è¦åˆ—è¡¨çš„åˆ—è¡¨
                candidate = generated_text.lower().split()
                
                # ä½¿ç”¨å¹³æ»‘å‡½æ•°é¿å…0åˆ†ï¼ˆå½“n-gramä¸åŒ¹é…æ—¶ï¼‰
                smoothing = SmoothingFunction().method1
                bleu_score = sentence_bleu(reference, candidate, smoothing_function=smoothing)
                bleu_scores.append(bleu_score)
            if idx%100==0:
                print(idx)
    # è®¡ç®—perplexity
    # print(f'total_nll: {total_nll}, total_tokens: {total_tokens}')
    if args.projectionhead or args.projectionhead_cls_tuning:
        print(f'å‡†ç¡®ç‡ï¼ˆåŸºäºprojection headï¼‰: {correct}/{total} = {correct/total*100:.2f}%')
        exit()
    if total_tokens > 0:
        avg_nll = total_nll / total_tokens
        perplexity = torch.exp(torch.tensor(avg_nll)).item()
        print(f'Perplexity: {perplexity:.4f} (åŸºäº {total_tokens} ä¸ªtokens)')
    
    if total > 0:
        accuracy = correct / total * 100
        print(f'å‡†ç¡®ç‡ï¼ˆåŸºäºlogitsï¼‰: {correct}/{total} = {accuracy:.2f}%')
    
    if total_gen > 0:
        accuracy_gen = correct_gen / total_gen * 100
        print(f'å‡†ç¡®ç‡ï¼ˆåŸºäºç”Ÿæˆ+æ­£åˆ™åŒ¹é…ï¼‰: {correct_gen}/{total_gen} = {accuracy_gen:.2f}%')
    
    if len(bleu_scores) > 0:
        avg_bleu = np.mean(bleu_scores)
        print(f'BLEUåˆ†æ•°: {avg_bleu:.4f} (åŸºäº {len(bleu_scores)} ä¸ªæ ·æœ¬)')
    if args.llm_as_a_judge:
        print(f'LLMä½œä¸ºæ³•å®˜å‡†ç¡®ç‡: {llm_as_a_judge_correct}/{total} = {llm_as_a_judge_correct/total*100:.2f}%')
    # for prompt in prompt_iter:
    #     setup_seed(2026) # or setup_seed(random.randint(0, 2048))
    #     if input_mode == 0: print(f'ğŸ‘¶: {prompt}')
    #     conversation = conversation[-args.historys:] if args.historys else []
    #     conversation.append({"role": "user", "content": prompt})

    #     templates = {"conversation": conversation, "tokenize": False, "add_generation_prompt": True}
    #     if args.weight == 'reason': templates["enable_thinking"] = True # ä»…Reasonæ¨¡å‹ä½¿ç”¨
    #     inputs = tokenizer.apply_chat_template(**templates) if args.weight != 'pretrain' else (tokenizer.bos_token + prompt)
    #     inputs = tokenizer(inputs, return_tensors="pt", truncation=True).to(args.device)

    #     print('ğŸ¤–ï¸: ', end='')
    #     generated_ids = model.generate(
    #         inputs=inputs["input_ids"], attention_mask=inputs["attention_mask"],
    #         max_new_tokens=args.max_new_tokens, do_sample=True, streamer=streamer,
    #         pad_token_id=tokenizer.pad_token_id, eos_token_id=tokenizer.eos_token_id,
    #         top_p=args.top_p, temperature=args.temperature, repetition_penalty=1.0
    #     )
    #     response = tokenizer.decode(generated_ids[0][len(inputs["input_ids"][0]):], skip_special_tokens=True)
    #     conversation.append({"role": "assistant", "content": response})
    #     print('\n\n')

if __name__ == "__main__":
    main()