{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9601a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "\n",
      "Total links: 0\n",
      "Extracting data...\n",
      "\n",
      "\n",
      "Done! Saved to dataset/crec_2025.jsonl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "from urllib.parse import urljoin\n",
    "import os\n",
    "import re\n",
    "\n",
    "BASE_URL = \"https://www.govinfo.gov\"\n",
    "COLLECTION_URL = f\"{BASE_URL}/app/collection/crec\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "\n",
    "def get_issue_links(page_url):\n",
    "    try:\n",
    "        response = requests.get(page_url, headers=HEADERS, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        links = set()\n",
    "        for a in soup.find_all('a', href=True):\n",
    "            href = a['href']\n",
    "            if '/details/CREC-2025' in href or '/app/details/CREC-2025' in href:\n",
    "                full_url = urljoin(BASE_URL, href)\n",
    "                links.add(full_url)\n",
    "        return list(links)\n",
    "    except Exception as e:\n",
    "        print(f\"Error fetching {page_url}: {e}\")\n",
    "        return []\n",
    "\n",
    "def extract_issue_data(issue_url):\n",
    "    try:\n",
    "        response = requests.get(issue_url, headers=HEADERS, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        data = {'url': issue_url, 'title': '', 'date': '', 'content': '', 'metadata': {}}\n",
    "        \n",
    "        title = soup.find('h1') or soup.find('title')\n",
    "        if title:\n",
    "            data['title'] = title.get_text(strip=True)\n",
    "        \n",
    "        date = soup.find('time') or soup.find('span', class_=re.compile('date', re.I))\n",
    "        if date:\n",
    "            data['date'] = date.get_text(strip=True)\n",
    "        elif soup.find('meta', property='article:published_time'):\n",
    "            data['date'] = soup.find('meta', property='article:published_time')['content']\n",
    "        \n",
    "        content = soup.find('div', class_=re.compile('content|text|body', re.I)) or soup.find('div', id=re.compile('content|text|body', re.I))\n",
    "        if content:\n",
    "            for script in content(['script', 'style', 'nav', 'footer', 'header']):\n",
    "                script.decompose()\n",
    "            data['content'] = content.get_text(separator=' ', strip=True)\n",
    "        else:\n",
    "            main = soup.find('main') or soup.find('article')\n",
    "            if main:\n",
    "                for script in main(['script', 'style', 'nav', 'footer', 'header']):\n",
    "                    script.decompose()\n",
    "                data['content'] = main.get_text(separator=' ', strip=True)\n",
    "        \n",
    "        for meta in soup.find_all('meta'):\n",
    "            name = meta.get('name') or meta.get('property', '')\n",
    "            content = meta.get('content', '')\n",
    "            if name and content:\n",
    "                data['metadata'][name] = content\n",
    "        \n",
    "        return data\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting {issue_url}: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_crec_2025(output_file='dataset/crec_2025.jsonl', max_pages=None, delay=0.5):\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    all_links = []\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        if max_pages and page > max_pages:\n",
    "            break\n",
    "        \n",
    "        page_url = f\"{COLLECTION_URL}?pageSize=100&page={page}\"\n",
    "        print(f\"Fetching page {page}...\")\n",
    "        \n",
    "        links = get_issue_links(page_url)\n",
    "        if not links:\n",
    "            break\n",
    "        \n",
    "        all_links.extend(links)\n",
    "        print(f\"Found {len(links)} links, total: {len(all_links)}\")\n",
    "        \n",
    "        if len(links) < 100:\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        time.sleep(1)\n",
    "    \n",
    "    print(f\"\\nTotal links: {len(all_links)}\")\n",
    "    print(\"Extracting data...\\n\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for i, link in enumerate(all_links, 1):\n",
    "            print(f\"[{i}/{len(all_links)}] {link}\")\n",
    "            data = extract_issue_data(link)\n",
    "            if data:\n",
    "                f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "            time.sleep(delay)\n",
    "    \n",
    "    print(f\"\\nDone! Saved to {output_file}\")\n",
    "\n",
    "crawl_crec_2025()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4e17bd83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching page 1...\n",
      "Found 767 papers, total: 767\n",
      "\n",
      "Total papers: 767\n",
      "Saving to dataset/arxiv_cs_new.jsonl...\n",
      "\n",
      "Done! Saved 767 papers to dataset/arxiv_cs_new.jsonl\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "\n",
    "ARXIV_BASE = \"https://arxiv.org\"\n",
    "ARXIV_LIST_URL = f\"{ARXIV_BASE}/list/cs/new\"\n",
    "HEADERS = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36\"}\n",
    "\n",
    "def extract_paper_info(dt, dd):\n",
    "    try:\n",
    "        arxiv_id = dt.find('a', href=re.compile(r'/abs/'))['href'].split('/')[-1]\n",
    "        title_elem = dd.find('div', class_='list-title')\n",
    "        title = title_elem.get_text(strip=True).replace('Title:', '').strip() if title_elem else ''\n",
    "        \n",
    "        authors_elem = dd.find('div', class_='list-authors')\n",
    "        authors = []\n",
    "        if authors_elem:\n",
    "            for a in authors_elem.find_all('a'):\n",
    "                authors.append(a.get_text(strip=True))\n",
    "        \n",
    "        subjects_elem = dd.find('div', class_='list-subjects')\n",
    "        subjects = []\n",
    "        if subjects_elem:\n",
    "            subjects_text = subjects_elem.get_text(strip=True).replace('Subjects:', '').strip()\n",
    "            subjects = [s.strip() for s in subjects_text.split(';') if s.strip()]\n",
    "        \n",
    "        abstract_elem = dd.find('p', class_='mathjax')\n",
    "        abstract = abstract_elem.get_text(strip=True) if abstract_elem else ''\n",
    "        \n",
    "        comments_elem = dd.find('div', class_='list-comments')\n",
    "        comments = comments_elem.get_text(strip=True).replace('Comments:', '').strip() if comments_elem else ''\n",
    "        \n",
    "        return {\n",
    "            'arxiv_id': arxiv_id,\n",
    "            'title': title,\n",
    "            'authors': authors,\n",
    "            'subjects': subjects,\n",
    "            'abstract': abstract,\n",
    "            'comments': comments,\n",
    "            'url': f\"{ARXIV_BASE}/abs/{arxiv_id}\"\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting paper info: {e}\")\n",
    "        return None\n",
    "\n",
    "def crawl_arxiv_cs_new(output_file='dataset/arxiv_cs_new.jsonl', max_pages=None, delay=1):\n",
    "    os.makedirs(os.path.dirname(output_file) if os.path.dirname(output_file) else '.', exist_ok=True)\n",
    "    \n",
    "    all_papers = []\n",
    "    page = 0\n",
    "    \n",
    "    while True:\n",
    "        if max_pages and page >= max_pages:\n",
    "            break\n",
    "        \n",
    "        if page == 0:\n",
    "            url = ARXIV_LIST_URL\n",
    "        else:\n",
    "            url = f\"{ARXIV_LIST_URL}?skip={page * 2000}&show=2000\"\n",
    "        \n",
    "        print(f\"Fetching page {page + 1}...\")\n",
    "        \n",
    "        try:\n",
    "            response = requests.get(url, headers=HEADERS, timeout=30)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            \n",
    "            dts = soup.find_all('dt')\n",
    "            if not dts:\n",
    "                break\n",
    "            \n",
    "            papers_on_page = []\n",
    "            for dt in dts:\n",
    "                dd = dt.find_next_sibling('dd')\n",
    "                if dd:\n",
    "                    paper = extract_paper_info(dt, dd)\n",
    "                    if paper:\n",
    "                        papers_on_page.append(paper)\n",
    "            \n",
    "            if not papers_on_page:\n",
    "                break\n",
    "            \n",
    "            all_papers.extend(papers_on_page)\n",
    "            print(f\"Found {len(papers_on_page)} papers, total: {len(all_papers)}\")\n",
    "            \n",
    "            if len(papers_on_page) < 2000:\n",
    "                break\n",
    "            \n",
    "            page += 1\n",
    "            time.sleep(delay)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error fetching page {page + 1}: {e}\")\n",
    "            break\n",
    "    \n",
    "    print(f\"\\nTotal papers: {len(all_papers)}\")\n",
    "    print(f\"Saving to {output_file}...\\n\")\n",
    "    \n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for i, paper in enumerate(all_papers, 1):\n",
    "            text = f\"Title: {paper['title']}\\n\"\n",
    "            text += f\"Authors: {', '.join(paper['authors'])}\\n\"\n",
    "            text += f\"Subjects: {'; '.join(paper['subjects'])}\\n\"\n",
    "            if paper['comments']:\n",
    "                text += f\"Comments: {paper['comments']}\\n\"\n",
    "            text += f\"Abstract: {paper['abstract']}\\n\"\n",
    "            \n",
    "            data = {\n",
    "                'text': text.strip(),\n",
    "                'arxiv_id': paper['arxiv_id'],\n",
    "                'title': paper['title'],\n",
    "                'authors': paper['authors'],\n",
    "                'subjects': paper['subjects'],\n",
    "                'abstract': paper['abstract'],\n",
    "                'url': paper['url']\n",
    "            }\n",
    "            \n",
    "            f.write(json.dumps(data, ensure_ascii=False) + '\\n')\n",
    "    \n",
    "    print(f\"Done! Saved {len(all_papers)} papers to {output_file}\")\n",
    "\n",
    "crawl_arxiv_cs_new()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312pt291cu128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
