{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37dd667c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from model.model_lora import *\n",
    "from trainer.trainer_utils import setup_seed\n",
    "from torch.utils.data import Dataset\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "173d39e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniMind模型参数: 25.96 M(illion)\n"
     ]
    }
   ],
   "source": [
    "args = argparse.Namespace(\n",
    "        load_from='model',\n",
    "        save_dir='out',\n",
    "        weight='full_sft',\n",
    "        lora_weight='lora_classifier',\n",
    "        hidden_size=512,\n",
    "        num_hidden_layers=8,\n",
    "        use_moe=0,\n",
    "        inference_rope_scaling=False,\n",
    "        max_new_tokens=8192,\n",
    "        temperature=0.85,\n",
    "        top_p=0.85,\n",
    "        historys=0,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "def init_model(args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.load_from)\n",
    "    if 'model' in args.load_from:\n",
    "        model = MiniMindForCausalLM(MiniMindConfig(\n",
    "            hidden_size=args.hidden_size,\n",
    "            num_hidden_layers=args.num_hidden_layers,\n",
    "            use_moe=bool(args.use_moe),\n",
    "            inference_rope_scaling=args.inference_rope_scaling\n",
    "        ))\n",
    "        moe_suffix = '_moe' if args.use_moe else ''\n",
    "        ckp = f'./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth'\n",
    "        model.load_state_dict(torch.load(ckp, map_location=args.device), strict=True)\n",
    "        if args.lora_weight != 'None':\n",
    "            apply_lora(model)\n",
    "            load_lora(model, f'./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth')\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)\n",
    "    print(f'MiniMind模型参数: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)')\n",
    "    return model.eval().to(args.device), tokenizer\n",
    "model, tokenizer = init_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa4fdd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "0\n",
      " MiniMindForCausalLM(\n",
      "  (model): MiniMindModel(\n",
      "    (embed_tokens): Embedding(6400, 512)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (layers): ModuleList(\n",
      "      (0-7): 8 x MiniMindBlock(\n",
      "        (self_attn): Attention(\n",
      "          (q_proj): Linear(\n",
      "            in_features=512, out_features=512, bias=False\n",
      "            (lora): LoRA(\n",
      "              (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "              (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "          (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "          (o_proj): Linear(\n",
      "            in_features=512, out_features=512, bias=False\n",
      "            (lora): LoRA(\n",
      "              (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "              (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "            )\n",
      "          )\n",
      "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "        )\n",
      "        (input_layernorm): RMSNorm()\n",
      "        (post_attention_layernorm): RMSNorm()\n",
      "        (mlp): FeedForward(\n",
      "          (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "          (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "          (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "          (dropout): Dropout(p=0.0, inplace=False)\n",
      "          (act_fn): SiLUActivation()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (norm): RMSNorm()\n",
      "  )\n",
      "  (lm_head): Linear(in_features=512, out_features=6400, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "1\n",
      "model MiniMindModel(\n",
      "  (embed_tokens): Embedding(6400, 512)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (layers): ModuleList(\n",
      "    (0-7): 8 x MiniMindBlock(\n",
      "      (self_attn): Attention(\n",
      "        (q_proj): Linear(\n",
      "          in_features=512, out_features=512, bias=False\n",
      "          (lora): LoRA(\n",
      "            (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "            (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "        (o_proj): Linear(\n",
      "          in_features=512, out_features=512, bias=False\n",
      "          (lora): LoRA(\n",
      "            (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "            (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "          )\n",
      "        )\n",
      "        (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "      )\n",
      "      (input_layernorm): RMSNorm()\n",
      "      (post_attention_layernorm): RMSNorm()\n",
      "      (mlp): FeedForward(\n",
      "        (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "        (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "        (dropout): Dropout(p=0.0, inplace=False)\n",
      "        (act_fn): SiLUActivation()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (norm): RMSNorm()\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "2\n",
      "model.embed_tokens Embedding(6400, 512)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "3\n",
      "model.dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "4\n",
      "model.layers ModuleList(\n",
      "  (0-7): 8 x MiniMindBlock(\n",
      "    (self_attn): Attention(\n",
      "      (q_proj): Linear(\n",
      "        in_features=512, out_features=512, bias=False\n",
      "        (lora): LoRA(\n",
      "          (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "          (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "      (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "      (o_proj): Linear(\n",
      "        in_features=512, out_features=512, bias=False\n",
      "        (lora): LoRA(\n",
      "          (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "          (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "        )\n",
      "      )\n",
      "      (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "      (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "    )\n",
      "    (input_layernorm): RMSNorm()\n",
      "    (post_attention_layernorm): RMSNorm()\n",
      "    (mlp): FeedForward(\n",
      "      (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "      (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "      (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "      (dropout): Dropout(p=0.0, inplace=False)\n",
      "      (act_fn): SiLUActivation()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "5\n",
      "model.layers.0 MiniMindBlock(\n",
      "  (self_attn): Attention(\n",
      "    (q_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (o_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (input_layernorm): RMSNorm()\n",
      "  (post_attention_layernorm): RMSNorm()\n",
      "  (mlp): FeedForward(\n",
      "    (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "    (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "6\n",
      "model.layers.0.self_attn Attention(\n",
      "  (q_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (o_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "7\n",
      "model.layers.0.self_attn.q_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "8\n",
      "model.layers.0.self_attn.q_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "9\n",
      "model.layers.0.self_attn.q_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "10\n",
      "model.layers.0.self_attn.q_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "11\n",
      "model.layers.0.self_attn.k_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "12\n",
      "model.layers.0.self_attn.v_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "13\n",
      "model.layers.0.self_attn.o_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "14\n",
      "model.layers.0.self_attn.o_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "15\n",
      "model.layers.0.self_attn.o_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "16\n",
      "model.layers.0.self_attn.o_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "17\n",
      "model.layers.0.self_attn.attn_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "18\n",
      "model.layers.0.self_attn.resid_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "19\n",
      "model.layers.0.input_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "20\n",
      "model.layers.0.post_attention_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "21\n",
      "model.layers.0.mlp FeedForward(\n",
      "  (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "  (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "22\n",
      "model.layers.0.mlp.gate_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "23\n",
      "model.layers.0.mlp.down_proj Linear(in_features=1408, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "24\n",
      "model.layers.0.mlp.up_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "25\n",
      "model.layers.0.mlp.dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "26\n",
      "model.layers.0.mlp.act_fn SiLUActivation()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "27\n",
      "model.layers.1 MiniMindBlock(\n",
      "  (self_attn): Attention(\n",
      "    (q_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (o_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (input_layernorm): RMSNorm()\n",
      "  (post_attention_layernorm): RMSNorm()\n",
      "  (mlp): FeedForward(\n",
      "    (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "    (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "28\n",
      "model.layers.1.self_attn Attention(\n",
      "  (q_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (o_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "29\n",
      "model.layers.1.self_attn.q_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "30\n",
      "model.layers.1.self_attn.q_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "31\n",
      "model.layers.1.self_attn.q_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "32\n",
      "model.layers.1.self_attn.q_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "33\n",
      "model.layers.1.self_attn.k_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "34\n",
      "model.layers.1.self_attn.v_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "35\n",
      "model.layers.1.self_attn.o_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "36\n",
      "model.layers.1.self_attn.o_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "37\n",
      "model.layers.1.self_attn.o_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "38\n",
      "model.layers.1.self_attn.o_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "39\n",
      "model.layers.1.self_attn.attn_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "40\n",
      "model.layers.1.self_attn.resid_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "41\n",
      "model.layers.1.input_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "42\n",
      "model.layers.1.post_attention_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "43\n",
      "model.layers.1.mlp FeedForward(\n",
      "  (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "  (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "44\n",
      "model.layers.1.mlp.gate_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "45\n",
      "model.layers.1.mlp.down_proj Linear(in_features=1408, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "46\n",
      "model.layers.1.mlp.up_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "47\n",
      "model.layers.1.mlp.dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "48\n",
      "model.layers.1.mlp.act_fn SiLUActivation()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "49\n",
      "model.layers.2 MiniMindBlock(\n",
      "  (self_attn): Attention(\n",
      "    (q_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (o_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (input_layernorm): RMSNorm()\n",
      "  (post_attention_layernorm): RMSNorm()\n",
      "  (mlp): FeedForward(\n",
      "    (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "    (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "50\n",
      "model.layers.2.self_attn Attention(\n",
      "  (q_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (o_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "51\n",
      "model.layers.2.self_attn.q_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "52\n",
      "model.layers.2.self_attn.q_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "53\n",
      "model.layers.2.self_attn.q_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "54\n",
      "model.layers.2.self_attn.q_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "55\n",
      "model.layers.2.self_attn.k_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "56\n",
      "model.layers.2.self_attn.v_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "57\n",
      "model.layers.2.self_attn.o_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "58\n",
      "model.layers.2.self_attn.o_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "59\n",
      "model.layers.2.self_attn.o_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "60\n",
      "model.layers.2.self_attn.o_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "61\n",
      "model.layers.2.self_attn.attn_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "62\n",
      "model.layers.2.self_attn.resid_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "63\n",
      "model.layers.2.input_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "64\n",
      "model.layers.2.post_attention_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "65\n",
      "model.layers.2.mlp FeedForward(\n",
      "  (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "  (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "66\n",
      "model.layers.2.mlp.gate_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "67\n",
      "model.layers.2.mlp.down_proj Linear(in_features=1408, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "68\n",
      "model.layers.2.mlp.up_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "69\n",
      "model.layers.2.mlp.dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "70\n",
      "model.layers.2.mlp.act_fn SiLUActivation()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "71\n",
      "model.layers.3 MiniMindBlock(\n",
      "  (self_attn): Attention(\n",
      "    (q_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (o_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (input_layernorm): RMSNorm()\n",
      "  (post_attention_layernorm): RMSNorm()\n",
      "  (mlp): FeedForward(\n",
      "    (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "    (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "72\n",
      "model.layers.3.self_attn Attention(\n",
      "  (q_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (o_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "73\n",
      "model.layers.3.self_attn.q_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "74\n",
      "model.layers.3.self_attn.q_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "75\n",
      "model.layers.3.self_attn.q_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "76\n",
      "model.layers.3.self_attn.q_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "77\n",
      "model.layers.3.self_attn.k_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "78\n",
      "model.layers.3.self_attn.v_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "79\n",
      "model.layers.3.self_attn.o_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "80\n",
      "model.layers.3.self_attn.o_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "81\n",
      "model.layers.3.self_attn.o_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "82\n",
      "model.layers.3.self_attn.o_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "83\n",
      "model.layers.3.self_attn.attn_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "84\n",
      "model.layers.3.self_attn.resid_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "85\n",
      "model.layers.3.input_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "86\n",
      "model.layers.3.post_attention_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "87\n",
      "model.layers.3.mlp FeedForward(\n",
      "  (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "  (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "88\n",
      "model.layers.3.mlp.gate_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "89\n",
      "model.layers.3.mlp.down_proj Linear(in_features=1408, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "90\n",
      "model.layers.3.mlp.up_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "91\n",
      "model.layers.3.mlp.dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "92\n",
      "model.layers.3.mlp.act_fn SiLUActivation()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "93\n",
      "model.layers.4 MiniMindBlock(\n",
      "  (self_attn): Attention(\n",
      "    (q_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (o_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (input_layernorm): RMSNorm()\n",
      "  (post_attention_layernorm): RMSNorm()\n",
      "  (mlp): FeedForward(\n",
      "    (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "    (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "94\n",
      "model.layers.4.self_attn Attention(\n",
      "  (q_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (o_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "95\n",
      "model.layers.4.self_attn.q_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "96\n",
      "model.layers.4.self_attn.q_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "97\n",
      "model.layers.4.self_attn.q_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "98\n",
      "model.layers.4.self_attn.q_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "99\n",
      "model.layers.4.self_attn.k_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "100\n",
      "model.layers.4.self_attn.v_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "101\n",
      "model.layers.4.self_attn.o_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "102\n",
      "model.layers.4.self_attn.o_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "103\n",
      "model.layers.4.self_attn.o_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "104\n",
      "model.layers.4.self_attn.o_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "105\n",
      "model.layers.4.self_attn.attn_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "106\n",
      "model.layers.4.self_attn.resid_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "107\n",
      "model.layers.4.input_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "108\n",
      "model.layers.4.post_attention_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "109\n",
      "model.layers.4.mlp FeedForward(\n",
      "  (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "  (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "110\n",
      "model.layers.4.mlp.gate_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "111\n",
      "model.layers.4.mlp.down_proj Linear(in_features=1408, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "112\n",
      "model.layers.4.mlp.up_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "113\n",
      "model.layers.4.mlp.dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "114\n",
      "model.layers.4.mlp.act_fn SiLUActivation()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "115\n",
      "model.layers.5 MiniMindBlock(\n",
      "  (self_attn): Attention(\n",
      "    (q_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (o_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (input_layernorm): RMSNorm()\n",
      "  (post_attention_layernorm): RMSNorm()\n",
      "  (mlp): FeedForward(\n",
      "    (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "    (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "116\n",
      "model.layers.5.self_attn Attention(\n",
      "  (q_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (o_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "117\n",
      "model.layers.5.self_attn.q_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "118\n",
      "model.layers.5.self_attn.q_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "119\n",
      "model.layers.5.self_attn.q_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "120\n",
      "model.layers.5.self_attn.q_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "121\n",
      "model.layers.5.self_attn.k_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "122\n",
      "model.layers.5.self_attn.v_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "123\n",
      "model.layers.5.self_attn.o_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "124\n",
      "model.layers.5.self_attn.o_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "125\n",
      "model.layers.5.self_attn.o_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "126\n",
      "model.layers.5.self_attn.o_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "127\n",
      "model.layers.5.self_attn.attn_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "128\n",
      "model.layers.5.self_attn.resid_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "129\n",
      "model.layers.5.input_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "130\n",
      "model.layers.5.post_attention_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "131\n",
      "model.layers.5.mlp FeedForward(\n",
      "  (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "  (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "132\n",
      "model.layers.5.mlp.gate_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "133\n",
      "model.layers.5.mlp.down_proj Linear(in_features=1408, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "134\n",
      "model.layers.5.mlp.up_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "135\n",
      "model.layers.5.mlp.dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "136\n",
      "model.layers.5.mlp.act_fn SiLUActivation()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "137\n",
      "model.layers.6 MiniMindBlock(\n",
      "  (self_attn): Attention(\n",
      "    (q_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (o_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (input_layernorm): RMSNorm()\n",
      "  (post_attention_layernorm): RMSNorm()\n",
      "  (mlp): FeedForward(\n",
      "    (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "    (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "138\n",
      "model.layers.6.self_attn Attention(\n",
      "  (q_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (o_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "139\n",
      "model.layers.6.self_attn.q_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "140\n",
      "model.layers.6.self_attn.q_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "141\n",
      "model.layers.6.self_attn.q_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "142\n",
      "model.layers.6.self_attn.q_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "143\n",
      "model.layers.6.self_attn.k_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "144\n",
      "model.layers.6.self_attn.v_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "145\n",
      "model.layers.6.self_attn.o_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "146\n",
      "model.layers.6.self_attn.o_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "147\n",
      "model.layers.6.self_attn.o_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "148\n",
      "model.layers.6.self_attn.o_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "149\n",
      "model.layers.6.self_attn.attn_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "150\n",
      "model.layers.6.self_attn.resid_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "151\n",
      "model.layers.6.input_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "152\n",
      "model.layers.6.post_attention_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "153\n",
      "model.layers.6.mlp FeedForward(\n",
      "  (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "  (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "154\n",
      "model.layers.6.mlp.gate_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "155\n",
      "model.layers.6.mlp.down_proj Linear(in_features=1408, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "156\n",
      "model.layers.6.mlp.up_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "157\n",
      "model.layers.6.mlp.dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "158\n",
      "model.layers.6.mlp.act_fn SiLUActivation()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "159\n",
      "model.layers.7 MiniMindBlock(\n",
      "  (self_attn): Attention(\n",
      "    (q_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "    (o_proj): Linear(\n",
      "      in_features=512, out_features=512, bias=False\n",
      "      (lora): LoRA(\n",
      "        (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "        (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "      )\n",
      "    )\n",
      "    (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "    (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      "  )\n",
      "  (input_layernorm): RMSNorm()\n",
      "  (post_attention_layernorm): RMSNorm()\n",
      "  (mlp): FeedForward(\n",
      "    (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "    (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "    (dropout): Dropout(p=0.0, inplace=False)\n",
      "    (act_fn): SiLUActivation()\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "160\n",
      "model.layers.7.self_attn Attention(\n",
      "  (q_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (k_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (v_proj): Linear(in_features=512, out_features=128, bias=False)\n",
      "  (o_proj): Linear(\n",
      "    in_features=512, out_features=512, bias=False\n",
      "    (lora): LoRA(\n",
      "      (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "      (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (attn_dropout): Dropout(p=0.0, inplace=False)\n",
      "  (resid_dropout): Dropout(p=0.0, inplace=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "161\n",
      "model.layers.7.self_attn.q_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "162\n",
      "model.layers.7.self_attn.q_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "163\n",
      "model.layers.7.self_attn.q_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "164\n",
      "model.layers.7.self_attn.q_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "165\n",
      "model.layers.7.self_attn.k_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "166\n",
      "model.layers.7.self_attn.v_proj Linear(in_features=512, out_features=128, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "167\n",
      "model.layers.7.self_attn.o_proj Linear(\n",
      "  in_features=512, out_features=512, bias=False\n",
      "  (lora): LoRA(\n",
      "    (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "    (B): Linear(in_features=8, out_features=512, bias=False)\n",
      "  )\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "168\n",
      "model.layers.7.self_attn.o_proj.lora LoRA(\n",
      "  (A): Linear(in_features=512, out_features=8, bias=False)\n",
      "  (B): Linear(in_features=8, out_features=512, bias=False)\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "169\n",
      "model.layers.7.self_attn.o_proj.lora.A Linear(in_features=512, out_features=8, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "170\n",
      "model.layers.7.self_attn.o_proj.lora.B Linear(in_features=8, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "171\n",
      "model.layers.7.self_attn.attn_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "172\n",
      "model.layers.7.self_attn.resid_dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "173\n",
      "model.layers.7.input_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "174\n",
      "model.layers.7.post_attention_layernorm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "175\n",
      "model.layers.7.mlp FeedForward(\n",
      "  (gate_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (down_proj): Linear(in_features=1408, out_features=512, bias=False)\n",
      "  (up_proj): Linear(in_features=512, out_features=1408, bias=False)\n",
      "  (dropout): Dropout(p=0.0, inplace=False)\n",
      "  (act_fn): SiLUActivation()\n",
      ")\n",
      "----------------------------------------------------------------------------------------------------\n",
      "176\n",
      "model.layers.7.mlp.gate_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "177\n",
      "model.layers.7.mlp.down_proj Linear(in_features=1408, out_features=512, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "178\n",
      "model.layers.7.mlp.up_proj Linear(in_features=512, out_features=1408, bias=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "179\n",
      "model.layers.7.mlp.dropout Dropout(p=0.0, inplace=False)\n",
      "----------------------------------------------------------------------------------------------------\n",
      "180\n",
      "model.layers.7.mlp.act_fn SiLUActivation()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "181\n",
      "model.norm RMSNorm()\n",
      "----------------------------------------------------------------------------------------------------\n",
      "182\n",
      "lm_head Linear(in_features=512, out_features=6400, bias=False)\n"
     ]
    }
   ],
   "source": [
    "for i, (name, module) in enumerate(model.named_modules()):\n",
    "    print('-'*100)\n",
    "    print(i)\n",
    "    print(name, module)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "88360c50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已修改 8 层 Attention 模块以捕获注意力权重\n"
     ]
    }
   ],
   "source": [
    "# 导入必要的函数\n",
    "from model.model_minimind import apply_rotary_pos_emb, repeat_kv\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import types\n",
    "\n",
    "# 用于存储每一层的注意力权重（格式: {layer_idx: attention_matrix}）\n",
    "attention_weights = {}\n",
    "\n",
    "def patch_attention_to_capture_scores(model):\n",
    "    \"\"\"修改模型中所有 Attention 模块的 forward 方法以捕获注意力分数\"\"\"\n",
    "    # 保存原始的 forward 方法\n",
    "    original_forwards = {}\n",
    "    \n",
    "    def create_capturing_forward(original_forward, layer_idx):\n",
    "        \"\"\"创建一个包装函数来捕获注意力分数\"\"\"\n",
    "        def capturing_forward(self, x, position_embeddings, past_key_value=None, use_cache=False, attention_mask=None):\n",
    "            bsz, seq_len, _ = x.shape\n",
    "            xq, xk, xv = self.q_proj(x), self.k_proj(x), self.v_proj(x)\n",
    "            xq = xq.view(bsz, seq_len, self.n_local_heads, self.head_dim)\n",
    "            xk = xk.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "            xv = xv.view(bsz, seq_len, self.n_local_kv_heads, self.head_dim)\n",
    "            \n",
    "            cos, sin = position_embeddings\n",
    "            xq, xk = apply_rotary_pos_emb(xq, xk, cos[:seq_len], sin[:seq_len])\n",
    "            \n",
    "            if past_key_value is not None:\n",
    "                xk = torch.cat([past_key_value[0], xk], dim=1)\n",
    "                xv = torch.cat([past_key_value[1], xv], dim=1)\n",
    "            past_kv = (xk, xv) if use_cache else None\n",
    "            \n",
    "            xq, xk, xv = (\n",
    "                xq.transpose(1, 2),\n",
    "                repeat_kv(xk, self.n_rep).transpose(1, 2),\n",
    "                repeat_kv(xv, self.n_rep).transpose(1, 2)\n",
    "            )\n",
    "            \n",
    "            # 计算注意力分数\n",
    "            scores = (xq @ xk.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "            seq_len_kv = xk.shape[2]  # 考虑 kv_cache 的情况\n",
    "            \n",
    "            # Causal mask\n",
    "            if seq_len_kv == seq_len:\n",
    "                causal_mask = torch.triu(\n",
    "                    torch.full((seq_len, seq_len), float(\"-inf\"), device=scores.device),\n",
    "                    diagonal=1\n",
    "                ).unsqueeze(0).unsqueeze(0)\n",
    "            else:\n",
    "                causal_mask = torch.triu(\n",
    "                    torch.full((seq_len, seq_len_kv), float(\"-inf\"), device=scores.device),\n",
    "                    diagonal=seq_len_kv - seq_len + 1\n",
    "                ).unsqueeze(0).unsqueeze(0)\n",
    "            scores = scores + causal_mask\n",
    "            \n",
    "            if attention_mask is not None:\n",
    "                extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
    "                extended_attention_mask = (1.0 - extended_attention_mask) * -1e9\n",
    "                if seq_len_kv > seq_len:\n",
    "                    pad_length = seq_len_kv - seq_len\n",
    "                    extended_attention_mask = F.pad(extended_attention_mask, (0, pad_length), value=-1e9)\n",
    "                scores = scores + extended_attention_mask\n",
    "            \n",
    "            attention_probs = F.softmax(scores.float(), dim=-1).type_as(xq)\n",
    "            \n",
    "            # 保存注意力权重（平均所有 head），形状: [num_heads, seq_len, seq_len_kv] -> [seq_len, seq_len_kv]\n",
    "            attention_weights[f'layer_{layer_idx}'] = attention_probs.detach().cpu().mean(dim=1).squeeze(0)\n",
    "            \n",
    "            attention_probs = self.attn_dropout(attention_probs)\n",
    "            output = attention_probs @ xv\n",
    "            \n",
    "            output = output.transpose(1, 2).reshape(bsz, seq_len, -1)\n",
    "            output = self.resid_dropout(self.o_proj(output))\n",
    "            return output, past_kv\n",
    "        \n",
    "        return capturing_forward\n",
    "    \n",
    "    # 修改每一层的 Attention 模块\n",
    "    for layer_idx, layer in enumerate(model.model.layers):\n",
    "        original_forwards[layer_idx] = layer.self_attn.forward\n",
    "        capturing_forward = create_capturing_forward(original_forwards[layer_idx], layer_idx)\n",
    "        # 使用 types.MethodType 正确绑定方法\n",
    "        layer.self_attn.forward = types.MethodType(capturing_forward, layer.self_attn)\n",
    "    \n",
    "    print(f\"已修改 {len(model.model.layers)} 层 Attention 模块以捕获注意力权重\")\n",
    "    return original_forwards\n",
    "\n",
    "# 应用修改\n",
    "original_forwards = patch_attention_to_capture_scores(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c828ea95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已定义生成函数\n"
     ]
    }
   ],
   "source": [
    "# 创建自定义生成函数，逐步生成并收集注意力权重\n",
    "def generate_with_attention(model, tokenizer, prompt, max_new_tokens=20, temperature=0.8, top_p=0.9):\n",
    "    \"\"\"\n",
    "    自定义生成函数，在生成过程中收集每一层的注意力权重\n",
    "    \n",
    "    返回: (generated_text, all_attention_weights)\n",
    "    \"\"\"\n",
    "    global attention_weights\n",
    "    all_step_attentions = {}  # {step: {layer_key: attention_matrix}}\n",
    "    \n",
    "    # 禁用 flash attention 以确保能捕获注意力分数\n",
    "    for layer in model.model.layers:\n",
    "        layer.self_attn.flash = False\n",
    "    \n",
    "    device = next(model.parameters()).device\n",
    "    \n",
    "    # 准备输入\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    input_ids = inputs['input_ids']\n",
    "    attention_mask = inputs.get('attention_mask', None)\n",
    "    \n",
    "    generated_ids = input_ids.clone()\n",
    "    past_key_values = None\n",
    "    next_token = None\n",
    "    \n",
    "    # 逐步生成\n",
    "    for step in range(max_new_tokens):\n",
    "        # 清空当前步骤的注意力权重\n",
    "        attention_weights = {}\n",
    "        \n",
    "        # 前向传播（会自动捕获注意力权重）\n",
    "        with torch.no_grad():\n",
    "            if step == 0:\n",
    "                # 第一次：使用完整的 input_ids\n",
    "                outputs = model(\n",
    "                    input_ids=generated_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    past_key_values=None,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            else:\n",
    "                # 后续步骤：只使用新生成的 token\n",
    "                outputs = model(\n",
    "                    input_ids=next_token,\n",
    "                    attention_mask=None,  # kv_cache 模式下不需要完整的 mask\n",
    "                    past_key_values=past_key_values,\n",
    "                    use_cache=True\n",
    "                )\n",
    "            logits = outputs.logits[:, -1, :]  # 取最后一个位置的 logits\n",
    "            past_key_values = outputs.past_key_values\n",
    "        \n",
    "        # 保存当前步骤的注意力权重\n",
    "        if attention_weights:\n",
    "            all_step_attentions[step] = attention_weights.copy()\n",
    "        \n",
    "        # 采样下一个 token\n",
    "        if temperature > 0:\n",
    "            probs = torch.softmax(logits / temperature, dim=-1)\n",
    "            # Top-p sampling\n",
    "            sorted_probs, sorted_indices = torch.sort(probs, descending=True)\n",
    "            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "            sorted_indices_to_remove = cumsum_probs > top_p\n",
    "            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
    "            sorted_indices_to_remove[..., 0] = 0\n",
    "            indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
    "            probs[indices_to_remove] = 0\n",
    "            probs = probs / probs.sum(dim=-1, keepdim=True)\n",
    "            next_token = torch.multinomial(probs, num_samples=1)\n",
    "        else:\n",
    "            next_token = torch.argmax(logits, dim=-1, keepdim=True)\n",
    "        \n",
    "        # 添加到生成的序列\n",
    "        generated_ids = torch.cat([generated_ids, next_token], dim=1)\n",
    "        \n",
    "        # 更新 attention mask（用于后续解码）\n",
    "        if attention_mask is not None:\n",
    "            attention_mask = torch.cat([attention_mask, torch.ones((attention_mask.shape[0], 1), device=device, dtype=attention_mask.dtype)], dim=1)\n",
    "        \n",
    "        # 检查是否遇到 EOS\n",
    "        if next_token.item() == tokenizer.eos_token_id:\n",
    "            break\n",
    "    \n",
    "    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n",
    "    return generated_text, all_step_attentions\n",
    "\n",
    "print(\"已定义生成函数\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9986baac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已定义可视化函数\n"
     ]
    }
   ],
   "source": [
    "# 可视化注意力权重的函数\n",
    "def visualize_attention(attention_matrix, layer_idx, step=None, tokenizer=None, input_ids=None, head_avg=True):\n",
    "    \"\"\"\n",
    "    可视化单层注意力权重\n",
    "    \n",
    "    Args:\n",
    "        attention_matrix: 注意力矩阵，形状 [seq_len, seq_len_kv]\n",
    "        layer_idx: 层索引\n",
    "        step: 生成步骤（可选）\n",
    "        tokenizer: tokenizer（用于显示 token）\n",
    "        input_ids: 输入 token IDs（用于显示 token）\n",
    "        head_avg: 是否已经平均了所有 head\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import numpy as np\n",
    "    \n",
    "    attn = attention_matrix.numpy() if isinstance(attention_matrix, torch.Tensor) else attention_matrix\n",
    "    \n",
    "    # 创建图形\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # 绘制热力图\n",
    "    im = ax.imshow(attn, cmap='Blues', aspect='auto', interpolation='nearest')\n",
    "    \n",
    "    # 添加颜色条\n",
    "    plt.colorbar(im, ax=ax, label='Attention Weight')\n",
    "    \n",
    "    # 设置标签\n",
    "    title = f'Layer {layer_idx} Attention'\n",
    "    if step is not None:\n",
    "        title += f' (Step {step})'\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Key Position (Previous Tokens)', fontsize=12)\n",
    "    ax.set_ylabel('Query Position (Current Token)', fontsize=12)\n",
    "    \n",
    "    # 如果提供了 tokenizer 和 input_ids，显示 token 标签\n",
    "    if tokenizer is not None and input_ids is not None:\n",
    "        tokens = tokenizer.convert_ids_to_tokens(input_ids[0])\n",
    "        # 只显示部分 token（避免标签太密集）\n",
    "        if len(tokens) > 20:\n",
    "            step_size = len(tokens) // 20\n",
    "            tick_positions = list(range(0, len(tokens), step_size))\n",
    "            tick_labels = [tokens[i] if i < len(tokens) else '' for i in tick_positions]\n",
    "        else:\n",
    "            tick_positions = list(range(len(tokens)))\n",
    "            tick_labels = tokens\n",
    "        \n",
    "        ax.set_xticks(tick_positions)\n",
    "        ax.set_xticklabels(tick_labels, rotation=45, ha='right', fontsize=8)\n",
    "        ax.set_yticks(tick_positions)\n",
    "        ax.set_yticklabels(tick_labels, fontsize=8)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 打印数值统计\n",
    "    print(f\"\\\\nLayer {layer_idx} Attention Statistics:\")\n",
    "    print(f\"  Shape: {attn.shape}\")\n",
    "    print(f\"  Min: {attn.min():.6f}\")\n",
    "    print(f\"  Max: {attn.max():.6f}\")\n",
    "    print(f\"  Mean: {attn.mean():.6f}\")\n",
    "    print(f\"  Sum per row (should be ~1.0): {attn.sum(axis=-1)[:5]}...\")  # 显示前5行\n",
    "\n",
    "def print_attention_details(attention_matrix, layer_idx, step=None, top_k=5):\n",
    "    \"\"\"打印注意力权重的详细信息\"\"\"\n",
    "    attn = attention_matrix.numpy() if isinstance(attention_matrix, torch.Tensor) else attention_matrix\n",
    "    \n",
    "    print(f\"\\\\n{'='*80}\")\n",
    "    title = f\"Layer {layer_idx} Attention Details\"\n",
    "    if step is not None:\n",
    "        title += f\" (Generation Step {step})\"\n",
    "    print(title)\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # 对于每一行（query position），显示注意力权重最高的 top_k 个位置\n",
    "    for query_pos in range(min(5, attn.shape[0])):  # 只显示前5个 query positions\n",
    "        row = attn[query_pos, :]\n",
    "        top_k_indices = np.argsort(row)[-top_k:][::-1]\n",
    "        top_k_values = row[top_k_indices]\n",
    "        \n",
    "        print(f\"\\\\nQuery Position {query_pos}:\")\n",
    "        for i, (idx, val) in enumerate(zip(top_k_indices, top_k_values)):\n",
    "            print(f\"  {i+1}. Key Position {idx}: {val:.6f}\")\n",
    "    \n",
    "    # 打印完整的注意力矩阵（如果不太大）\n",
    "    if attn.shape[0] <= 10 and attn.shape[1] <= 10:\n",
    "        print(f\"\\\\nFull Attention Matrix:\")\n",
    "        print(attn)\n",
    "    else:\n",
    "        print(f\"\\\\nAttention Matrix (first 5x5):\")\n",
    "        print(attn[:5, :5])\n",
    "\n",
    "print(\"已定义可视化函数\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "95d193e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入提示: <|im_start|>user\n",
      "标题：The dollar has hit its highest level\n",
      "内容：The dollar has hit its highest level against the euro in almost 3 months\n",
      "请根据标题和内容，给出文章的分类。(在以下选项中选：['entertainment' 'sport' 'politics' 'tech' 'business'])<|im_end|>\n",
      "<|im_start|>assistant\n",
      "开始生成并收集注意力权重...\n",
      "\n",
      "\n",
      "生成文本: user\n",
      "标题：The dollar has hit its highest level\n",
      "内容：The dollar has hit its highest level against the euro in almost 3 months\n",
      "请根据标题和内容，给出文章的分类。(在以下选项中选：['entertainment' 'sport' 'politics' 'tech' 'business'])\n",
      "assistant>politiness\n",
      "\n",
      "收集到 6 个生成步骤的注意力权重\n"
     ]
    }
   ],
   "source": [
    "# 测试生成并收集注意力权重\n",
    "prompt = '''<|im_start|>user\n",
    "标题：The dollar has hit its highest level\n",
    "内容：The dollar has hit its highest level against the euro in almost 3 months\n",
    "请根据标题和内容，给出文章的分类。(在以下选项中选：['entertainment' 'sport' 'politics' 'tech' 'business'])<|im_end|>\n",
    "<|im_start|>assistant'''\n",
    "max_new_tokens = 50\n",
    "\n",
    "print(f\"输入提示: {prompt}\")\n",
    "print(f\"开始生成并收集注意力权重...\\n\")\n",
    "\n",
    "generated_text, all_attention = generate_with_attention(\n",
    "    model, tokenizer, prompt, \n",
    "    max_new_tokens=max_new_tokens,\n",
    "    temperature=1.2,\n",
    "    top_p=0.8\n",
    ")\n",
    "\n",
    "print(f\"\\n生成文本: {generated_text}\")\n",
    "print(f\"\\n收集到 {len(all_attention)} 个生成步骤的注意力权重\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "10dda57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "显示生成步骤 5 的注意力权重\n",
      "================================================================================\n",
      "\\n================================================================================\n",
      "Layer 0 Attention Details (Generation Step 5)\n",
      "================================================================================\n",
      "\\nQuery Position 0:\n",
      "  1. Key Position 104: 0.804786\n",
      "  2. Key Position 102: 0.068178\n",
      "  3. Key Position 103: 0.021745\n",
      "  4. Key Position 98: 0.013952\n",
      "  5. Key Position 84: 0.007418\n",
      "\\nAttention Matrix (first 5x5):\n",
      "[[5.1872390e-05 8.4732991e-04 6.6318625e-04 4.1106701e-04 1.2921287e-04]]\n",
      "\\n================================================================================\n",
      "Layer 1 Attention Details (Generation Step 5)\n",
      "================================================================================\n",
      "\\nQuery Position 0:\n",
      "  1. Key Position 104: 0.245569\n",
      "  2. Key Position 103: 0.194255\n",
      "  3. Key Position 96: 0.091334\n",
      "  4. Key Position 98: 0.077849\n",
      "  5. Key Position 102: 0.069417\n",
      "\\nAttention Matrix (first 5x5):\n",
      "[[1.3912680e-02 2.0646994e-04 7.5389363e-04 4.9090316e-04 8.9023779e-05]]\n",
      "\\n================================================================================\n",
      "Layer 2 Attention Details (Generation Step 5)\n",
      "================================================================================\n",
      "\\nQuery Position 0:\n",
      "  1. Key Position 0: 0.432069\n",
      "  2. Key Position 104: 0.095296\n",
      "  3. Key Position 103: 0.073218\n",
      "  4. Key Position 102: 0.045389\n",
      "  5. Key Position 100: 0.043947\n",
      "\\nAttention Matrix (first 5x5):\n",
      "[[4.3206862e-01 1.4222973e-03 2.6972985e-03 5.0568772e-03 3.1000169e-04]]\n",
      "\\n================================================================================\n",
      "Layer 3 Attention Details (Generation Step 5)\n",
      "================================================================================\n",
      "\\nQuery Position 0:\n",
      "  1. Key Position 103: 0.230615\n",
      "  2. Key Position 100: 0.124000\n",
      "  3. Key Position 0: 0.082115\n",
      "  4. Key Position 102: 0.050520\n",
      "  5. Key Position 96: 0.041104\n",
      "\\nAttention Matrix (first 5x5):\n",
      "[[0.08211539 0.00242196 0.00674853 0.00976302 0.00233489]]\n",
      "\\n================================================================================\n",
      "Layer 4 Attention Details (Generation Step 5)\n",
      "================================================================================\n",
      "\\nQuery Position 0:\n",
      "  1. Key Position 100: 0.144372\n",
      "  2. Key Position 96: 0.119877\n",
      "  3. Key Position 95: 0.070067\n",
      "  4. Key Position 0: 0.052144\n",
      "  5. Key Position 93: 0.044666\n",
      "\\nAttention Matrix (first 5x5):\n",
      "[[0.05214417 0.00291364 0.0056544  0.00647478 0.00278825]]\n",
      "\\n================================================================================\n",
      "Layer 5 Attention Details (Generation Step 5)\n",
      "================================================================================\n",
      "\\nQuery Position 0:\n",
      "  1. Key Position 103: 0.219091\n",
      "  2. Key Position 96: 0.182861\n",
      "  3. Key Position 100: 0.131191\n",
      "  4. Key Position 0: 0.109071\n",
      "  5. Key Position 102: 0.100037\n",
      "\\nAttention Matrix (first 5x5):\n",
      "[[0.10907086 0.00073404 0.00245377 0.00383658 0.00379254]]\n",
      "\\n================================================================================\n",
      "Layer 6 Attention Details (Generation Step 5)\n",
      "================================================================================\n",
      "\\nQuery Position 0:\n",
      "  1. Key Position 0: 0.138527\n",
      "  2. Key Position 96: 0.073938\n",
      "  3. Key Position 73: 0.068174\n",
      "  4. Key Position 100: 0.065043\n",
      "  5. Key Position 4: 0.052312\n",
      "\\nAttention Matrix (first 5x5):\n",
      "[[0.13852654 0.01093135 0.00154017 0.01600754 0.05231247]]\n",
      "\\n================================================================================\n",
      "Layer 7 Attention Details (Generation Step 5)\n",
      "================================================================================\n",
      "\\nQuery Position 0:\n",
      "  1. Key Position 100: 0.143211\n",
      "  2. Key Position 104: 0.118012\n",
      "  3. Key Position 99: 0.107889\n",
      "  4. Key Position 0: 0.095044\n",
      "  5. Key Position 98: 0.067006\n",
      "\\nAttention Matrix (first 5x5):\n",
      "[[0.09504422 0.00281482 0.00700933 0.00974269 0.00470269]]\n"
     ]
    }
   ],
   "source": [
    "# 显示每个生成步骤每一层的注意力权重详情\n",
    "import numpy as np\n",
    "\n",
    "# 选择要显示的步骤（例如最后一个步骤）\n",
    "step_to_show = len(all_attention) - 1  # 最后一个生成步骤\n",
    "\n",
    "if step_to_show in all_attention:\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"显示生成步骤 {step_to_show} 的注意力权重\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    step_attentions = all_attention[step_to_show]\n",
    "    \n",
    "    for layer_key in sorted(step_attentions.keys(), key=lambda x: int(x.split('_')[1])):\n",
    "        layer_idx = int(layer_key.split('_')[1])\n",
    "        attn_matrix = step_attentions[layer_key]\n",
    "        \n",
    "        # 打印详细信息\n",
    "        print_attention_details(attn_matrix, layer_idx, step=step_to_show, top_k=5)\n",
    "        \n",
    "        # 可选：可视化（取消注释以显示）\n",
    "        # visualize_attention(attn_matrix, layer_idx, step=step_to_show)\n",
    "else:\n",
    "    print(f\"步骤 {step_to_show} 不存在\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6c655848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "注意力权重统计信息:\n",
      " Step  Layer      Shape      Min      Max     Mean      Std   Sum\n",
      "    0      0 (100, 100) 0.000000 1.000000 0.010000 0.064100 100.0\n",
      "    0      1 (100, 100) 0.000000 1.000000 0.010000 0.038689 100.0\n",
      "    0      2 (100, 100) 0.000000 1.000000 0.010000 0.045502 100.0\n",
      "    0      3 (100, 100) 0.000000 1.000000 0.010000 0.036393 100.0\n",
      "    0      4 (100, 100) 0.000000 1.000000 0.010000 0.035212 100.0\n",
      "    0      5 (100, 100) 0.000000 1.000000 0.010000 0.039258 100.0\n",
      "    0      6 (100, 100) 0.000000 1.000000 0.010000 0.053254 100.0\n",
      "    0      7 (100, 100) 0.000000 1.000000 0.010000 0.033236 100.0\n",
      "    1      0   (1, 101) 0.001344 0.177436 0.009901 0.017942   1.0\n",
      "    1      1   (1, 101) 0.000095 0.200046 0.009901 0.030237   1.0\n",
      "    1      2   (1, 101) 0.000396 0.229295 0.009901 0.025841   1.0\n",
      "    1      3   (1, 101) 0.000248 0.199773 0.009901 0.027445   1.0\n",
      "    1      4   (1, 101) 0.000720 0.218724 0.009901 0.026122   1.0\n",
      "    1      5   (1, 101) 0.000199 0.188120 0.009901 0.032826   1.0\n",
      "    1      6   (1, 101) 0.000181 0.275371 0.009901 0.029252   1.0\n",
      "    1      7   (1, 101) 0.000661 0.159834 0.009901 0.027649   1.0\n",
      "    2      0   (1, 102) 0.000222 0.477123 0.009804 0.047214   1.0\n",
      "    2      1   (1, 102) 0.000024 0.296570 0.009804 0.036714   1.0\n",
      "    2      2   (1, 102) 0.000429 0.360003 0.009804 0.037102   1.0\n",
      "    2      3   (1, 102) 0.000435 0.272094 0.009804 0.033698   1.0\n",
      "    2      4   (1, 102) 0.000598 0.272850 0.009804 0.031492   1.0\n",
      "    2      5   (1, 102) 0.000139 0.359305 0.009804 0.039495   1.0\n",
      "    2      6   (1, 102) 0.000025 0.472069 0.009804 0.050157   1.0\n",
      "    2      7   (1, 102) 0.000742 0.192653 0.009804 0.025771   1.0\n",
      "    3      0   (1, 103) 0.000009 0.832797 0.009709 0.081771   1.0\n",
      "    3      1   (1, 103) 0.000037 0.298070 0.009709 0.036739   1.0\n",
      "    3      2   (1, 103) 0.000108 0.385569 0.009709 0.046553   1.0\n",
      "    3      3   (1, 103) 0.000173 0.302443 0.009709 0.038864   1.0\n",
      "    3      4   (1, 103) 0.000319 0.316276 0.009709 0.034695   1.0\n",
      "    3      5   (1, 103) 0.000079 0.234736 0.009709 0.037175   1.0\n",
      "    3      6   (1, 103) 0.000093 0.326914 0.009709 0.038351   1.0\n",
      "    3      7   (1, 103) 0.000797 0.185967 0.009709 0.022533   1.0\n",
      "    4      0   (1, 104) 0.000305 0.345133 0.009615 0.043907   1.0\n",
      "    4      1   (1, 104) 0.000069 0.376022 0.009615 0.040191   1.0\n",
      "    4      2   (1, 104) 0.000112 0.369022 0.009615 0.040893   1.0\n",
      "    4      3   (1, 104) 0.000126 0.320409 0.009615 0.037809   1.0\n",
      "    4      4   (1, 104) 0.000382 0.184276 0.009615 0.025397   1.0\n",
      "    4      5   (1, 104) 0.000012 0.258704 0.009615 0.039048   1.0\n",
      "    4      6   (1, 104) 0.000032 0.357362 0.009615 0.037392   1.0\n",
      "    4      7   (1, 104) 0.000769 0.131653 0.009615 0.023212   1.0\n",
      "    5      0   (1, 105) 0.000014 0.804786 0.009524 0.078300   1.0\n",
      "    5      1   (1, 105) 0.000026 0.245569 0.009524 0.033187   1.0\n",
      "    5      2   (1, 105) 0.000083 0.432069 0.009524 0.043523   1.0\n",
      "    5      3   (1, 105) 0.000617 0.230615 0.009524 0.027089   1.0\n",
      "    5      4   (1, 105) 0.000973 0.144372 0.009524 0.020401   1.0\n",
      "    5      5   (1, 105) 0.000149 0.219091 0.009524 0.033113   1.0\n",
      "    5      6   (1, 105) 0.000069 0.138527 0.009524 0.018928   1.0\n",
      "    5      7   (1, 105) 0.000352 0.143211 0.009524 0.023688   1.0\n",
      "\n",
      "\n",
      "================================================================================\n",
      "详细注意力矩阵数值:\n",
      "================================================================================\n",
      "\n",
      "步骤 5, 层 0 的完整注意力矩阵:\n",
      "形状: (1, 105)\n",
      "\n",
      "矩阵数值 (每个元素表示 query position 对 key position 的注意力权重):\n",
      "[[5.18723900e-05 8.47329909e-04 6.63186249e-04 4.11067012e-04\n",
      "  1.29212873e-04 1.48835403e-04 3.75125557e-04 1.09544082e-03\n",
      "  5.76521328e-04 5.69691067e-04 4.88410704e-04 4.22003854e-04\n",
      "  4.59191069e-04 9.44075524e-04 1.08870713e-03 1.18798553e-03\n",
      "  1.47380040e-03 2.07477901e-03 5.55176812e-04 2.43075512e-04\n",
      "  5.07287798e-04 2.28280245e-04 3.35208635e-04 1.95771252e-04\n",
      "  3.32821626e-04 6.05858746e-04 3.82816157e-04 3.47830501e-04\n",
      "  6.14460616e-04 6.98639138e-04 6.43109379e-04 3.59330152e-04\n",
      "  5.66283823e-04 1.11572118e-03 1.29401626e-03 1.99911394e-03\n",
      "  1.51679176e-03 7.45430239e-04 5.86864538e-04 6.20353501e-04\n",
      "  3.13999219e-04 1.27198058e-03 1.09902467e-03 1.06351264e-03\n",
      "  6.38593163e-04 9.06119414e-04 9.28616617e-04 3.09205388e-05\n",
      "  7.44649951e-05 5.86864589e-05 3.93182418e-04 2.76035367e-04\n",
      "  2.70839868e-04 2.36851047e-04 1.93949745e-04 7.22357800e-05\n",
      "  4.47470120e-05 1.44588057e-05 1.13826936e-04 5.42816597e-05\n",
      "  4.76030749e-04 5.84203444e-05 3.34352153e-05 2.79794564e-04\n",
      "  1.28306026e-04 8.84394540e-05 9.68832246e-05 5.18071931e-04\n",
      "  3.53594223e-04 6.33300748e-04 2.63190886e-04 9.24730557e-04\n",
      "  3.28115741e-04 8.65456415e-04 8.81239306e-04 5.14741288e-03\n",
      "  2.15218824e-04 3.08677088e-04 6.03654480e-04 2.17085425e-03\n",
      "  9.53534443e-04 3.44882463e-03 1.15312650e-04 2.65324808e-04\n",
      "  7.41806859e-03 3.64237092e-03 1.21830060e-04 3.49196722e-04\n",
      "  7.41344003e-04 3.42004211e-03 5.41671924e-03 1.62417782e-04\n",
      "  3.27699934e-04 8.59152278e-05 4.21886180e-05 2.23256811e-05\n",
      "  2.51723613e-05 5.40155824e-03 1.39520653e-02 1.81110378e-03\n",
      "  3.60668171e-04 7.27474596e-03 6.81778491e-02 2.17448920e-02\n",
      "  8.04786205e-01]]\n",
      "\n",
      "每一行的和（验证概率分布）:\n",
      "  Row 0: 1.000000\n"
     ]
    }
   ],
   "source": [
    "# 显示所有步骤、所有层的注意力权重数值（以表格形式）\n",
    "import pandas as pd\n",
    "\n",
    "# 创建一个 DataFrame 来存储注意力权重的统计信息\n",
    "attention_stats = []\n",
    "\n",
    "for step, step_attentions in all_attention.items():\n",
    "    for layer_key, attn_matrix in step_attentions.items():\n",
    "        layer_idx = int(layer_key.split('_')[1])\n",
    "        attn_np = attn_matrix.numpy() if isinstance(attn_matrix, torch.Tensor) else attn_matrix\n",
    "        \n",
    "        attention_stats.append({\n",
    "            'Step': step,\n",
    "            'Layer': layer_idx,\n",
    "            'Shape': str(attn_np.shape),\n",
    "            'Min': attn_np.min(),\n",
    "            'Max': attn_np.max(),\n",
    "            'Mean': attn_np.mean(),\n",
    "            'Std': attn_np.std(),\n",
    "            'Sum': attn_np.sum()\n",
    "        })\n",
    "\n",
    "df = pd.DataFrame(attention_stats)\n",
    "print(\"注意力权重统计信息:\")\n",
    "print(df.to_string(index=False))\n",
    "\n",
    "# 对于特定步骤和层，显示完整的注意力矩阵\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(\"详细注意力矩阵数值:\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "# 选择最后一个生成步骤和第一层作为示例\n",
    "last_step = len(all_attention) - 1\n",
    "first_layer_key = f'layer_0'\n",
    "\n",
    "if last_step in all_attention and first_layer_key in all_attention[last_step]:\n",
    "    attn_matrix = all_attention[last_step][first_layer_key]\n",
    "    attn_np = attn_matrix.numpy() if isinstance(attn_matrix, torch.Tensor) else attn_matrix\n",
    "    \n",
    "    print(f\"\\n步骤 {last_step}, 层 0 的完整注意力矩阵:\")\n",
    "    print(f\"形状: {attn_np.shape}\")\n",
    "    print(\"\\n矩阵数值 (每个元素表示 query position 对 key position 的注意力权重):\")\n",
    "    print(attn_np)\n",
    "    \n",
    "    # 打印每一行的和（应该接近1.0）\n",
    "    print(f\"\\n每一行的和（验证概率分布）:\")\n",
    "    row_sums = attn_np.sum(axis=-1)\n",
    "    for i, row_sum in enumerate(row_sums):\n",
    "        print(f\"  Row {i}: {row_sum:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "b87e34ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 320, 275, 201, 1232, 674, 355, 1148, 302, 393, 78, 305, 780, 322, 284, 1113, 4143, 262, 377, 2187, 201, 1911, 355, 1148, 302, 393, 78, 305, 780, 322, 284, 1113, 4143, 262, 377, 2187, 4021, 276, 311, 5521, 295, 439, 79, 1802, 1328, 1804, 4567, 201, 1055, 1227, 1232, 674, 315, 1911, 270, 3496, 2239, 269, 2467, 286, 10, 368, 1104, 5752, 413, 838, 355, 61, 9, 309, 1544, 410, 9, 2040, 85, 1519, 9, 2040, 82, 393, 284, 1238, 9, 2040, 1491, 352, 9, 2040, 68, 320, 1532, 9, 63, 11, 2, 201, 1, 1078, 538, 501]\n",
      "<|im_start|>|us|er|\n",
      "|标|题|：|The| d|ol|l|ar| has| h|it| its| hig|he|st| level|\n",
      "|内容|：|The| d|ol|l|ar| has| h|it| its| hig|he|st| level| against| the| e|uro| in| al|m|ost| 3| mon|ths|\n",
      "|请|根据|标|题|和|内容|，|给出|文章|的|分类|。|(|在|以下|选项|中|选|：|[|'|ent|ertain|ment|'| '|s|port|'| '|p|ol|it|ics|'| '|te|ch|'| '|b|us|iness|'|]|)|<|im_end|>|\n",
      "|<|im_start|>|ass|ist|ant\n"
     ]
    }
   ],
   "source": [
    "encoded=tokenizer.encode(prompt)\n",
    "splitted='|'.join([tokenizer.decode([encoded[i]]) for i in range(len(encoded))])\n",
    "print(encoded)\n",
    "print(splitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a0a74aa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[302, 393, 78, 305]\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.encode(' dollar'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252acb66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0147)\n",
      "tensor(0.0165)\n",
      "tensor(0.0051)\n",
      "tensor(0.0077)\n"
     ]
    }
   ],
   "source": [
    "# 24-27:' dollar'\n",
    "print(all_attention[0]['layer_0'][:,24].mean())\n",
    "print(all_attention[0]['layer_0'][:,25].mean())\n",
    "print(all_attention[0]['layer_0'][:,26].mean())\n",
    "print(all_attention[0]['layer_0'][:,27].mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312pt291cu128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
