{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adfc117c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from model.model_lora import *\n",
    "from trainer.trainer_utils import setup_seed\n",
    "from torch.utils.data import Dataset\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e38b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('dataset/bbc-news-data.csv',on_bad_lines='warn',encoding='utf-8',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c8277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech']\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "for index,row in df.iterrows():\n",
    "    data.append(row[df.columns[0]])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c8d73418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index = [i for i in range(len(data))]\n",
    "random.shuffle(index)\n",
    "df = df.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b0b1cfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(title,content):\n",
    "    return f\"æ ‡é¢˜ï¼š{title}\\nå†…å®¹ï¼š{content}\"\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data=data\n",
    "        self.prompt = []\n",
    "        self.label=[]\n",
    "        for i in range(len(data)):\n",
    "            self.prompt.append(generate_prompt(data.at[i,'title'],data.at[i,'content']))\n",
    "            self.label.append(data.at[i,'category'])\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "df1=df[:1113]\n",
    "df2=df[1113:]\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = df2.reset_index(drop=True)\n",
    "train_dataset=dataset(df1)\n",
    "test_dataset=dataset(df2)\n",
    "full_dataset=dataset(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ede7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å·²ä¿å­˜ 2225 æ¡æ•°æ®åˆ° dataset/bbc_news_data_no_instruction.jsonl\n",
      "å·²ä¿å­˜ 1113 æ¡æ•°æ®åˆ° dataset/bbc_news_train_no_instruction.jsonl\n",
      "å·²ä¿å­˜ 1112 æ¡æ•°æ®åˆ° dataset/bbc_news_test_no_instruction.jsonl\n"
     ]
    }
   ],
   "source": [
    "# å°†è®­ç»ƒé›†è½¬æ¢ä¸ºjsonlæ ¼å¼ï¼ˆå‚è€ƒlora_identity.jsonlçš„ç»“æ„ï¼‰\n",
    "import json\n",
    "\n",
    "def save_dataset_to_jsonl(dataset, output_file):\n",
    "    \"\"\"\n",
    "    å°†datasetè½¬æ¢ä¸ºjsonlæ ¼å¼å¹¶ä¿å­˜\n",
    "    \n",
    "    Args:\n",
    "        dataset: datasetå¯¹è±¡ï¼ŒåŒ…å«promptå’Œlabelå±æ€§\n",
    "        output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for i in range(len(dataset)):\n",
    "            # æ„å»ºconversationsæ ¼å¼\n",
    "            conversation = {\n",
    "                \"conversations\": [\n",
    "                    {\"role\": \"user\", \"content\": dataset.prompt[i]},\n",
    "                    {\"role\": \"assistant\", \"content\": dataset.label[i]}\n",
    "                ]\n",
    "            }\n",
    "            # å†™å…¥jsonlæ–‡ä»¶ï¼ˆæ¯è¡Œä¸€ä¸ªJSONå¯¹è±¡ï¼‰\n",
    "            f.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "    print(f\"å·²ä¿å­˜ {len(dataset)} æ¡æ•°æ®åˆ° {output_file}\")\n",
    "save_dataset_to_jsonl(full_dataset, 'dataset/bbc_news_data_no_instruction.jsonl')\n",
    "# ä¿å­˜è®­ç»ƒé›†\n",
    "save_dataset_to_jsonl(train_dataset, 'dataset/bbc_news_train_no_instruction.jsonl')\n",
    "\n",
    "# # ä¿å­˜æµ‹è¯•é›†ï¼ˆå¯é€‰ï¼‰\n",
    "save_dataset_to_jsonl(test_dataset, 'dataset/bbc_news_test_no_instruction.jsonl')\n",
    "\n",
    "# # éªŒè¯ï¼šè¯»å–å‰å‡ è¡ŒæŸ¥çœ‹æ ¼å¼\n",
    "# print(\"\\néªŒè¯æ ¼å¼ï¼ˆå‰3æ¡ï¼‰ï¼š\")\n",
    "# with open('dataset/bbc_news_train.jsonl', 'r', encoding='utf-8') as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         if i >= 3:\n",
    "#             break\n",
    "#         data = json.loads(line)\n",
    "#         print(f\"\\nç¬¬{i+1}æ¡:\")\n",
    "#         print(f\"  User: {data['conversations'][0]['content'][:100]}...\")\n",
    "#         print(f\"  Assistant: {data['conversations'][1]['content']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f8537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨æ£€æŸ¥æ•°æ®æ–‡ä»¶: dataset/bbc_news_train.jsonl\n",
      "================================================================================\n",
      "\n",
      "[åŸºç¡€æ£€æŸ¥] JSONæ ¼å¼å’ŒåŸºæœ¬ç»“æ„...\n",
      "  æ€»æ ·æœ¬æ•°: 1113\n",
      "  JSONæ ¼å¼é”™è¯¯: 0\n",
      "  ç©ºconversations: 0\n",
      "  ç»“æ„é”™è¯¯: 0\n",
      "  ç©ºå†…å®¹: 0\n",
      "\n",
      "âœ… åŸºç¡€æ£€æŸ¥é€šè¿‡ï¼\n"
     ]
    }
   ],
   "source": [
    "# åŸºç¡€æ£€æŸ¥ï¼šä¸éœ€è¦tokenizerï¼Œåªæ£€æŸ¥JSONæ ¼å¼å’Œç»“æ„\n",
    "import json\n",
    "\n",
    "def check_training_data_basic(jsonl_path):\n",
    "    \"\"\"\n",
    "    åŸºç¡€æ£€æŸ¥ï¼šä¸éœ€è¦tokenizerï¼Œåªæ£€æŸ¥JSONæ ¼å¼å’Œç»“æ„\n",
    "    \"\"\"\n",
    "    print(f\"æ­£åœ¨æ£€æŸ¥æ•°æ®æ–‡ä»¶: {jsonl_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    stats = {\n",
    "        'total_samples': 0,\n",
    "        'json_errors': [],\n",
    "        'empty_conversations': [],\n",
    "        'invalid_structure': [],\n",
    "        'empty_content': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\n[åŸºç¡€æ£€æŸ¥] JSONæ ¼å¼å’ŒåŸºæœ¬ç»“æ„...\")\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            stats['total_samples'] += 1\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                if 'conversations' not in data:\n",
    "                    stats['invalid_structure'].append({\n",
    "                        'line': line_num,\n",
    "                        'error': 'Missing \"conversations\" field'\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                if not data['conversations'] or len(data['conversations']) == 0:\n",
    "                    stats['empty_conversations'].append({'line': line_num})\n",
    "                    continue\n",
    "                \n",
    "                for i, conv in enumerate(data['conversations']):\n",
    "                    if 'role' not in conv or 'content' not in conv:\n",
    "                        stats['invalid_structure'].append({\n",
    "                            'line': line_num,\n",
    "                            'error': f'Conversation {i} missing \"role\" or \"content\"'\n",
    "                        })\n",
    "                        break\n",
    "                    if not conv['content'] or len(conv['content'].strip()) == 0:\n",
    "                        stats['empty_content'].append({\n",
    "                            'line': line_num,\n",
    "                            'conversation_index': i,\n",
    "                            'role': conv.get('role', 'unknown')\n",
    "                        })\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                stats['json_errors'].append({\n",
    "                    'line': line_num,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    print(f\"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}\")\n",
    "    print(f\"  JSONæ ¼å¼é”™è¯¯: {len(stats['json_errors'])}\")\n",
    "    print(f\"  ç©ºconversations: {len(stats['empty_conversations'])}\")\n",
    "    print(f\"  ç»“æ„é”™è¯¯: {len(stats['invalid_structure'])}\")\n",
    "    print(f\"  ç©ºå†…å®¹: {len(stats['empty_content'])}\")\n",
    "    \n",
    "    if stats['json_errors'] or stats['invalid_structure'] or stats['empty_conversations']:\n",
    "        print(\"\\nâš ï¸  å‘ç°åŸºç¡€é—®é¢˜ï¼Œè¯·å…ˆä¿®å¤ï¼\")\n",
    "    else:\n",
    "        print(\"\\nâœ… åŸºç¡€æ£€æŸ¥é€šè¿‡ï¼\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# å…ˆæ‰§è¡ŒåŸºç¡€æ£€æŸ¥ï¼ˆä¸éœ€è¦tokenizerï¼‰\n",
    "basic_stats = check_training_data_basic('dataset/bbc_news_train.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a78a058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å®Œæ•´æ£€æŸ¥æ•°æ®æ–‡ä»¶: dataset/bbc_news_train.jsonl\n",
      "================================================================================\n",
      "\n",
      "[1] æ£€æŸ¥JSONæ ¼å¼å’ŒåŸºæœ¬ç»“æ„...\n",
      "  æ€»æ ·æœ¬æ•°: 1113\n",
      "  JSONæ ¼å¼é”™è¯¯: 0\n",
      "  ç©ºconversations: 0\n",
      "  ç»“æ„é”™è¯¯: 0\n",
      "  ç©ºå†…å®¹: 0\n",
      "\n",
      "[2] æ£€æŸ¥loss_maskï¼ˆè¿™æ˜¯å¯¼è‡´nançš„å…³é”®ï¼‰...\n",
      "\n",
      "================================================================================\n",
      "è¯¦ç»†å¼‚å¸¸æŠ¥å‘Š:\n",
      "================================================================================\n",
      "\n",
      "âš ï¸  å…³é”®é—®é¢˜: loss_maskå…¨ä¸º0 (901 ä¸ª)\n",
      "   è¿™ä¼šå¯¼è‡´è®­ç»ƒæ—¶é™¤é›¶é”™è¯¯ï¼Œäº§ç”Ÿnanï¼\n",
      "  æ ·æœ¬ç´¢å¼• 0 (è¡Œ 1)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼šFockers fuel festive film chart\n",
      "å†…å®¹ï¼š Comedy Meet...\n",
      "      [1] assistant: entertainment\n",
      "  æ ·æœ¬ç´¢å¼• 2 (è¡Œ 3)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼š'Hitler' row over Welsh arts cash\n",
      "å†…å®¹ï¼š An artist...\n",
      "      [1] assistant: politics\n",
      "  æ ·æœ¬ç´¢å¼• 3 (è¡Œ 4)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼šDavenport hits out at Wimbledon\n",
      "å†…å®¹ï¼š World numbe...\n",
      "      [1] assistant: sport\n",
      "  æ ·æœ¬ç´¢å¼• 5 (è¡Œ 6)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼šStraw to attend Auschwitz service\n",
      "å†…å®¹ï¼š Foreign S...\n",
      "      [1] assistant: politics\n",
      "  æ ·æœ¬ç´¢å¼• 6 (è¡Œ 7)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼šUS state acts to stop 'spammers'\n",
      "å†…å®¹ï¼š US state T...\n",
      "      [1] assistant: tech\n",
      "  æ ·æœ¬ç´¢å¼• 7 (è¡Œ 8)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼šA-listers flock to Gervais sitcom\n",
      "å†…å®¹ï¼š Hollywood...\n",
      "      [1] assistant: entertainment\n",
      "  æ ·æœ¬ç´¢å¼• 8 (è¡Œ 9)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼šRow brewing over peer-to-peer ads\n",
      "å†…å®¹ï¼š Music dow...\n",
      "      [1] assistant: tech\n",
      "  æ ·æœ¬ç´¢å¼• 9 (è¡Œ 10)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼šYoung debut cut short by Ginepri\n",
      "å†…å®¹ï¼š Fifteen-ye...\n",
      "      [1] assistant: sport\n",
      "  æ ·æœ¬ç´¢å¼• 10 (è¡Œ 11)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼šReboot ordered for EU patent law\n",
      "å†…å®¹ï¼š A European...\n",
      "      [1] assistant: tech\n",
      "  æ ·æœ¬ç´¢å¼• 11 (è¡Œ 12)\n",
      "    conversationsæ•°é‡: 2\n",
      "      [0] user: æ ‡é¢˜ï¼šBlair looks to election campaign\n",
      "å†…å®¹ï¼š Tony Blair...\n",
      "      [1] assistant: politics\n",
      "\n",
      "âš ï¸  åºåˆ—è¿‡é•¿ (1113 ä¸ª):\n",
      "  æ ·æœ¬ç´¢å¼• 0 (è¡Œ 1): é•¿åº¦ 511\n",
      "  æ ·æœ¬ç´¢å¼• 1 (è¡Œ 2): é•¿åº¦ 511\n",
      "  æ ·æœ¬ç´¢å¼• 2 (è¡Œ 3): é•¿åº¦ 511\n",
      "  æ ·æœ¬ç´¢å¼• 3 (è¡Œ 4): é•¿åº¦ 511\n",
      "  æ ·æœ¬ç´¢å¼• 4 (è¡Œ 5): é•¿åº¦ 511\n",
      "\n",
      "================================================================================\n",
      "æ£€æŸ¥æ€»ç»“:\n",
      "================================================================================\n",
      "âš ï¸  å‘ç° 901 ä¸ªæ½œåœ¨é—®é¢˜\n",
      "\n",
      "ğŸ”´ ä¸¥é‡: æœ‰ 901 ä¸ªæ ·æœ¬çš„loss_maskå…¨ä¸º0\n",
      "   è¿™äº›æ ·æœ¬ä¼šå¯¼è‡´è®­ç»ƒæ—¶å‡ºç°nanï¼Œå»ºè®®ä¿®å¤æˆ–è¿‡æ»¤ï¼\n"
     ]
    }
   ],
   "source": [
    "# å®Œæ•´æ£€æŸ¥ï¼šéœ€è¦tokenizerï¼Œæ£€æŸ¥loss_maskï¼ˆè¿™æ˜¯å¯¼è‡´nançš„å…³é”®ï¼‰\n",
    "# æ³¨æ„ï¼šéœ€è¦å…ˆè¿è¡ŒCell 10åˆå§‹åŒ–modelå’Œtokenizer\n",
    "from dataset.lm_dataset import SFTDataset\n",
    "\n",
    "def check_training_data_full(jsonl_path, tokenizer, max_length=10240):\n",
    "    \"\"\"\n",
    "    å®Œæ•´æ£€æŸ¥ï¼šéœ€è¦tokenizerï¼Œæ£€æŸ¥loss_maskï¼ˆè¿™æ˜¯å¯¼è‡´nançš„å…³é”®ï¼‰\n",
    "    \n",
    "    æ£€æŸ¥é¡¹ï¼š\n",
    "    1. JSONæ ¼å¼æ˜¯å¦æ­£ç¡®\n",
    "    2. conversationsç»“æ„æ˜¯å¦å®Œæ•´\n",
    "    3. loss_maskæ˜¯å¦å…¨ä¸º0ï¼ˆä¼šå¯¼è‡´é™¤é›¶é”™è¯¯ï¼‰\n",
    "    4. tokenizedåçš„é•¿åº¦æ˜¯å¦å¼‚å¸¸\n",
    "    5. æ˜¯å¦æœ‰ç©ºå†…å®¹\n",
    "    \"\"\"\n",
    "    print(f\"æ­£åœ¨å®Œæ•´æ£€æŸ¥æ•°æ®æ–‡ä»¶: {jsonl_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # ç»Ÿè®¡ä¿¡æ¯\n",
    "    stats = {\n",
    "        'total_samples': 0,\n",
    "        'json_errors': [],\n",
    "        'empty_conversations': [],\n",
    "        'invalid_structure': [],\n",
    "        'zero_loss_mask': [],\n",
    "        'too_long': [],\n",
    "        'too_short': [],\n",
    "        'empty_content': [],\n",
    "        'special_chars_issues': []\n",
    "    }\n",
    "    \n",
    "    # 1. æ£€æŸ¥JSONæ ¼å¼å’ŒåŸºæœ¬ç»“æ„\n",
    "    print(\"\\n[1] æ£€æŸ¥JSONæ ¼å¼å’ŒåŸºæœ¬ç»“æ„...\")\n",
    "    valid_samples = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            stats['total_samples'] += 1\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                if 'conversations' not in data:\n",
    "                    stats['invalid_structure'].append({\n",
    "                        'line': line_num,\n",
    "                        'error': 'Missing \"conversations\" field'\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                if not data['conversations'] or len(data['conversations']) == 0:\n",
    "                    stats['empty_conversations'].append({\n",
    "                        'line': line_num,\n",
    "                        'data': data\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                for i, conv in enumerate(data['conversations']):\n",
    "                    if 'role' not in conv or 'content' not in conv:\n",
    "                        stats['invalid_structure'].append({\n",
    "                            'line': line_num,\n",
    "                            'error': f'Conversation {i} missing \"role\" or \"content\"'\n",
    "                        })\n",
    "                        break\n",
    "                    if not conv['content'] or len(conv['content'].strip()) == 0:\n",
    "                        stats['empty_content'].append({\n",
    "                            'line': line_num,\n",
    "                            'conversation_index': i,\n",
    "                            'role': conv.get('role', 'unknown')\n",
    "                        })\n",
    "                \n",
    "                valid_samples.append((line_num, data))\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                stats['json_errors'].append({\n",
    "                    'line': line_num,\n",
    "                    'error': str(e),\n",
    "                    'content': line[:100]\n",
    "                })\n",
    "    \n",
    "    print(f\"  æ€»æ ·æœ¬æ•°: {stats['total_samples']}\")\n",
    "    print(f\"  JSONæ ¼å¼é”™è¯¯: {len(stats['json_errors'])}\")\n",
    "    print(f\"  ç©ºconversations: {len(stats['empty_conversations'])}\")\n",
    "    print(f\"  ç»“æ„é”™è¯¯: {len(stats['invalid_structure'])}\")\n",
    "    print(f\"  ç©ºå†…å®¹: {len(stats['empty_content'])}\")\n",
    "    \n",
    "    # 2. ä½¿ç”¨SFTDatasetæ£€æŸ¥loss_maskï¼ˆè¿™æ˜¯å¯¼è‡´nançš„å…³é”®ï¼‰\n",
    "    print(\"\\n[2] æ£€æŸ¥loss_maskï¼ˆè¿™æ˜¯å¯¼è‡´nançš„å…³é”®ï¼‰...\")\n",
    "    try:\n",
    "        dataset = SFTDataset(jsonl_path, tokenizer, max_length=max_length)\n",
    "        \n",
    "        for idx in range(len(dataset)):\n",
    "            try:\n",
    "                X, Y, loss_mask = dataset[idx]\n",
    "                \n",
    "                # æ£€æŸ¥loss_maskæ˜¯å¦å…¨ä¸º0\n",
    "                loss_mask_sum = loss_mask.sum().item()\n",
    "                if loss_mask_sum == 0:\n",
    "                    stats['zero_loss_mask'].append({\n",
    "                        'index': idx,\n",
    "                        'line': valid_samples[idx][0] if idx < len(valid_samples) else 'unknown',\n",
    "                        'X_shape': X.shape,\n",
    "                        'Y_shape': Y.shape\n",
    "                    })\n",
    "                \n",
    "                # æ£€æŸ¥é•¿åº¦\n",
    "                seq_len = X.shape[0]\n",
    "                if seq_len < 10:\n",
    "                    stats['too_short'].append({\n",
    "                        'index': idx,\n",
    "                        'line': valid_samples[idx][0] if idx < len(valid_samples) else 'unknown',\n",
    "                        'length': seq_len\n",
    "                    })\n",
    "                elif seq_len >= max_length - 1:\n",
    "                    stats['too_long'].append({\n",
    "                        'index': idx,\n",
    "                        'line': valid_samples[idx][0] if idx < len(valid_samples) else 'unknown',\n",
    "                        'length': seq_len\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                stats['special_chars_issues'].append({\n",
    "                    'index': idx,\n",
    "                    'line': valid_samples[idx][0] if idx < len(valid_samples) else 'unknown',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  é”™è¯¯: æ— æ³•åˆ›å»ºdataset - {e}\")\n",
    "    \n",
    "    # 3. æ‰“å°è¯¦ç»†æŠ¥å‘Š\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"è¯¦ç»†å¼‚å¸¸æŠ¥å‘Š:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if stats['json_errors']:\n",
    "        print(f\"\\nâŒ JSONæ ¼å¼é”™è¯¯ ({len(stats['json_errors'])} ä¸ª):\")\n",
    "        for err in stats['json_errors'][:5]:\n",
    "            print(f\"  è¡Œ {err['line']}: {err['error']}\")\n",
    "    \n",
    "    if stats['zero_loss_mask']:\n",
    "        print(f\"\\nâš ï¸  å…³é”®é—®é¢˜: loss_maskå…¨ä¸º0 ({len(stats['zero_loss_mask'])} ä¸ª)\")\n",
    "        print(\"   è¿™ä¼šå¯¼è‡´è®­ç»ƒæ—¶é™¤é›¶é”™è¯¯ï¼Œäº§ç”Ÿnanï¼\")\n",
    "        for err in stats['zero_loss_mask'][:10]:\n",
    "            print(f\"  æ ·æœ¬ç´¢å¼• {err['index']} (è¡Œ {err['line']})\")\n",
    "            if err['index'] < len(valid_samples):\n",
    "                line_num, data = valid_samples[err['index']]\n",
    "                print(f\"    conversationsæ•°é‡: {len(data['conversations'])}\")\n",
    "                for i, conv in enumerate(data['conversations']):\n",
    "                    content_preview = conv['content'][:50] + \"...\" if len(conv['content']) > 50 else conv['content']\n",
    "                    print(f\"      [{i}] {conv['role']}: {content_preview}\")\n",
    "    \n",
    "    if stats['empty_conversations']:\n",
    "        print(f\"\\nâŒ ç©ºconversations ({len(stats['empty_conversations'])} ä¸ª):\")\n",
    "        for err in stats['empty_conversations'][:5]:\n",
    "            print(f\"  è¡Œ {err['line']}\")\n",
    "    \n",
    "    if stats['invalid_structure']:\n",
    "        print(f\"\\nâŒ ç»“æ„é”™è¯¯ ({len(stats['invalid_structure'])} ä¸ª):\")\n",
    "        for err in stats['invalid_structure'][:5]:\n",
    "            print(f\"  è¡Œ {err['line']}: {err['error']}\")\n",
    "    \n",
    "    if stats['empty_content']:\n",
    "        print(f\"\\nâš ï¸  ç©ºå†…å®¹ ({len(stats['empty_content'])} ä¸ª):\")\n",
    "        for err in stats['empty_content'][:5]:\n",
    "            print(f\"  è¡Œ {err['line']}, conversation {err['conversation_index']}, role: {err['role']}\")\n",
    "    \n",
    "    if stats['too_short']:\n",
    "        print(f\"\\nâš ï¸  åºåˆ—è¿‡çŸ­ ({len(stats['too_short'])} ä¸ª):\")\n",
    "        for err in stats['too_short'][:5]:\n",
    "            print(f\"  æ ·æœ¬ç´¢å¼• {err['index']} (è¡Œ {err['line']}): é•¿åº¦ {err['length']}\")\n",
    "    \n",
    "    if stats['too_long']:\n",
    "        print(f\"\\nâš ï¸  åºåˆ—è¿‡é•¿ ({len(stats['too_long'])} ä¸ª):\")\n",
    "        for err in stats['too_long'][:5]:\n",
    "            print(f\"  æ ·æœ¬ç´¢å¼• {err['index']} (è¡Œ {err['line']}): é•¿åº¦ {err['length']}\")\n",
    "    \n",
    "    if stats['special_chars_issues']:\n",
    "        print(f\"\\nâš ï¸  ç‰¹æ®Šå­—ç¬¦é—®é¢˜ ({len(stats['special_chars_issues'])} ä¸ª):\")\n",
    "        for err in stats['special_chars_issues'][:5]:\n",
    "            print(f\"  æ ·æœ¬ç´¢å¼• {err['index']} (è¡Œ {err['line']}): {err['error']}\")\n",
    "    \n",
    "    # 4. æ€»ç»“\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"æ£€æŸ¥æ€»ç»“:\")\n",
    "    print(\"=\" * 80)\n",
    "    total_issues = (len(stats['json_errors']) + \n",
    "                   len(stats['empty_conversations']) + \n",
    "                   len(stats['invalid_structure']) + \n",
    "                   len(stats['zero_loss_mask']) + \n",
    "                   len(stats['empty_content']))\n",
    "    \n",
    "    if total_issues == 0:\n",
    "        print(\"âœ… æœªå‘ç°æ˜æ˜¾å¼‚å¸¸ï¼\")\n",
    "    else:\n",
    "        print(f\"âš ï¸  å‘ç° {total_issues} ä¸ªæ½œåœ¨é—®é¢˜\")\n",
    "        if stats['zero_loss_mask']:\n",
    "            print(f\"\\nğŸ”´ ä¸¥é‡: æœ‰ {len(stats['zero_loss_mask'])} ä¸ªæ ·æœ¬çš„loss_maskå…¨ä¸º0\")\n",
    "            print(\"   è¿™äº›æ ·æœ¬ä¼šå¯¼è‡´è®­ç»ƒæ—¶å‡ºç°nanï¼Œå»ºè®®ä¿®å¤æˆ–è¿‡æ»¤ï¼\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "full_stats = check_training_data_full(\n",
    "    jsonl_path='dataset/bbc_news_train.jsonl',\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7e35a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniMindæ¨¡å‹å‚æ•°: 25.83 M(illion)\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = argparse.Namespace(\n",
    "        load_from='model',\n",
    "        save_dir='out',\n",
    "        weight='full_sft',\n",
    "        lora_weight='None',\n",
    "        hidden_size=512,\n",
    "        num_hidden_layers=8,\n",
    "        use_moe=0,\n",
    "        inference_rope_scaling=False,\n",
    "        max_new_tokens=8192,\n",
    "        temperature=0.85,\n",
    "        top_p=0.85,\n",
    "        historys=0,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "def init_model(args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.load_from)\n",
    "    if 'model' in args.load_from:\n",
    "        model = MiniMindForCausalLM(MiniMindConfig(\n",
    "            hidden_size=args.hidden_size,\n",
    "            num_hidden_layers=args.num_hidden_layers,\n",
    "            use_moe=bool(args.use_moe),\n",
    "            inference_rope_scaling=args.inference_rope_scaling\n",
    "        ))\n",
    "        moe_suffix = '_moe' if args.use_moe else ''\n",
    "        ckp = f'./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth'\n",
    "        model.load_state_dict(torch.load(ckp, map_location=args.device), strict=True)\n",
    "        if args.lora_weight != 'None':\n",
    "            apply_lora(model)\n",
    "            load_lora(model, f'./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth')\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)\n",
    "    print(f'MiniMindæ¨¡å‹å‚æ•°: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)')\n",
    "    return model.eval().to(device), tokenizer\n",
    "model, tokenizer = init_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc16676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 2344, 363, 1059, 3067]\n",
      "I| love| you| so| much|"
     ]
    }
   ],
   "source": [
    "sentence='''I love you so much'''\n",
    "encoded=tokenizer.encode(sentence,add_special_tokens=False)\n",
    "print(encoded)\n",
    "for i in encoded:\n",
    "    print(tokenizer.decode([i]),end='|')\n",
    "# decoded=tokenizer.decode(encoded)\n",
    "# print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d311ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '<|im_start|>', '<|im_end|>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', 'Â¡', 'Â¢', 'Â£', 'Â¤', 'Â¥', 'Â¦', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', 'Â¬', 'Â®', 'Â¯', 'Â°', 'Â±', 'Â²', 'Â³', 'Â´', 'Âµ', 'Â¶', 'Â·', 'Â¸', 'Â¹', 'Âº', 'Â»', 'Â¼', 'Â½', 'Â¾', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã„', 'Ã…', 'Ã†', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'Ã', 'Ã', 'Ã', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã˜', 'Ã™', 'Ãš', 'Ã›', 'Ãœ', 'Ã', 'Ã', 'ÃŸ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã¥', 'Ã¦', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã·', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ã½', 'Ã¾', 'Ã¿', 'Ä€', 'Ä', 'Ä‚', 'Äƒ', 'Ä„', 'Ä…', 'Ä†', 'Ä‡', 'Äˆ', 'Ä‰', 'ÄŠ', 'Ä‹', 'ÄŒ', 'Ä', 'Ä', 'Ä', 'Ä', 'Ä‘', 'Ä’', 'Ä“', 'Ä”', 'Ä•', 'Ä–', 'Ä—', 'Ä˜', 'Ä™', 'Äš', 'Ä›', 'Äœ', 'Ä', 'Ä', 'ÄŸ', 'Ä ', 'Ä¡', 'Ä¢', 'Ä£', 'Ä¤', 'Ä¥', 'Ä¦', 'Ä§', 'Ä¨', 'Ä©', 'Äª', 'Ä«', 'Ä¬', 'Ä­', 'Ä®', 'Ä¯', 'Ä°', 'Ä±', 'Ä²', 'Ä³', 'Ä´', 'Äµ', 'Ä¶', 'Ä·', 'Ä¸', 'Ä¹', 'Äº', 'Ä»', 'Ä¼', 'Ä½', 'Ä¾', 'Ä¿', 'Å€', 'Å', 'Å‚', 'Åƒ', 'Ä t', 'Ä a', 'in', 'he', 're', 'Ã¯Â¼', 'Ã¤Â¸', 'on', 'at', 'Ã§Ä¼', 'Ã§Ä¼Ä¦', 'Ã¯Â¼Ä®', 'Ä s', 'Ä c', 'nd', 'Ã£Ä¢', 'er', 'Ä the', 'es', 'en', 'or', 'an', 'Ä and', 'ing', 'Ä p', 'it', 'al', 'Ã£Ä¢Ä¤', 'Ä o', 'Ä w', 'Ã¤Â»', 'Ä to', 'is', 'ou', 'Ä m', 'Ã¤Âº', 'Ä in', 'Ä f', 'Ä b', 'ed', 'ion', 'Ã¥Ä±', 'ic', 'Ä d', 'Ä of', 'le', 'ar', 'ro', 'Ä Ä ', 'Ã¥Ä§', 'ent', 'Ã¦Ä¾', 'Ä e', 'Ã¥Ä´', 'Ã¨Â¿', 'Ã¤Â½', 'Ã¥Ä´Ä®', 'Ã¦Äª', 'Ã¥Â®', 'Ã¥Äª', 've', 'us', 'Ä re', 'Ä h', 'Ä th', 'as', 'ct', 'Ã§Ä¶', 'om', 'Ã¥Ä¾', 'Ã¥Â¤', 'Ã¦Äº', 'Ã¥Ä¬', 'Ã¥Ä²', 'Ã¤Â¸Ä¢', 'im', 'Ã¨Â¯', 'Ã¦Ä¸', 'ation', 'lo', 'Ã§Â»', 'Ä be', 'Ã£Ä¢Ä£', 'id', 'Ä can', 'il', 'Ã¦ÄºÂ¯', 'Ã¤Â¹', 'Ã¨Â®', 'Ä A', 'Ä that', 'Ä T', 'Ã¤Â»Â¥', 'ch', 'Ä y', 'ce', 'Ã¯Â¼Ä¼', 'ot', 'ers', 'Ä n', 'Ã©Ä¢', 'ra', 'Ã¥Â°', 'Ä g', 'Ä you', 'Ã¥Åƒ', 'Ä pro', 'et', 'Ã¥Âº', 'Ã¥Ä¾Â¨', 'ly', 'Ä is', 'Ã¤Â¸Âª', 'Ä l', 'ur', 'Ä for', 'Ã¥Ä±Â¯', 'Ã©Ä©', 'st', 'Ã§Ä¼Ä¦Ã¦', 'ut', 'Ä he', 'if', 'Ä¥Â½', 'Ã¤Â¼', 'Ä I', 'Ã¨Â¡', 'ir', 'ith', 'Ã¥Â¹', 'Ä are', 'ig', 'Ä st', 'el', 'ol', 'Ã¥Â¸', 'ul', 'Ã¦Ä¿', 'Ã¦ÄªÄ³', 'Ä on', 'Ã¨Â¦', 'Ã¦Ä¾Ä«', 'Ã¦Ä¹', 'Ã¥Â¯', 'Ã¨Â§', 'Ã¨Â¦Ä£', 'Ä us', 'ay', 'Ã¦Ä·', 'Ã§Ä«', 'ow', 'ment', 'Ã§Ä¶Â¨', 'ess', 'Ã¤Â¸Åƒ', 'Ã¤Â»Â¬', 'Ã¤ÂºÂº', 'Ã¥Ä©', 'Ä ex', 'Ä Ä Ä Ä ', 'Ã¥Ä½', 'Ã¥Ä®', 'Ã¥Â¼', 'Ä con', 'se', 'Ã¨Ä¥Â½', 'Ã§Ä°', 'Ä an', 'Ä with', 'Ã¤Â¸Âº', 'ate', 'iv', 'am', 'Ä as', 'ure', 'Ã¨Â¿Ä»', 'Ã¥Ä¨', 'Ã§Åƒ', 'Ä or', 'Ã¥Â·', 'Ä al', 'ies', 'Ã§Â§', 'Ä im', 'Ã¦Ä¢', 'ver', 'ab', 'Ã¤ÂºÄ¨', 'Ä su', 'Ä de', 'ge', 'th', 'Ã¥Ä±Â¯Ã¤Â»Â¥', 'Ã¨Ä¢', 'Ã¤Â¸Ä¯', 'Ã¥Â¾', 'Ä AI', 'Ä en', 'Ã©Ä¹', 'Ã¦Ä«', 'ak', 'ive', 'Ä mo', 'Ã¥Â¥', 'Ã©Ä¿', 'Ã§Ä½', 'ity', 'Ã¤Â¿', 'un', 'Ã¨Â´', 'Ã¥Ä¯', 'Ä it', 'Ä imp', 'ect', 'Ã¦Å‚', 'Ã¥Â½', 'Ã¨Ä©', 'Ã©Â¢', 'Ã¥Äµ', 'Ã¦Â³', 'ort', 'ad', 'Ã¦Å€', 'em', 'Ä com', 'Ã¥Â¦', 'her', 'ere', 'Ä S', 'ial', 'Ä C', 'Ä The', 'Ã§Ä²', 'Ã§Ä¶Å', 'Ã¦Ä¦', 'pp', 'Ã¦Åƒ', 'Ã¦Ä¸Â¹', 'qu', 'Ä wh', 'Ã¥Â¦Ä¤', 'Ã©Ä¾', 'ant', 'Ä le', 'Ä v', 'Ã¦Ä­', 'Ã¦Ä¬', 'ust', 'Ã¦Ä¹Â¶', 'Ã§ÅƒÄ«', 'Ã¥Ä³', 'Ã¥Â¯Â¹', 'ter', 'ld', 'Ã¨Â¡Ä®', 'Ä ch', 'ud', 'Ã©Ä¾Ä¢', 'Ã¦Â°', 'Ã¦ÄªÄ²', 'Ä |', 'ac', 'ain', 'iz', 'Ã¦Ä±', 'ions', 'Ä ha', 'Ã¦Ä½', '--', 'Ã¦Ä¿Â¥', 'ome', 'Ã¥Â¿', \"'s\", 'Ä ne', 'est', 'Ã¤Â¾', 'um', 'Ã¥ÄªÂ°', 'Ã¥Ä¾Â°', 'ist', 'Ã¢Ä¢', 'Ã§Ä«Â©', 'Ã¤Â¸Ä¢Ã¤Â¸Âª', 'lp', 'Ã¦Ä°', 'Ã¨Ä©Âª', 'Ä help', 'Ä their', 'Ã¦Ä¶', 'Ã¤Â½Ä¾', 'Ã¤Â¼Ä¼', 'Ã¦Ä®', 'Ã¦ÄªÄ³Ã¤Â»Â¬', 'nt', 'Ã¤ÂºÄ°', 'Ã¥ÄªÄ¨', 'res', 'pe', 'Ã¥Ä©Âº', 'ide', 'Ã¦Ä¥', 'Ä H', 'Ã¨Â¾', 'Ä M', 'ff', 'Ã¦Â¯', 'od', 'ical', 'Ä wor', 'Ã¤Â¸Ä¬', 'are', 'Ã¦Ä½Â´', 'Ä your', 'Ã¤Â¸Ä­', 'Ã¨Âµ', 'ations', 'Ã¦Ä·Â°', 'Ä te', 'Ã¥Ä°', 'Ã§Ä²Ä¨', 'Ä Th', 'Ã¨Â¿Ä©', 'Ã¥Â¹Â¶', 'du', 'Ã©Ä¿Â¢', 'Ä ad', 'ill', 'Ã¦Âµ', 'Ã¥Â¥Â½', 'oc', 'act', 'Ã©Ä¾Ä¢Ã¨Â¦Ä£', 'Ã¤Â»Ä¸', 'Ã¥Â±', 'Ä r', 'Ä more', 'Ã¥ÅƒÂ¦', 'Ã§Â®', 'igh', 'Ã¤ÂºÄ½', 'Ä B', 'Ã¥Ä¬Â¨', 'Ã¥ÄµÄ£', 'Ã¨Ä«', 'ple', 'Ä inc', 'Ã¥Ä²Ä®', 'Ä exp', 'ould', 'Ã¤Â½Å‚', 'Ã¦Ä¯', 'Ã¦Ä±Ä²', 'Ã¥Â¤Â§', 'Ã§Ä°Â°', 'pt', 'Ä P', 'all', 'Ã¥Ä¬Å‚', 'Ã§Â§Ä¯', 'Ä se', 'Ã¥Ä¬Ä½', 'out', 'Ä have', 'Ã§Âº', 'Ã¤Â½Äµ', 'Ä prov', 'Ã¥Ä®Ä¸', 'Ã¥Â¤Ä¼', 'Ã¥Â®Ä¼', 'Ä used', 'Ã©Ä¢Ä¼', 'cc', 'Ã¨Â¿Ä½', 'Ã¦Â´', 'Ä sh', 'Ä ab', 'os', 'Ä res', 'Ä This', 'Ã§Â¨', 'Ã¦Ä¢Â§', 'age', 'ri', 'Ã¦Â¸', 'able', 'Ã¥ÅƒÄ²', 'Ä by', 'Ã¥Ä±Ä³', 'Ã©Ä©Ä±', 'Ã¥ÂºÄ¶', 'Ä lo', 'Ã¤Â½Â¿', 'Ã¥Ä§Â¶', 'Ã©Â«', 'Ã©Ä»', 'Ã©Â«Äº', 'Ã¥ÂºÂ¦', 'Ã¨Â§Â£', 'Ã©Â£', 'Ã¥Â°Ä¨', 'Ã¦Â³Ä·', 'and', 'Ã¤Â¿Ä¿', 'ans', 'for', 'rom', 'reat', 'Ä pl', 'Ã§Ä¼Ä¦Ã§', 'Ã¥Â¸Â¸', 'Ã¨Â½', 'Ä we', 'Ã¨Â¡Â¨', 'ake', 'Ã¦ÄªÄ¸', 'Ã©Â¢Äº', 'Ã¥Å', 'Ä me', 'Ã¦Ä¸Ä©', 'ther', 'ke', 'Ã¥Â®Â¶', 'Ã¥Ä²Äª', 'Ã¦Ä¾Ä¢', 'ine', 'Ä some', 'Ã§Â±', 'Ã©Ä©Ä¯', 'Ã¦Å€Ä¾', 'Ä W', 'Ä E', 'Ã©Äº', 'our', 'rou', 'Ã§Ä¤', 'Ã¦Â±', 'Ã¥Ä§Â³', 'Ä int', 'ance', 'Ã¤Â¹Å', 'Ã©Ä£', 'Ä Ä Ä ', 'Ã¥Â®Ä¥', 'ag', 'Ã¦Â¬', '00', 'Ã¨Â°', 'ult', 'yst', 'Ã©Ä¹Â´', 'Ã§Â³', 'Ä tr', 'pl', 'art', 'Ã¦Ä¦Å', 'Ã¦Ä¤', 'ata', 'Ä F', 'form', 'Ã¨Â®Â¡', 'Ä from', 'Ä D', 'Ã©Ä¹Â®', 'ight', 'ces', 'Ã¦Ä¯Â®', 'lop', 'Ã¤Â¹Ä­', 'Ä fe', 'Ã¥Ä£', 'velop', 'Ä 1', 'Ã¥Ä½Å‚', 'ks', 'Ã¦Â²', 'Ä u', 'Ã¥Â°Ä±', 'ystem', 'Ä dis', 'Ä R', 'gy', 'Ã¥Â·Â¥', 'Ã§Â¨Ä­', 'Ã¥Â¢', 'ence', 'Ã¨Ä¤', 'Ã§Â¡', 'Ä tra', 'Ã¥Â»', 'Ã¥Ä§Â¥', 'ign', 'alth', 'Ä such', 'ach', 'Ã¦Ä»', 'arn', 'Ä data', 'Ã¨Â¶', 'Ã¥Â®Å€', 'so', 'Ä develop', 'Ã§Â¤', 'Ä acc', 'ast', 'Ã¨Ä¢Ä®', 'Ä \"', 'Ä other', 'Ã¥Â»Âº', 'Ä eff', 'Ã§Â«', 'Ä man', 'Ã¥Ä§Â¬', 'Ã¥Ä¢', 'Ã§Ä¦', 'ms', 'Ã¥Â¼Ä±', 'Ã¨Ä«Â²', 'Ã¥Â¾Ä¹', 'ific', 'Ä j', 'Ä ro', 'Ä has', 'chn', 'olo', 'Ã¥ÄªÂ¶', 'Ã¨Ä¬', 'Ã¤Â½Â¿Ã§Ä¶Â¨', 'ous', 'ual', 'Ä at', 'Ä em', 'ell', 'Ä system', 'Ä health', 'ities', 'Ä exam', 'ib', 'Ã©Ä¶', 'Ä about', 'Ã¤ÂºÂ§', 'Ã¥Ä²Ä°', 'Ã¦Ä¦Ä±', 'Ã§Â±Â»', 'Ä pre', 'Ã¦Ä¤Â¨', 'Ä also', 'ents', 'Ä ind', 'ind', 'Ã©Ä¢Ä¤', 'Ä techn', 'ress', 'Ã¦Ä¥Ä§', 'Ã©Ä¹Â®Ã©Â¢Äº', 'Ä use', 'Ã¯Â¼Å', 'Ä incl', 'Ä spe', 'ich', 'ps', 'Ã¦Ä¾Âº', 'Ä they', 'ie', 'Ä how', 'Ä work', 'Ã¤Â¸Ä¼', 'Ã§Â´', 'Ä impro', 'Ä learn', 'Ã¦Ä¸Â°', 'Ã§Ä¤Â¹', 'Ä cont', 'ard', 'Ã§Ä¦Â¶', 'Ã¦Ä¾Â¬', 'Ã§Â³Â»', 'Ã§Â¡Â®', 'Ã¨Â®Â¾', 'Ã¥Ä§Â·', 'Ã©Ä¢Ä«', 'Ã¨Ä¢Ä§', 'Ã©Ä§', 'gh', '__', 'Ä not', 'Ã§Ä¾', 'Ã§Ä½Â¸', 'Ä provide', 'Ã¥Ä«', 'ional', 'Ä ens', 'Ã¤Â¸Ä°', 'Ã¨Â´Â¨', 'ential', 'Ã§Â»Ä±', 'Ã¥Â¿Ä¥', 'ang', 'Ã¦ÅƒÂ¤', 'end', 'Ä po', 'Ã¨Â¿Ä½Ã¨Â¡Ä®', 'ice', 'Ä -', 'Ä way', 'Ã¥Â·Â±', 'Ä 2', 'ime', 'Ã§Â½', 'Ã¨Ä©ÂªÃ¥Â·Â±', 'Ä un', 'bot', 'Ä includ', 'ated', 'Ã¦Â°Â´', 'Ã©Ä·', 'Ã¦Ä®Ä£', 'Ã¤Â»Â£', 'Ã©Â¡', 'Ã¦Ä«Ä¢', 'Ã§Ä¿', 'pport', 'ood', 'ike', 'ru', 'Ä comm', 'Ä L', 'Ã¤Â¿Â¡', 'Ä G', 'Ã§Å', 'Ã§Ä¶Âµ', 'Ä was', 'low', 'erv', 'Ã¥Ä®Ä§', 'Ä Ä Ä Ä Ä Ä Ä Ä ', 'Ä whe', 'dit', 'Ä which', 'Ä comp', 'Ã©Âª', 'ore', 'Ã§Â¾', 'Ä =', 'Ã§Ä«Â¹', 'iff', 'ert', 'Ã¦Ä£', 'rit', 'Ä rec', 'Ã¥Ä¨Ä§', 'Ã¦ÄºÄ°', 'ors', 'Ä pat', '----', 'Ã¦Å', 'Ä app', 'ns', 'Ã¥Ä¬Â¡', 'aly', 'ace', 'Ã¦Â´Â»', 'Ã¤Â¾Ä½', 'av', 'Ã¤Â¸Â»', 'Ä pers', 'Ã§Ä¥', 'Ã¨Â¯Â¥', 'Ä my', 'Ã§Â©', 'eri', 'Ã¨Â®Â©', 'Ã¦Ä¬Ä¢', 'Ã©Ä·Â¿', 'ack', 'Ä N', 'Ä diff', 'Ä this', 'Ã¥Ä¿', 'Ä ensure', 'Ã¥Â½Äµ', 'Ä out', 'Ä cl', 'Ä k', 'Ã©Â¦', 'ount', 'Ã§Ä°Â¯', 'Ã¥Ä¬Â©', 'Ä technolo', 'Ä these', 'ful', 'Ã©Ä¼', 'Ã¦Â·', 'Ã¤Â¸Ä¢Ã¤ÂºÄ½', 'Ä soc', 'Ã¥Â¼Ä¢', 'Ã¥Â¤Â©', 'Ä ev', 'Ä redu', 'Ä them', 'Ä (', 'Ã©Ä¥Â½', 'Ã¦ÄªÂ·', 'Ã¨Â·', 'Ã¥Ä¾Âº', 'Ã¦Â°Ä¶', 'Ä Y', 'Ã¨Â¯Åƒ', 'Ã©Ä¢Ä¼Ã¨Â¿Ä©', 'Ã¥Â±Ä·', 'Ä co', 'Ã¥Â½Â±', 'Ã§Â¬', 'Ä analy', 'Ã¦Â¯Ä¶', 'Ã¥Ä§Â¨', 'Ä improve', 'Ã§Â»Äµ', 'Ã¥Â¹Â´', 'Ã§Ä·', 'Ã§Ä¿Ä¢', 'Ä hum', 'Ä qu', 'Ã§Â®Ä¹', 'Ä O', 'Ã©Â£Å', 'ility', 'Ä systems', 'Ã¥Ä±Äº', 'ail', 'Ã§Â¼', 'Ã§Å‚', 'Ã¨Â¿Ä»Ã¤Â¸Âª', 'Ã¦Ä±Ä²Ã¤Â¾Ä½', 'ase', 'Ã¥Å€', 'ments', 'Ä pot', 'Ä any', 'Ã¤Â½Ä¨', 'Ä cons', 'Ä It', 'Ã¦Å‚Â¼', 'Ä ar', 'Ã¦Ä¾Â¯', 'Ã©Ä¿Å€', 'Ä do', 'Ä may', 'Ã¦Ä­Â©', 'ue', 'Ã©Ä¢Ä«Ã¦Ä­Â©', 'ry', 'Ã©Ä¥', 'Ä like', 'ong', 'Ã¨Ä£', '``', 'ile', 'Ã¦Â±Ä¤', 'Ä new', 'ient', 'Ä impact', 'Ã¨Â¿Äº', 'Ã¦Â³Â¨', 'Ã¤Â¹Äª', 'Ã§Ä½Â®', 'Ã¢Ä¢Ä¾', 'Ã¢Ä¢Ä¿', 'ef', 'Ã¤Â¾Ä­', 'Ä potential', 'ok', 'Ã¥Ä±Â¯Ã¨Ä¥Â½', 'Ä trans', 'Ä act', 'Ã¯Â¼Ä«', 'Ä spec', 'Ã¦Â¶', 'Ä will', 'Ã¤ÂºÂ¤', 'ize', 'Ã§Â¾Ä°', 'Ã¥Â¸Ä¤', 'Ä stud', 'pon', 'Ã¨Âº', 'Ã¤Â¸Ä¯Ã¥Ä²Ä®', 'one', 'Ã¥Â¾Äª', 'Ã¥Ä±Ä¬', 'Ã¥Â¦Ä¤Ã¦Å€Ä¾', 'Ã§Ä²Ä¥', 'ange', 'Ä need', 'Ã¥Â¤Ä¸', 'ety', 'aking', 'Ã¨Â¯Â·', 'ater', 'Ä person', 'ident', 'Ä so', 'Ä make', 'Ã¥Â¹Â³', 'Ã¥Â¤Å', 'Ã¨ÂºÂ«', 'Ã¯Â¼Äª', 'Ä inform', 'Ã¦Â¡', 'Ã¤ÂºÄ­', 'Ã¥Ä±Ä¹', 'ased', 'ild', 'Ä off', 'Ä there', 'cis', 'Ã¨Â¢', 'Ã©Ä¥Â¨', 'Ã¦Â¯Ä±', 'ract', 'ass', 'Ä learning', 'Ã¥Ä¸', 'Ã¥Â½Â¢', 'ire', 'Ã¤Â»Ä°', 'bots', 'Ã¨Ä»', 'Ã¥Â¸Â®', 'Ä des', 'Ä In', 'cess', 'Ä pe', 'ify', 'Ä who', 'Ã¤Â¹Å‚', 'Ã¦Ä¾Å', 'Ä experi', 'Ã©Ä¤', 'Ä sc', 'ep', 'Ã¤Â½Ä·', 'Ä time', 'Ã©Ä¿Å€Ã¥Â¸Â¸', 'Ã¦Ä­Â¬', 'Ã¥Ä·', 'Ã¤Â»Â¥Ã¤Â¸Ä­', 'Ã©Ä£Äµ', 'Ä commun', 'Ä could', 'ap', 'Ã¨Ä²', 'Ã¨Â°Ä¥', 'lic', 'duct', 'Ä its', 'cy', 'Ã¨Â¯Â´', 'Ä med', 'Ä col', 'ular', 'Ã©Ä©Ä¯Ã¨Â¦Ä£', 'Ä sp', 'Ã¥ÄªÂ©', 'Ã¨ÂµÂ·', 'Ä provid', 'ices', 'Ã¥Ä»', 'Ã¦Ä¸Ä»', 'Ä import', 'ural', 'Ã¥ÅƒÄ¹', 'Ä und', 'int', 'Ä over', 'Ã¥Ä±Â¸', 'Ã¦Å‚Â¹', 'Ã©Â¥', 'ples', 'Ã¤Â»Ä¸Ã¤Â»Â¬', 'gra', 'uring', 'now', 'Ã¥Ä¯Ä·', 'Ã¨Â¿Ä»Ã¤ÂºÄ½', 'Ã¥Ä«Ä¯', 'Ã¥Â®Ä«', 'Ä pr', 'Ã¥Ä®Ä§Ã¦Ä­Â¬', 'Ã§Â»Ä»', 'The', 'Ã¤Â½Ä¯', 'Ã¥Â§', 'Ã§Â´Å‚', 'Ã¥Ä³Äº', 'Ä ident', 'Ã¥Å€Ä­', 'Ä add', 'Ã¥Â¼Âº', 'Ã¦ÄºÂ¯Ã¤Â¸Ä¢', 'ip', 'gor', 'Ä support', 'ne', 'Ä differe', 'Ã¥Ä§Ä¥', 'Ä ass', 'Ã¥Ä¨Â³', 'Ã©Ä½', 'Ã¥Ä²Ä¯', 'Ä go', 'Ä technology', 'Ã¦Ä¢Â»', 'Ã¨Â®Â®', 'Ä inter', 'Ä inv', 'Ä our', 'Ã¦Ä·Äª', 'ustom', 'Ä rel', 'ife', 'Ã¥Ä»Â¨', 'ings', 'Ã¤Â»Â·', 'Ä part', 'Ã¨Â¢Â«', 'Ã¦Ä«Ä­', 'ary', 'Ä respon', 'ÄŠÄ Ä Ä ', 'Ã¥Â¥Â½Ã§Ä¼Ä¦', 'ative', 'Ã¥Â¸Â®Ã¥Ä¬Â©', 'Ã§Â»Å', 'Ã¦Ä¶Â¾', 'Ä Here', 'Ã§Ä£', 'Ä but', 'Ã¦Ä£Â¯', 'Ã¦ÅƒÂ£', 'ark', 'Ã¥Ä§Â¬Ã¥Ä±Â¸', 'ory', 'Ã¥Â¢Ä¥', 'lect', 'Ã©Å', 'Ã¦Ä¥Â³', 'Ã©Â£Ä°', 'ating', 'Ä am', 'its', 'Ã¦Â»', 'gorith', 'Ã¥ÄµÄ¯', 'ures', 'Ä effect', 'Ä should', 'Ä per', 'Ã¨Â±', 'Ã§Â²', 'ict', 'Ä algorith', 'uc', 'rough', 'Ã¤Â»Â»', 'Ã¤Â»Â¶', 'Ä bet', 'ia', 'Ä analyz', 'Ã¦Å‚Â¹Ã¦Ä¯Â®', 'ized', 'Ã¦ÂµÄ£', 'Ã¨Â§Ä¤', 'Ã¨Â£', 'Ã¦Å‚Ä©', 'iron', 'Ä custom', 'Ä reg', 'Ä personal', 'Ã¨Ä¥Â½Ã¥Â¤Å', 'ics', 'ivid', 'Ã§Äª', 'Ã¨ÂµÄ¦', 'Ã¦ÅƒÂ¥', 'Ã¥Â®Â¹', 'Ã¥ÄªÄ½', 'Ã¨Äª', 'Ã¤Â¹Ä²', 'Ã¥Â¯Â¼', 'gan', 'Ã¨Ä¬Ä¤', 'Ä all', 'ens', 'ame', 'ness', 'Ä up', 'Ä U', 'Ã¨Ä¢Ä¥', 'elf', 'Ã¥Ä¢Â¼', 'Ã¥Â°Ä³', 'Ã¦Ä¾Ä¯', 'ari', 'thical', 'viron', 'Ã¨Ä¥', 'ord', 'Ä sign', 'Ã©Ä©Ä®', 'ound', 'ople', 'Ã¥ÅÂº', 'Ä information', 'Ä identify', 'Ã¥Ä½Å€', 'Ä cre', 'Ã©ÅÂ³', 'ible', 'ub', 'Ã¨Â¿Ä²', 'Ä lead', 'Ã¦Â¸Â¸', 'Ã¦Â¬Â¡', 'Ã¥Ä¨Ä»', 'Ã©Ä¤Â£', 'get', 'Ã¨Ä¯', 'Ä example', 'Ã¤Â¼Äº', 'Ã¥Â½Â±Ã¥ÄµÄ¯', 'ish', 'xt', 'Ã¦Âº', 'Ã©ÂªÄ®', 'ob', 'Ã¥Â®Â¢', 'Ã¥Â¤Ä©', 'Ã¥Ä£Â¥', 'Ã¨Â½Â¦', 'Ã§Â¤Â¾', 'ividual', 'ered', 'les', 'Ä environ', 'Ä people', 'Ã¦ÄºÅ', 'Ã§Ä¸', 'Ã§Ä­', 'Ä det', 'Ã¦Ä¹Å‚', 'Ä if', 'ose', 'ite', 'Ã¥Â¢Å€', 'Ã©Ä´', 'Ã¥Ä²Ä®Ã¦Ä¹Â¶', 'Ã¨Â¿Â°', 'Ã¦Ä¸Â¹Ã¥Â¼Ä±', 'Ã¥Ä½Â½', 'Ã©Â»', 'Ã¥Â¤Ä¦', 'Ä examples', 'Ã¦Â®', 'Ä into', 'Ã¦Ä®Ä©', 'Ä human', 'Ã¥Ä²Ä³', 'Ã§Â¤Âº', 'Ã¦Ä·Â°Ã¦Ä¯Â®', 'Ä 3', 'Ä J', 'Ã¨Ä±', 'Ã§Ä°Â¯Ã¥Â¢Ä¥', 'als', 'erst', 'Ä ethical', 'Ã§Â»Ä¦', 'Ã¤Â¼Å‚', 'Ä different', 'Ä know', 'Ã¥ÂºÄ±', 'Ä individual', 'Ã¦Ä±Ä²Ã©Â«Äº', 'round', 'Ã¥Â°Â±', 'Ã¥Ä±Ä¸', 'Ã¥ÅƒÄº', 'Ã¤Â¸Â¤', 'Ã§ÅÂ¥', 'ources', 'ck', 'Ã¥Â£', 'ines', 'Ã¨Â¾Â¾', 'Ä many', 'Ã¦Ä·Â´', 'Ã¦Å‚Â·', 'ditional', 'omm', 'Ã§Ä¶Â±', 'Ã©Ä¢Å‚', 'Ã¥Â®Ä¥Ã¤Â»Â¬', 'ues', 'Ä ment', 'Ä important', 'Ä opt', 'Ä loc', 'ph', 'Ä process', 'Ä algorithms', 'Ã¨Â®Â¾Ã¨Â®Â¡', 'Ä social', 'very', 'Ã¥ÄªÄ»', 'Ã¤Â¾Ä­Ã¥Â¦Ä¤', 'Ã¨Â®Â¤', 'Ä aut', 'Ä serv', 'gg', 'Ã¤ÂºÂ§Ã¥ÄµÄ£', 'Ã¨Â§Ä¦', 'Ã§Ä¾Ä­', 'vel', 'Ã¦Ä¸Â¹Ã¦Â³Ä·', 'Ä ben', 'Ã¥Ä½Å‚Ã¦ÅƒÂ¤', 'care', 'per', 'Ã¥Ä¬Å', 'Ã¥Â»ÂºÃ¨Â®Â®', 'Ä pos', 'Ã¦Â¤', 'we', 'Ã¥Ä®Âº', 'iqu', 'Ä real', 'Ã¦Ä¹Â¥', 'Ä reduce', 'af', 'angu', 'Ä sk', 'Ä ed', 'erstand', 'Ã¥Ä¨Âµ', 'mot', 'Ã¥Ä§Äª', 'Ã§Â¥', 'Ã¥ÂºÄ¶Ã¨Â¯Â¥', 'Ä through', 'Ä conc', 'Ã¥Ä±Ä³Ã¥Â±Ä·', 'Ã¨Â¯Ä·', 'Ã¦Â¡Äª', 'Ä environment', 'Ã¥Ä±Â£', 'Ä adv', 'Ã¥ÄªÂ«', 'Ä benef', 'Ã¦Â¸Ä§', 'Ã¥Ä³Â³', 'Ã¥Ä§Ä«', 'Ä development', 'eng', 'Ã¥Â¦Ä¤Ã¤Â½Ä·', 'Ã§Â®Â¡', 'ivers', 'Ã¥Ä²Ä¦', 'Ä ris', 'row', 'ergy', 'Ã¨Â®Â¡Ã§Â®Ä¹', 'Ã¤Â¿Â¡Ã¦Ä£Â¯', 'Ä product', 'Ã¨Â¾Ä¥', 'Ã¨Â®Âº', 'Ã¨Ä©ÂªÃ¥Â·Â±Ã§Ä¼Ä¦', 'Ã¦Ä¬Â¤', 'Ã¥Ä±Ä¯', 'Ã¥Ä§Â¶Ã¤Â»Ä¸', 'Ã¥ÄªÄ¹', 'Ã§Â»Ä¨', 'Ã§Â©Âº', 'Ä great', 'ear', 'Ã¦ÂºÄ²', 'ject', 'Ã§Ä¶ÅÃ¦Â´Â»', 'Ã¤Â¸ÅƒÃ§Ä¼Ä¦', 'Ä understand', 'Ã¨Ä­', 'hat', 'Ä progra', 'Ã§Ä¬', 'Ã©Ä©Ä³', 'Ä including', 'Ä access', 'Ä Ä Ä Ä Ä Ä Ä ', 'Ã¨Â¯Ä¨', 'Ã§Â¦', 'og', 'Ã¨Â£Ä§', 'Ä art', 'Ä writ', 'Ä incre', 'Ä ph', 'Ã¦Ä¸Â¹Ã©Ä¿Â¢', 'Ä pract', 'Ä using', 'Ã©Â¡Â¹', 'Ã¦Ä°Â¥', 'Ä ways', 'Ä langu', 'Ã¦Ä¶Â¯', 'Ä chall', 'Ã¥Ä°Â»', '____', 'imate', 'Ã¦Ä¸Åƒ', 'Ã¨Â¨', 'Ä well', 'll', 'Ä pol', 'Ã¦Ä¢Ä£', 'Ä ra', 'Can', 'Ã¥Ä°Å', 'ber', 'Ã¨Â¨Ä¢', 'Ã§Â«Ä­', 'Ä gen', 'Ã©Ä§Ä¯', 'Ã¦Â·Â±', 'te', 'Ã¤Â¸Ä«', 'Ã§Â§Ä³', 'Ä For', 'Ã§ÂºÂ¿', 'Ã§Ä§', 'Ã¦Â¼', 'Ã¥Ä·Ä¨', 'Ã¦Ä¿Ä²', 'Ä signific', 'Ä gu', 'Ä decis', 'Ä train', 'Ä ag', 'Ä creat', 'Ã¥Â®Ä®', 'Ã¦Ä¹Â¶Ã©Ä¹Â´', 'Ä one', 'Ã¨Ä¦', 'Ä nat', 'Ã¥ÅƒÂ¦Ã¤Â¹Å‚', 'Ã§Ä¼Ä¦Ã¦Ä·', 'ced', 'Ä when', 'Ä bi', 'Ã¨Ä°', 'Ã¦Ä½Â´Ã¥Ä¬Å‚', 'ives', 'port', 'Ã¥Â·Â¥Ã¤Â½Ä¾', 'ving', 'Ä been', 'Ã¦Ä»Âº', 'Ä life', 'Ã¥Â¼Ä·', 'arm', 'Ã§Ä°Ä©', 'Ã§Ä¶Â¨Ã¦ÄªÂ·', 'Ã¤Â¹Ä«', 'Ã¤Â»Â½', 'Ã¨Â¯Ä¿', 'iness', 'com', 'Ã¥ÂºÂ·', 'Ã¥Ä©Ä±', 'Ã¤Â»Ä¢', 'Ã¨Â¾Äµ', 'Ä vari', 'con', 'Ä mod', 'Ã¤Â»Ä¢Ã¤Â¹Äª', 'Ä energy', 'Ã¦Ä¬Ä¢Ã¦Ä¾Â¯', 'ertain', 'mm', 'verall', 'Ã¥ÄªÄ´', 'Ä robots', 'Ä organ', 'Ã¦Ä°Â¨', 'ants', 'Ã¥Ä©Ä¨', 'ds', 'Ã¦Å€Ä£', 'Ã§Ä»', 'Ä requ', 'Ä ess', 'Ã§Â®Ä¢', 'ustain', 'Ã¦Â¨', 'Ä str', 'cing', 'ability', 'ree', 'Ä educ', 'Ã¥Ä°Ä¨', 'Ä create', 'Ã¥Ä£Â¥Ã¥ÂºÂ·', 'Ä design', 'ips', 'Ã¥Ä£Ä¼', 'Ã¨Ä¬Â±', 'ink', 'Ã¨Ä±Ä¾', 'Ã¦Ä«Â¾', 'Ã¦Â®Âµ', 'Ã¦ÂµÄ­', 'Ä V', 'Ä By', 'Ã¥Ä¶', 'Ã©Â¦Ä¸', 'Ã¨Â¯Ä¯', 'Ä where', 'Ä disc', 'Ã¤ÂºÄ¨Ã¨Â§Â£', 'ric', 'Ã¤Â¸Ä¶', 'Ã¨Â¶Â³', 'Ã¦ÄºÂ¯Ã¤Â¸Ä¢Ã¤Â¸Âª', 'arch', 'Ã§Â§Â¯', 'Ã¥Â¸Â¦', 'Ä while', 'Ä significant', 'Ã§Å‚Ä£', 'Ã¦ÄªÂ¿', 'Ä being', 'Ä language', 'itive', '20', 'Ä analyze', 'Ã¦Ä»Â¯', 'Ã¨Ä®', 'rib', 'Ã¦Â¨Â¡', 'Ä St', 'Ã¨Â´Â¹', \"'t\", 'Ä healthcare', 'Ä experience', 'Ä 5', 'Ã¤Â¸ÂªÃ¤ÂºÂº', 'ays', 'Ã¨Â±Â¡', 'plo', 'Ä would', 'Ã¨Ä»Ä³', 'Ã¦Ä¶Â¶', 'Ã©Â¢Ä¦', 'Ã©Â¢Ä¨', 'Ã¤Â¿Ä¿Ã¦Ä®Ä£', 'ences', 'Ã¥Ä±Âª', 'Ã¨Ä©Â´', 'Ã¦ÄªÄ±', 'Ä mental', 'Ä few', 'ates', 'Ã¨Â¿Ä©Ã§Â¨Ä­', 'Ã¥Â®Ä«Ã¥Ä§Â¨', 'Ä sustain', 'Ä were', 'Ã¥Â¤Âª', 'Ã§Ä®', 'Ä specific', 'Ä world', 'Ã§ÅƒÄ¶', '```', 'Ä take', 'Ã¥Ä§Â»', 'Ã©Ä¢Å', 'ever', 'SS', 'Ã©Ä¶Ä¢', 'Ä bo', 'hes', 'Ä mus', 'Ã¦Ä¾Ä¯Ã¥Ä¬Â¡', 'Ã¨Â§Ä´', 'ten', 'Ã¦Å€Ä²', 'pow', 'dict', 'vent', '10', 'Ã§Ä¼Ä¦Ã¦Ä¹', 'Ä¸Ã§Ä·', 'Ä prot', 'Ã§Â½Â®', 'Ä high', 'Ä bus', 'Ä indust', 'Ã¥Ä²Â¦', 'cial', 'Ã¤ÂºÂºÃ¤Â»Â¬', 'Ä As', 'Ã¥Ä³Ä¬', 'ade', 'Ã¦Ä¶Â¹', 'Ã§Ä¹', 'Ä had', 'Ä her', 'Ä just', 'Ã¯Â¼Ä½', 'Ã¨Â´Åƒ', 'Ã§Â¬Â¬', 'Ã©Äµ', 'Ä water', 'Ä food', 'Ã©ÄºÅ', 'aus', 'Ä challeng', 'Ã¥Ä§Ä¯', 'Ã¦Ä¸Ä©Ã¥Ä®Ä¸', 'Ä most', 'Ã©Â¸', 'Ã§Â½Ä³', 'Ã§Ä½Â´', 'Ä sm', 'Ä activ', 'ploy', 'Overall', 'Ã¥Â¿Â«', 'ruct', 'Ä individuals', 'Ã¥Â§Ä­', 'gies', 'Ã¦ÅÂ¥', 'Ã§ÄªÂ±', 'iety', 'In', 'Ã¥ÄªÄ¨Ã¦Å€Ä²', 'Ã¨Â§Ä¨', 'Ã¦Â¸Â©', 'Ã§Â»Â´', 'olut', 'Ã¥ÅÅ', 'ommend', 'Ä comple', 'Ã¦Ä·Ä»', 'Ä bu', 'Ä education', 'ather', 'Ä 4', 'ting', 'Ä find', 'Ã¦Â²Â¡', 'Ä his', 'Ã¤Â¹Ä­Ã©Ä¹Â´', 'Ä effective', 'Ä att', 'Ä rese', 'Ã¨Ä¥Â½Ã¥Ä¬Ä½', 'Ã¥ÅÄ°', 'Ä allow', 'Ä av', 'Ä promot', 'Ã¦Ä»ÂºÃ¨Ä¥Â½', 'Ã¦Â»Â¡', 'Ã¥Ä§Â±', 'iew', 'come', 'Ã§Â³Â»Ã§Â»Å', 'Ä respons', 'Ã¤ÂºÄ´', 'Ä cult', 'powered', 'Ä recommend', 'Ã¨Ä²Â¥', 'OSS', 'Ä change', 'Ã¨Â¯Ä£', 'ved', 'Ã¦Ä°Ä´', 'Ã¨Â§Â£Ã¥Ä¨Â³', 'ici', 'Ä How', 'Ä feel', 'Ã¦Ä¾Äª', 'Ä what', 'Ã¤Â»Â¥Ã¥Ä±Ä¬', 'Ä see', 'Ã¥ÅƒÂ©', 'bs', 'Ä sur', 'Ã¦Â£', 'ality', 'Ä vis', 'Ã§Â¡Â®Ã¤Â¿Ä¿', 'pect', 'Ã¥Â®Å€Ã§Ä°Â°', 'Ä care', 'Ã¥Â¹Â¿', 'ills', 'Ã¥ÂºÅƒ', 'ases', 'Ã¥Â¤Ä¯', 'Ã¥ÂºÄ¶Ã§Ä¶Â¨', 'Ã§Ä¼Ä¦Ã¦Ä¥', 'ards', 'Ä address', 'Ä compan', 'Ä invol', 'Ä customer', 'Ã¥Ä½Å‚Ã¤Â¸Âº', 'Ä students', 'Ä ins', 'Ã¦Â³Â¨Ã¦Ä¦Ä±', 'Ã¦Å€Ä¦', 'Ã¦Â¬Â¢', 'Ã¦ÂµÂ·', 'Ã¥Ä±Ä¤', 'Ã¨Ä©ÂªÃ§Ä¦Â¶', 'Ã©Â©', 'Ä These', 'wn', 'Ã¦ÄºÄµ', 'Ã§Ä¬Â¶', 'ren', 'Ä treat', 'Ä benefits', 'ÄŠÄ Ä Ä Ä Ä Ä Ä ', 'Ã¥Â¯Â¹Ã¤ÂºÄ°', 'Ã¦Ä¢Ä¿', 'ider', 'Ä Yes', 'Ä K', 'Ã¥Ä¸Ä¾', 'Ä ke', 'Ä eng', 'Ä pop', 'ost', 'pare', 'Ä mon', 'Ã¦Â¬Â¾', 'Ä MOSS', 'Ä emot', 'Ä ac', 'Ã§Â¼Ä¸', 'fore', 'Ã¥Ä±Â¥', 'Ä val', 'ily', 'Ä iss', 'Ã¨Ä¤Ä«', 'Ã¨Ä©Â³', 'Ã¦Â¸Â¸Ã¦ÄªÄ±', 'ween', 'Ä include', 'Ä protect', 'Ã¥Ä§Â³Ã§Â³Â»', 'Ã©Ä»Â©', 'Ä sever', 'Ä than', 'Ã©Ä¾Ä¢Ã¦Â±Ä¤', 'Ã§Â»Ä¥', 'Ä They', 'iss', 'ys', 'Ä job', 'Ã©ÄºÂ³', 'Ã¦Ä²', 'Ä between', 'Ä mach', '--------', 'Ã¨Ä¢Ä¥Ã¨Ä»Ä³', 'Ã¨Â´Â¨Ã©Ä©Ä±', 'Ä business', 'wor', 'ick', 'eg', 'Ã¥Ä§Ä§', 'Ã§Â¯', 'Ã¦Ä¿Â¡', 'ner', 'apt', 'Ä appro', 'Ä play', 'Ã¦Â²Â¡Ã¦Ä¾Ä«', 'Â¤Ä²', 'Ã¦Ä¾Âª', 'Ã¦ÄªÄº', 'Ã¥Â®Â¶Ã¥ÂºÅƒ', 'Ã£Ä¢Ä­', 'ency', 'Ä Ch', 'Ã£Ä¢Ä¬', 'Ä providing', 'Ä resources', 'Ã¢Ä¢Ä»', 'Ä assist', 'Ä natural', 'Ã¨Â¯Ä¦', 'Ã¤Â¾Â¿', 'Ä saf', 'Ã¥Ä§Â·Ã¦Ä¾Ä«', 'Ã¨Â°Â¢', 'Ã§Ä¥Åƒ', 'ss', 'eth', 'old', 'Ä perform', 'Ä several', 'Ã©Â¤Ä²', 'Ä each', 'Ã¨Â½Â¬', 'ci', 'Ä ty', 'Ä pub', 'Ã¦Â´Â»Ã¥Ä¬Â¨', 'ocus', 'Ã§Ä«Ä®', 'Ã¨Â¶Ä¬', 'Ã¥Ä½Â¢', 'Ã¨Â½Â»', 'Ã¨Â¯ÅƒÃ¨Â¨Ä¢', 'Ä areas', 'Ã©Ä©Ä©', 'ft', 'riend', 'Ã¥Â·Â²', 'Ã¥Â¸Ä¤Ã¥Ä¾Âº', 'ition', 'ients', 'Ã§Â®Â¡Ã§Ä²Ä¨', 'Ã¨Â®Â¸', 'Ã¤ÂºÂºÃ§Â±Â»', 'Ã¨ÂºÂ«Ã¤Â½Äµ', 'ique', 'Ä partic', 'Ã§Â»Åƒ', 'agement', 'ves', 'Ã§Â¬Â¦', 'line', 'Ã§ÂºÂ¢', 'Ã¥Ä²Â¸', 'Ä patter', '000', 'Ã§Â¤Â¾Ã¤Â¼Ä¼', 'Ã¥Ä¨Ä§Ã¥Â®Â¹', 'Ä organiz', 'ough', 'Ä ve', 'Ã¥ÅƒÂ©Ã¥ÅƒÄ²', 'Ã¦Ä¸Â½', 'Ã¦Â¤Ä¯', 'Ã¥Ä©Å‚', 'Ã¤Â½Ä¨Ã¦ÄºÂ¯', 'Ä aff', 'Ä num', 'lement', 'Ã¨Ä«Âº', 'Ã¨Ä³', 'Ä car', 'ages', 'abor', 'Ã¦ÄºÂ¯Ã¤Â¸Ä¢Ã§Â§Ä¯', 'Ä inst', 'Ã¨Ä½', 'Ã¤Â¹Ä­Ã¤Â¸Ä¢', 'Ã¨Â·Â¯', 'Ã¥Ä¯Â³', 'Ä main', 'Ã©Ä¼Ä±', 'How', 'Ã¥Â¿Ä§', 'Ã§Â¨Ä­Ã¥ÂºÄ±', 'Ã©ÅÂ³Ã¤Â¹Ä²', 'red', 'Ã¦Â²Â¹', 'Ä offer', 'ets', 'Ã§Â¢', 'Ä during', 'Ã§Ä¼Ä¦Ã¤ÂºÂº', 'Ã¦Ä½Â´Ã¥Â¤Ä¼', 'Ä di', 'Ã¤Â»Â£Ã§Å‚Ä£', 'Ã¨Ä°Â·', 'Ã¥Ä§Ä­', 'Ä guid', 'Ã¤Â¸Â»Ã¨Â¦Ä£', 'Ä fam', 'Ã¦Ä°Â§', 'Ã©Ä¢Ä¼Ã¥Â¸Â¸', 'Ä Ad', 'Ã¥Â¤Ä¦Ã§Ä²Ä¨', 'urn', 'ower', 'Ã¥Ä³Â½', 'Ã¦Ä±Ä±', 'Ä skills', 'Ä tool', 'ware', 'Ã¦Ä¸Ä©Ã¦Ä¾Â¬', 'Ä patterns', 'Ã§Ä½Â®Ã¦Å‚Ä©', 'acy', 'Ã¦Ä«Äµ', 'Ã¥ÅÄ°Ã¥Â¸Ä¤', 'Ä every', 'ries', 'Ã¨Â¯Â»', 'Ã©Ä£Â¿', 'Ã§Ä»Â½', 'Ã©Ä¢Ä¤Ã¥Ä²Äª', 'Ä patient', 'Ã§Ä¾Å', 'oth', 'Ã¥Â¥Â¹', 'Ã¥Ä¶Â®', 'Ã¤Â¸Ä¢Ã§Â§Ä¯', 'Ä made', 'Ã¤Â½Ä°', 'ise', 'Ä rem', 'Ã¦Â¶Äª', 'Ã¥Ä²Â«', 'air', 'Ä gener', 'oy', 'Ã§Â²Â¾', 'Ã¦Ä¥Ä§Ã¥Ä¨Âµ', 'ights', 'Ä expl', 'Ã¨Â§Ä£', 'Ä predict', 'Ã§Â±Â³', 'Ã¦Ä½Â´Ã¥Â¥Â½', 'Ã¤Â¿Â®', 'Ä climate', 'Ä focus', 'Ä grow', 'Ã¥Â®Â¢Ã¦ÄªÂ·', 'Ã¤Â¸Ä¯Ã¦Ä¸Åƒ', 'itor', 'Ä En', 'Ã§ÂºÂ¦', 'Ã¦ÄºÂ¯Ã¥Ä²Â¦', 'Ã¤Â»Ä§', 'Ã¦ÄªÄ³Ã¤Â»Â¬Ã§Ä¼Ä¦', 'Ã¦Ä¾Ä½', 'op', 'Ä making', 'yth', 'ccess', 'Ä own', 'ggest', 'Ä tas', 'uture', 'Ä model', 'put', 'Ä research', 'erest', 'Ã©Ä¼Â¾', 'Ä [', 'iel', 'ational', 'Ä communic', 'Ã§Â¥Å€', 'Ã§Â©Â¶', 'Ä rest', 'Ã¦ÄªÄ²Ã¤Â¸Âº', 'king', 'pr', 'Ã¥Ä®Â»', 'cur', 'Ã¨Ä¤Â²', \"Ä '\", 'Ã¨Â¿Ä»Ã§Â§Ä¯', 'Ã§Â¯Ä©', 'Ä che', 'own', 'Ã©Ä»Ä§', 'Ä fin', 'Ã¥ÄªÂ¶Ã¤Â½Ä¾', 'Ä suggest', 'Ã¥Â¢Å€Ã¥Ä¬Å‚', 'Ä media', 'ribut', 'Ã§Ä¼Ä¦Ã¦Ä¥Ä§', 'Ã¥Ä¬Å‚Ã¥Ä§Â¥', 'Ä cle', 'Ã¥Ä³Â¨', 'Ã§Â«Å‚', 'Ä think', 'Ä local', 'pportun', 'Ä You', 'Ä plan', 'Ä even', 'Ã©Ä½Ä¨', 'Ã¥Â·Â§', 'ax', 'Ä challenges', 'Ä prof', 'Ä Can', 'Ä concer', 'Ä future', 'Ã¥Ä¬Â¿', 'Ä ref', 'Ã¨Ä£Ä¶', 'Ä self', 'Ã¦ÄªÄ¸Ã¨Ä¢Ä§', 'ble', 'Ã¥Ä½Â´', 'Ã¨Â¿Ä²Ã¥Ä¬Â¨', 'Ä inf', 'Ã©Ä©Ä¬', 'Ä sustainable', 'Ä text', 'Ä gra', 'Ã¤ÂºÄ®', 'Ã¥ÄµÄ£Ã§Ä«Ä®', 'Ã¤Â¸Ä¯Ã¥Ä²Ä®Ã§Ä¼Ä¦', 'led', 'Ã§Ä­Â¬', 'Ä opportun', 'Ä contin', 'ym', 'Ä get', 'Ã¥Â¯Ä¨', 'Ã©Ä»Â¤', 'Ã¦Ä§', 'Ã©Ä£Â¿Ã¥Ä§Ä¯', 'Ä +', 'Ã¨Â§Ä«', 'Ä ret', 'Ã¥Â¸Ä¥', 'Ä interest', 'Ä society', 'Ã§Â»ÄµÃ¦Å€Ä¾', 'Ã¥Ä²Â¬', 'Ã©Â¦Ä¸Ã¥Ä§Äª', 'Ä bre', 'Ä 20', 'Ä However', 'Ã¨Â®Â°', 'ons', 'Ã¨Â¿Ä³', 'Ã¥Â¼Ä¢Ã¥Â§Ä­', 'Ä build', 'Ä beh', \"'m\", 'vers', 'Ä good', 'Ã§Ä²Ä¨Ã¨Â§Â£', 'resent', 'Ã§Â¦Â»', 'Ã¥Ä¬ÅÃ¨Ä¥Â½', 'Ä effort', 'labor', 'Ã©Â»Ä³', 'Ä better', 'Ä read', 'Ã¥Â¾Ä­', 'Ã¨Ä½Ä­', 'hed', 'Ã¤Â¹Â°', 'Ã¥Â¯Â¼Ã¨Ä©Â´', 'Ä implement', 'Ã§Â¿', 'Ã¤ÂºÂ«', 'Ã¥Â¤Â´', 'ense', 'Ä long', 'other', 'Ã©Â¥Â®', 'Ã¥ÅƒÄºÃ¥Ä¾Â¨', 'Ã§Ä¼Ä¦Ã¦Ä¦', 'Ã¤Â¸Ä¢Ã¤Â»Â½', 'ython', 'ning', 'Ã¥Ä©Ä±Ã¥Â°Ä³', 'Ã¥Ä¢Ä»', 'Ã¤Â¸Äµ', 'Ã¥Ä²Ä¦Ã§Â§Ä¯', 'Ã¨Ä§', 'Ã¥Â°Â½', 'Ã¥Ä¯Ä©', 'Ã¦Ä¬Â¥', 'Ä public', 'Ä lar', 'Ã¤Â½Å‚Ã§Ä¼Ä¦', 'aut', 'Ã©Â¢Ä¨Ã¥ÅÅ', 'Ã¦Ä¼', 'ollow', 'Ã¨Ä£Ä®', 'Ä chang', 'Ä best', 'hip', 'Ã¥Ä¨Ä¯', 'akes', 'Ä chat', 'ited', 'Ä power', 'Ã¤Â¿Ä¿Ã¦Ä¬Â¤', 'Ã¤Â¹Â¦', 'Ã¨Â®Â¡Ã¥ÄªÄ´', 'Ã©Ä©Ä¯Ã¨Â¦Ä£Ã§Ä¼Ä¦', 'Ã¥Ä±ÄºÃ¥Ä®Ä¸', 'ilities', 'Ä consider', 'Ã¦ÄªÄ³Ã¤Â»Â¬Ã¥Ä±Â¯Ã¤Â»Â¥', 'Ã©Ä¤Â£Ã¤Â¹Äª', 'Ä ide', 'Ã¦Â¼Ä¶', 'aging', 'Ä based', 'Ã¥Â®Ä¿', 'Ä range', 'Ä result', 'Ä mem', 'Ã§Ä§Â§', 'Ä level', 'cou', 'Ä br', 'Th', 'Ã¤Â¼Ä£', 'Ã¥Â»ÂºÃ§Â«Ä­', 'Ä unique', 'Ã¨Â®Åƒ', 'Ä mark', 'Ã¨Â®Â¸Ã¥Â¤Ä¼', 'Ã¨Â¡Ä®Ã¤Â¸Âº', 'Ä¶Ã§Â©Â¶', 'Ã§Ä¼Ä¦Ã¦Ä¬', 'Ä set', 'Ã©ÂªÂ¤', 'ts', 'Ä hist', 'Ä around', 'Ä rev', 'Ã¥Ä§Â¶Ã¤Â¸Åƒ', 'Ã¯Â¼Ä£', 'Ã¦Ä±Ä±Ã¨Â¿Â°', 'Ã¦Ä¾Ä¢Ã¥Ä²Ä°', 'Ä sim', 'nect', 'Ã¥Ä½Å€Ã§ÅƒÄ¶', 'Ã©ÄºÂ²', 'Ã¨Ä«Â¯', 'Ã¥ÄªÂ°Ã¤ÂºÄ¨', 'Ã¤Â¸Ä¸Ã§Ä·', 'Ã¦Ä¸Â¹Ã¦Â¡Äª', 'Ã¦Ä¿Ä²Ã¦Ä¸Ä»', 'Ã¤Â¸Ä¸Ã§Ä·Ä®', 'Ã¦Ä½Â´Ã¥Â¥Â½Ã¥Ä¾Â°', 'Ã¤Â¸Â¤Ã¤Â¸Âª', 'Ä employ', 'Ä try', 'Ã¦Äµ', 'Ä back', 'Ã¥ÄªÄ©', 'Ä success', 'Ä decisions', 'Ä those', 'Ã¥Â¯Ä®', 'Ä fact', 'Ã¦Ä°Â¢', 'Ã¨Â¶Â£', 'Ä practices', 'Ã¥Ä²Ä¹', 'Ã¦Ä«Ä¯', 'Ã§Ä°Â©', 'ption', 'Ã¦Ä¸Ä©Ã§Â«Å‚', 'Ä feat', 'Ä prevent', 'Ä writing', 'Ã§Ä¼Ä¦Ã¦Ä¢', 'Ä no', 'Ã¤Â»Ä­', 'Ã©Ä¹Â¨', 'Ä del', 'Ã¦Ä´', 'Ä optim', 'ination', 'Ä ÄŠ', 'usion', 'Ä account', 'ling', 'Ä divers', '.\"', 'ath', 'Ã¨Ä­Â±', 'Ã¤Â¼Ä£Ã¤Â¸Ä¼', 'Ä grou', 'Ã¥Ä¾Â°Ã§Ä²Ä¥', 'Ã¥Â¤Â±', 'Ä personalized', 'Ä He', 'Ã¨Â¡Â¨Ã¨Â¾Â¾', 'curity', 'Ä follow', 'Ã¤ÂºÂ§Ã§Ä¶Å', 'Ä ear', 'Ã¥Ä°Ä­', 'vern', 'Ä issues', 'Ã¥Ä¿Ä©', 'Ã©Â²', 'Ä dr', 'iving', 'Ä training', 'Ä risk', 'Ã¥Ä©Â½', 'Ã¥Ä±Â²', 'Ã¦Ä³', 'Ã§Ä¼Ä¦Ã¦Ä¹Â¶', 'ogn', 'Ä require', 'Ä environmental', 'back', 'Ã©Ä¶Â®', 'Ã§Ä¸Ä¹', 'Ä interact', 'Ã¥Ä½Â¢Ã©ÄºÅ', 'Ã¦Â¯Ä±Ã¤Â¸Âª', 'Ã§Ä¦Â¶Ã¥Ä²Ä°', 'Ä dist', 'Ã§Ä¶Â¨Ã¤ÂºÄ°', 'Ã¨Â®Â¤Ã¤Â¸Âº', 'Ã¥Ä©Â½Ã¦Ä·Â°', 'Ä sent', 'ÄŠÄ Ä Ä Ä Ä Ä Ä Ä ', 'Ä reducing', 'Ã¥Â¹Â²', 'Ä rep', 'Ä caus', 'Ä music', 'Ã§Âª', 'Ä monitor', 'Ä form', 'Ã©Â¢Ä¾', 'Ã§Ä¹Ä§', 'Ã©Â¦Ä»', 'Ä often', 'Ã¥Ä±Â¯Ã¨Ä¥Â½Ã¤Â¼Ä¼', 'Ã¥Ä³ÄºÃ¥Â·Â¥', 'Ä hand', 'Ã¦Ä¬Ä·', 'Ä needs', 'Ã¦ÅƒÂ¤Ã¥Â¤Ä¸', 'Ã¥Ä±Ä­', 'ivity', 'Ä activities', 'Ã¥Ä¸Ä¾Ã¦Â¬Â¢', 'Ä pur', 'ian', 'self', 'Ã¥Ä¬Â¨Ã§Ä«Â©', 'comes', 'Ã¥Â©', 'Ä priv', 'az', 'Ä relations', 'Ä machine', 'Ã§Ä¼Ä¦Ã¦Â°', 'Ã¤Â»Â·Ã¦Å‚Â¼', 'Ã¤Â»Â·Ã¥Ä¢Â¼', 'Ã§Â´Â¢', 'Ä feed', 'Ã¤Â¸Ä¢Ã¤Â¸Ä­', 'Ä team', 'Ä industry', 'Ã¨Â´Â¢', 'Ä Pro', 'Ä want', 'Ã§Â§Â°', 'Ä class', 'Ä love', 'Ã¥Ä§Â³Ã¤ÂºÄ°', 'Ã¨Â¾ÄµÃ¥Ä§Â¥', 'Ä transport', 'Ä complex', 'Ä year', 'Ã©Ä¶Ä¢Ã¥Ä¶Â®', 'Ã¥Â¯Â»', 'ience', 'ists', 'Ã¦Ä¶Â¯Ã¦Ä®Ä£', 'Ä mind', 'Ä fun', 'Ä char', 'Ã¦Ä®Ä«', 'Ä concerns', 'conom', 'Ã§Â®Ä¢Ã¥Ä¯Ä·', 'Ã¤Â»Â¥Ã¤Â¸Ä­Ã¦ÄºÂ¯', 'Ä start', 'Ã¥Â¹Â¶Ã¤Â¸Ä¶', 'avi', 'Ã¤Â¸ÅƒÃ¥Ä½Â½', 'Ã¥Ä§Ä¥Ã§Â´Å‚', 'Ä conf', 'Ä positive', 'Ä cur', 'Ä count', 'ery', 'Ã¥Â¡', 'Ã¥Â®Â¤', 'Ä cost', 'Ä equ', 'Ä polic', 'aste', 'aw', 'Ã©Ä§Ä´', 'coura', 'iven', 'place', 'chie', 'Ã§Ä¼Ä¦Ã¦Ä·Â°', 'Ã¥Ä½Å‚Ã§Â´Å‚', 'Ä fl', 'ism', 'Ä medical', 'Ä humans', 'Ä autom', 'ertainly', 'Ä 0', 'Ä offers', 'Ä detect', 'Ä 6', 'Ã©Â£Ä°Ã¦Å‚Â¼', 'Ä show', 'Ã§Ä£Â«', 'Ä anim', 'Ã©Â¢Ä¾Ã¨Ä«Â²', 'lease', 'ave', 'Ã¥ÄµÂª', 'Ä There', 'Ã¤Â»Â¥Ã¤Â¸Ä¬', 'Ã¦Ä¾ÂªÃ¦Ä¿Â¥', 'XX', 'Ã§Ä«Ä©', 'uch', 'Ä tasks', 'Ã¥Ä§Â·Ã¤Â½Äµ', 'Ã¦Â¤Ä¯Ã§Ä«Â©', 'Ä min', 'Ã¨Ä«ÂºÃ¦Ä¾Â¯', 'icult', 'Ä experiences', 'Ã¦Ä°Â§Ã¥ÄªÂ¶', 'be', 'Ä patients', 'Ã¥Â²', 'Ä We', 'Ä recogn', 'Ã§Ä¥Â¤', 'Ä small', 'Ã¥Ä¿Ä¹', 'Ã¥Ä¦', 'Ã¥Â¤ÂªÃ©ÄºÂ³', 'ction', 'Ä ent', 'Ã¦Ä¯Â¢', 'Ä before', 'Ä become', 'Ã¥Â·Â²Ã§Â»Ä±', 'Ã¨Â¡Â¨Ã§Ä°Â°', 'Ä explo', 'Ä achie', 'Ã¤Â»Â»Ã¥Ä¬Â¡', 'Ã¥Â¤Â§Ã§Ä¼Ä¦', 'Ä day', 'Ä found', 'Ã¥Â±Â±', 'ond', 'Ä treatment', 'pend', 'hen', 'Ä condit', 'Ã§Â¡Â®Ã¥Â®Ä¼', 'Ä businesses', 'Ä Wh', 'Ã¦Ä«Ä¢Ã¦Ä¾Ä«', 'Ä developed', 'Ã§Â»Äª', 'Ã¦ÅƒÂ¥Ã©ÂªÂ¤', 'Ä difficult', 'Ã¥Ä±Â·', 'Ä Re', 'Ã©Ä¶Ä»', 'Ä cho', 'Ä quest', 'Ä transpare', 'Ä project', 'Ä community', 'ov', 'Ã¥Â¸Äª', 'Ã¥Â¼Å‚', 'Ã¥ÄªÄ¨Ã§Â±Â»', 'Ã¤ÂºÂºÃ§Ä¼Ä¦', 'sis', 'Ã§Ä½Ä¬', 'oid', 'Ä An', 'ways', 'Ä eas', 'Ä affect', 'Ä others', 'Ä regul', 'Ã¦Ä¢Â§Ã¥Ä´Ä®', 'Ã¥Ä¸Ä¦', 'agn', 'Ã¤Â½Ä¾Ã¤Â¸Âº', 'Ã¥Ä±Â¯Ã¤Â»Â¥Ã¥Â¸Â®Ã¥Ä¬Â©', 'Ã¥Ä¦Â¿', 'Ä organizations', 'Ã©Â¸Â¡', 'Ã¥Ä§Â´', 'Ä friend', 'Ä $', 'Ä detail', 'Ä traditional', 'Ä designed', 'Ã¨Â´ÅƒÃ¤Â¹Â°', 'Ã¤Â½ÄµÃ©ÂªÄ®', 'Ã§Â»Ä¯', 'erm', 'Ä connect', 'Ã¨Â¿Ä»Ã¦Å‚Â·', 'Ä recommendations', 'Ä both', 'ÅÃ©Ä¢Ä¼', 'Ã¦Â¯Ä¯', 'Ä sit', 'Ã¤Â½Ä¾Ã§Ä¶Â¨', 'Ã¤Â»Ä­Ã§Â»Ä¯', 'Ä ste', 'Ä Sure', 'Ã¥Ä±Â°', 'Ã¦Ä¤Â¨Ã§Ä¼Ä¦', 'Ä she', 'Ä management', 'joy', 'Ã¨Â´Å', 'Ä promote', 'Ä various', '(\"', 'por', 'Ä sens', 'Ä essential', 'gether', 'ularly', 'Ã¤ÂºÄ«', 'irst', 'Ä op', 'Ä species', 'Ã§Ä°Â°Ã¥Ä¾Â¨', 'cho', 'Ä behavi', 'Ã§ÅƒÄ³', 'Ã¥Â¥Â³', 'Ä quality', 'Ä ext', 'Ã¨Â¥', 'Ã¥Â®Ä®Ã¦ÄªÄ²', 'Ã¦Ä¢Â»Ã¤Â¹Ä­', 'Ã©Ä¥Â¨Ã¥ÄªÄ¨', 'Ã¤Â»Ä°Ã¨Ä¢Ä®', 'Ã¥Ä½Â¾', 'Ä typ', 'Ä strate', 'Ã¨Â¥Â¿', 'Ä here', 'ars', 'Ã¥Â¸Ä®', 'Ã§Ä¼Ä¦Ã¦Ä¿', 'Ã¥Â°Ä¿', 'ee', 'ier', 'Ä ec', 'ically', 'ering', 'Ã¥Â¿Âµ', 'Ä De', 'Ä neg', 'Ã¥Â»ÂºÃ§ÅƒÄ³', 'Ä services', 'Ä able', 'imes', 'Ä options', 'Ã§Ä½Â¸Ã¥Ä§Â³', 'Ä sub', 'Ä decision', 'Ä Certainly', 'Ä Ã¥Ä¾Â¨', 'Ã¦Â¢', 'Ä service', '):', 'Ã¥Â¸Â¦Ã¦Ä¿Â¥', 'Ä child', 'Ã¨Â§Â£Ã©Ä©Ä¬', 'irt', 'Ã§Ä¨', 'Ã¤Â¸Ä¯Ã¤Â»Ä§', 'Ã¦Ä¿Â¾', 'Ã§Â§Â¯Ã¦Å€Ä£', 'ron', 'Ã¥Ä±Â¤', 'Ã§Å‚Ä¶Ã§Â©Â¶', 'Ã§Â²Ä«', 'hor', 'Ä profess', 'Ã§Ä¼Ä¦Ã©Ä¹Â®Ã©Â¢Äº', 'Ä opportunities', 'Ã¥Ä°Ä¨Ã¥Ä±Â²', 'Ä def', 'Ä Am', 'Ä gr', 'aur', 'Ã¥Â±Ä¤', 'Ã§ÅƒÄ¸', 'Ä popular', 'Ã¦Â´Ä£', 'Ã¥Ä±Ä³Ã§Ä°Â°', 'Ä poem', 'Ã¨ÂµÄ½', 'Ä ob', 'Ä don', 'Ä sound', 'Ä transportation', 'ious', 'Ã¥Ä±Â¦', 'Ä role', 'Ä fiel', 'Ã§Â§Ä³Ã¥ÅƒÂ¦', 'Ã¨Ä¢Ä£', 'reen', 'Ã¦Ä¾Ä«Ã¦Ä·Äª', 'Ä cor', 'Ä feedback', 'Ä technologies', 'Ã¤ÂºÂ¤Ã©Ä¢Ä¼', 'Ä adapt', \"'re\", 'ervation', 'Ä communities', 'Ã§Ä°Â°Ã¤Â»Â£', 'Ä look', 'Ä fac', 'Ã§Ä¶ÂµÃ¥Â½Â±', 'Ä collect', 'Ã¥Â¾Ä¹Ã¥ÄªÂ°', 'hips', 'Ä avail', 'eren', 'Ã¤Â¸Ä¢Ã¨ÂµÂ·', 'Ã§Ä«Ä½', 'Ä poss', 'Ä weather', 'Ä efforts', 'Â¿Ä¢', 'Ã¦Ä¹Ä§', 'oh', 'Ä collabor', 'Ã¦Ä­Â¥', 'Ã¦ÄªÄ²Ã¥Ä¬Å', 'Ã¨Ä°Â·Ã¥Â¾Ä¹', 'Ã¥Â±Ä§', 'Ä tre', 'Ä sources', 'Ä study', 'Ä programs', 'Ã©Ä»Ä²', 'Ä tips', 'Ä market', 'ally', 'Ã¥Â®Â³', 'wards', 'Ã¦Â£Ä¢', 'Ã¤Â¸Ä¢Ã§Â¯Ä©', 'rior', 'Ä top', 'Ä end', 'Ã¥Ä­', 'Ä large', 'iciency', 'Ä dec', 'Ã¥Â®Ä¼Ã§Ä¼Ä¦', 'icient', 'Ã¨Â¿Ä©Ã§Â¨Ä­Ã¤Â¸Åƒ', 'lications', 'Ã§Â¼Âº', 'Ä tour', 'Ä together', 'Ã¤ÂºÂºÃ¥Â·Â¥', 'Ä tools', 'Ã¦Ä¸Â¯', 'Ã¦Â°Ä³', 'Ã¦Ä¬Ä¬', 'Ã¤Â¹Ä­Ã©Ä¹Â´Ã§Ä¼Ä¦', 'Ã§Ä«Â¹Ã§Ä¤Â¹', 'Ä bel', 'ditionally', 'Ã¥ÄªÂ©Ã§Ä¶Â¨', 'Ã¨Â¾Â¹', 'Ã©Ä»Ä¯', 'Ä If', 'Ã©Â¢Ä¿', 'Ã¥Ä¯Ä±', 'Ã¥Â¾Ä¢', 'lish', 'Ã¨Â¯Ä«', 'ins', 'Ã¥Â¥Â¶', 'Ä econom', 'Ä invest', 'Ä Do', 'tain', 'Ã¥Ä©ÂºÃ§Ä°Â°', 'Ã§Ä¼Ä¦Ã¥Â½Â±Ã¥ÄµÄ¯', 'aterial', 'Ä sure', 'Ä pass', 'Ã§Ä¶Â»', 'Ã¨Â´Â£', 'Ã§Â»ÄµÃ¦Å€Ä¦', 'Ã¦Ä·Ä§', 'Ã¦Ä¥Ä§Ã¦Ä¦Å', 'Ã¦Â¿Ä¢', 'ellig', 'Ã¤Â¼Ä¹', 'Ã¦Â¯Ä¶Ã¨Â¾Ä¥', 'tern', 'Ä outcomes', 'up', 'Ä beaut', 'read', 'Ã§Ä¶ÅÃ¦ÄªÄ²', 'Ã¦Ä·Â°Ã¥ÅƒÄ¹', 'Ä dem', 'ires', 'Ã¥Ä±Â¯Ã¤Â»Â¥Ã©Ä¢Ä¼Ã¨Â¿Ä©', 'Ã¦Ä¸Â°Ã§Ä¼Ä¦', 'Ä deep', 'Ã¥Â¨', 'Ã§Ä­Ä¹', 'Ã¥Ä§Â³Ã¦Â³Â¨', 'Ã§Ä¶ÅÃ¥Ä³Â½', 'Ã¤Â¼Å‚Ã§Â»Å', 'Ä stay', 'Ã¦ÅƒÄ®', 'Ã¥Ä§Â³Ã©Ä¶Â®', 'Ä place', 'Ã¤Â¸Â»Ã©Â¢Äº', 'Ã¥Â¾ÄªÃ¥Â¤Ä¼', 'Ã¨ÄªÄ´', 'Ä professional', 'yle', 'Ã¦Ä½Â²', '19', 'Ä essay', 'Ä give', 'Ã§Â³Ä¸', 'Ä only', 'Ã¦ÅÄ²', 'Ä phys', 'Ã¥Â¯Â¹Ã¨Â¯Ä¿', 'Ä contro', 'Ä amount', 'cept', 'ization', 'Ã§Â¼Ä¸Ã¥Ä¨Ä»', 'Ã¥Ä±Ä¹Ã¥ÄªÂ°', 'Ä always', 'Ã¦Â¯Ä¶Ã¥Â¦Ä¤', 'Ä privacy', 'au', '________', 'Ä responsible', '()', 'Ã§ÅƒÄ«Ã§ÅƒÄ«', 'Ä material', 'Ä online', 'Ã©Â¼', 'Ã¦Ä¶Â¿', 'Ã¥Ä½Ä½', 'Ä enjoy', 'Ã¥Ä¾Å', 'Ä safety', 'Ä tw', 'Ä communication', 'Ã¤Â¸Â½', 'Ã¦ÄºÂ¾', 'olution', 'erg', 'Ä¯Ã¤Â½Ä¾', 'Ä user', 'Ä emotional', 'time', 'Ã©Â¾', 'Ä security', 'Ä sense', 'elines', 'Ã¥Ä¬Â±', 'Ã§Ä«Â©Ã¨Â´Â¨', 'ura', 'Ä share', 'Ä analyzing', 'ital', 'Ã©Â±', 'irtual', 'Ä visit', 'bers', 'Ä cour', 'Ä proble', 'Ã¨Â®Â¾Ã¥Â¤Ä©', 'atch', 'land', 'Ã©Â±Â¼', 'Ã¦ÄªÄ³Ã¤Â»Â¬Ã©Ä¾Ä¢Ã¨Â¦Ä£', 'Ã§Â¨Â³', 'ibility', 'Ä efficiency', 'Ã¥Â£Â°', 'Ã¨Ä´', 'Ã¦Ä¾ÂºÃ¥Ä»Â¨', 'Ä clear', 'Ã¥ÄªÂ¶Ã¥Â®Ä¼', 'izing', 'Ä conditions', 'lusion', 'Ä low', 'Ä lim', 'hers', 'Ä risks', 'Ã§Â¿Â»', 'Ä let', 'Ã¥Ä´Ä¸', 'Ã¥Â¿Ä¥Ã§Ä²Ä¨', 'Ã¨Â¿Ä¾', 'print', 'Ä changes', 'Ä meas', 'Ä improving', 'Ä crit', '50', 'Ã¥Â¸Ä®Ã¦Ä¾Ä½', 'Ä aud', 'Ã¥Ä¯Ä¹', 'Ã¦Ä¹Å‚Ã¦Â³Ä·', 'Ä negative', 'Ã©Â¡Â¹Ã§Ä½Â®', 'und', 'ats', 'Ä companies', 'Ã¦Ä«Â¾Ã¥ÄªÂ°', 'Ä contribut', 'Ã¦ÅƒÂ£Ã§Â¡Â®', 'Ã©Â»Ä¦', 'Ã¥Â±Å€', 'Ä understanding', 'Ä mult', 'Ä clo', 'Ã¥Â¾Ä£', 'Ä prior', 'rim', 'Ã¤ÂºÂºÃ¥Â·Â¥Ã¦Ä»ÂºÃ¨Ä¥Â½', 'Ä variety', 'Ä taking', 'Ã¥Ä¤', 'aster', 'ody', 'Ä {', 'Ã§Ä¼Ä¦Ã©Ä©Ä¯Ã¨Â¦Ä£', 'Ä fore', 'Ã¨ÂµÄ¦Ã¦ÂºÄ²', 'Ã¨Â¦Ä£Ã¦Â±Ä¤', 'Ä features', 'Ã¨Ä¯Ä«', 'me', 'Ã¨Ä®Ä¥', 'Ä oper', 'Ã§ÂºÂ§', 'Ã©Â²Ä¾', 'Ã¦Ä¬Ä¢Ã¥Â·Â§', 'Ä³Ã¦ÄªÄº', 'Ã§Â±Â»Ã¥Å€Ä­', 'Ã¦Ä¿Â¿', 'Ã¨Â½Â¯', 'ew', 'Ä restaur', 'Ä without', 'ructure', 'Ã§Ä¼Ä¦Ã¦ÄºÂ¯', 'Ã§Ä±', 'Ä list', 'urate', 'Ä book', 'Ã¤ÂºÂ²', 'Ã¥ÂºÄ¹', 'Ã¤Â¹ÅÃ¦ÄºÂ¯', 'Ã¤Â»Â»Ã¤Â½Ä·', 'Ä cam', 'Ä Be', 'Ä govern', 'Ä behavior', 'Ã¨Â®ÅƒÃ§Â»Ä¥', 'Ä family', 'Ã¦Ä¿Ä¤', 'Ä city', 'Ä approach', 'Ä accurate', 'Ä som', 'Ä el', 'Ã¨ÄªÅ€', 'Ã¨Å€', 'Ã¥ÅÂºÃ¦Ä¾Â¬', 'Ä dise', 'Ä encoura', 'Ä What', 'Ã¥Ä¥', 'Ã¨Â¯Â¦', 'Â¦Ä¤', 'Ã¥Â·Â¥Ã¥Ä§Â·', 'Ã¥Ä·Â¡', 'Ä still', 'chool', 'Ã¦Ä¦ÅÃ¥ÄªÂ°', 'Ã§Ä¶ÅÃ§Ä«Â©', 'Ã¥Ä´Ä¸Ã¥Ä·Â¡', 'Ã¥Ä©Ä¨Ã¥Â¤Ä©', 'Ä waste', 'Ä events', 'Ã¦Ä·Ä»Ã¨Ä¤Â²', 'Ä 8', 'Ä must', 'ied', 'asing', 'Ã¥Â½Â¢Ã¦ÄªÄ²', 'Ä products', 'Ã¥Ä§Â¸', 'Ã¨Â®Â²', 'fter', 'Ã¥Â·Â®', 'less', 'Ä cro', 'Ä finan', 'Ã¥Ä±Ä¯Ã¥ÂºÄ¶', 'Ã¥ÄªÄ½Ã©Ä¢Å‚', 'Ä guidelines', 'Ã¥ÄªÂ¤', 'Ã¤Â½Ä¾Ã¥ÄµÄ£', 'Ã¨Â¡Â¨Ã§Â¤Âº', 'Ã¥Â¼Ä¤', 'Ä known', 'Ä test', 'Ã¨Â¯Â¯', 'ope', 'Ä users', 'AI', 'Ã¥Â¾Â·', 'new', 'Ã¨Â¿Â½', 'iques', 'Ã¦Â¨Â¡Ã¥Å€Ä­', 'Ã¥Ä¬Ä½Ã¥Ä´Ä®', 'Ä history', 'Ä Al', 'Ã¦Ä¬Ä·Ã¨ÂµÄ¦', 'Ã¥Â°Ä¿Ã¨Â¯Ä·', 'ank', 'Ä home', 'Ã©Ä´Å', 'Ã¤Â¸Â°', 'Ã¨ÄªÄ´Ã©Ä¢Ä¤', 'Ä increase', 'Ä hab', 'Ã¥ÄªÂ»', 'Ã¨Â¾ÄµÃ¥Ä©Âº', 'Ä leading', 'Ä 7', 'Ã©Â£Ä°Ã©Ä»Â©', 'Ä performance', 'Ä happ', 'Ã¥ÅƒÂ£', 'Ä stand', 'ty', 'Ã§Â¦Ä±', 'Ä customers', 'Ã¥Ä¯Ä°', 'Ä belie', 'Ä company', 'Ã¥Â½Ä·', 'Ã©Â£ÅÃ§Ä«Â©', 'Ä Un', 'Ä summ', 'rent', 'Ä Con', 'Ã©Ä¢Ä¤Ã©Ä©Ä±', 'anced', 'Ä i', 'Ä light', 'Ä analysis', 'Ã¥Â°Ä¬', 'Ä Use', 'ouse', 'ted', 'Ä charact', 'Ä #', 'to', 'Ã§Â»Ä¾', 'Ã¤Â¸Ä¯Ã¦ÄºÂ¯', 'Ä developing', 'Ã¥ÅÂ¹', 'Ä strategies', 'Ä might', 'Ã§ÅÅƒ', 'Ã§Ä¼Ä¦Ã¦Ä°', 'Ä first', 'Ã¨Ä¥Ä®', 'Ã§Ä®Â«', 'Ä includes', 'Ã¥Ä½Åƒ', 'Ä diagn', 'Ä growth', 'Ã¤Â¸ÄµÃ¤Â¸Ä¼', 'Ä does', '12', 'Ã§Â»Â¿', 'Ä keep', 'Ã¨Â¯Â¦Ã§Â»Ä¨', 'Ã¥Ä¥Ä±', 'Ã¥Ä±Ä³Ã§Ä¶Å', 'fact', 'Ã¥Ä±Â¯Ã¤Â»Â¥Ã¥Ä¾Â¨', 'Ã§Â«Ä»', 'Ã¦Ä­Ä«', 'Ã¦ÂµÄ°', 'Ä chatbots', 'Ä break', 'Ã¨Â¡Â¡', 'Ã§ÅÂ³', 'Ã¦Ä®Ä£Ã§Â»Åƒ', 'life', 'Ä 10', 'Ã¦Â´Ä¹', 'Ä Additionally', 'Ã¥Â£Â«', 'ember', 'Ä goals', 'Ã¥Â¾Â®', 'Ä view', 'Ã‚Â·', 'ove', 'Ã¥ÅÂºÃ§Â¡', 'Ä optimize', 'Ä tem', 'Ä down', 'Ã¥ÅÂºÃ§Â¡Ä¢', 'Ã¨Â¶Ä§', 'ercis', 'Ä less', 'ees', 'Ã¦Ä¿Ä¥', 'Ä key', 'Ä works', 'Ã¨Â®Â¨', 'Ã¥Ä±Â¥Ã¥ÅƒÄ²', 'Ä robot', 'uss', 'Ã¥Ä§Â¨Ã§Ä²Ä¥', 'Ã§Â»Ä±Ã¦ÂµÄ°', 'Ã¦Ä«Ä¯Ã¨Ä¥Â½', 'egr', 'Ã¤Â»Ä¸Ã¤Â»Â¬Ã§Ä¼Ä¦', 'Ã¤ÂºÄ¶', 'Ã¨ÂµÂ·Ã¦Ä¿Â¥', 'Ã§Äµ', 'Ä factors', 'Ä cultural', 'Ã¦Ä¾Â¨', 'Ä working', 'Ã¤Â¼Â¼', 'Ã¨Ä²Â½', 'Ã©Ä¢ÅÃ¥ÂºÂ¦', 'Ã¤Â½Ä±', 'Ä effects', 'Ã¥Â©Ä¼', 'br', 'Ã¥Ä°Ä§', 'rain', '\")', 'Ã¥ÅƒÂ¦Ã§Ä¶Å', '\",', 'Ä par', 'atform', 'Ä ensuring', 'Ã§Ä¶Â±Ã¤ÂºÄ°', 'Ä much', 'Ä words', 'Ä mar', 'Ã§Â»Ä±Ã©ÂªÄ®', 'Ã¤Â¸ÂºÃ¤ÂºÄ¨', 'Ã¥Ä²ÄªÃ¤Â½Ä¾', 'ven', 'Ä /', 'Ä financial', 'work', 'ories', 'Ã¦Â²Â»', 'Ä techniques', 'Ã¦Ä­Â¥Ã¦Ä¾Ä«', 'rap', 'Ã¥Â°Ä¶', 'Ä est', 'Ä available', 'Ä lit', 'Ã¦Â¹', 'Ä efficient', 'els', 'over', 'Ä land', 'Ä area', 'Ä intellig', 'Ä pref', 'ature', 'Ã§ÅÂ¥Ã¨Â¯Ä¨', 'Ã¦ÄµÄ¯Ã¤Â½Ä¾', 'Ã¥Â¾Ä§', 'igate', 'Ã§Ä¼Ä¦Ã¦Ä¶', 'Ä mean', 'bo', 'Ä control', 'Ã©Ä©Ä©Ã§Ä¶Â¨', 'ricult', 'Ä programm', 'Ä towards', 'thing', 'Ã¤Â¸Ä¯Ã¨Â¦Ä£', 'Ä though', 'Ã¥Â½Â©', 'Ä certain', 'Ä wild', 'Ã¤Â»Ä¬', 'Ä conservation', 'Ã§ÅÂ¥Ã©Ä£Äµ', 'Ä really', 'Ã§Ä¼Ä¦Ã¥Ä¾Â°', 'io', 'Ã©Â¥Â°', 'Ä ful', 'Ã§Ä°Â¯Ã¤Â¿Ä¿', 'Ä explore', 'Ã§Ä¼Ä¦Ã¦Â¸', 'Ä diverse', 'Ã¥Ä¬Å‚Ã¥Â¼Âº', 'Ã§Ä¼Â®', 'Ä emotions', 'Ä avoid', \"'ll\", 'Ã§Ä¼Ä¦Ã¦Ä«', 'Ã¥Ä¯Â¡', 'Ä platform', 'ances', 'Ä situ', 'Ã¤Â»Äº', 'Ã¤Â½Ä¯Ã§Â½Â®', 'oring', 'Ã§Ä½Ä²', 'Ã¤Â¸Ä©', 'Ä dev', 'nov', 'ash', 'Ä two', 'Ã¥Â®Å‚', 'bon', 'Ã¨ÂµÂ°', 'Ã¥ÄªÄ¹Ã¨Â¡Â¨', 'Ä cy', 'Ã¨Ä¯Ä²', 'Ä Some', 'Ä explain', 'Ä aware', 'Ã§Â¤Â¾Ã¤ÂºÂ¤', 'day', 'Ã¥Ä±Ä®', 'Ã¦Â²ÅÃ©Ä¢Ä¼', 'Ã¦Â°Â§', 'Ã¥Â¼Ä¢Ã¥Ä±Ä³', 'Ã¥Ä§Â¬Ã¥Ä±Â¸Ã§Ä¼Ä¦', 'Ä air', 'Ã¥Ä©Â»', 'aring', 'Ã©Ä¥Â½Ã¦ÄºÂ¯', 'Ä levels', 'ods', 'Ä steps', 'Ä cap', 'Ã¦Â´Å€', 'Ã©Â©Â¬', 'Ä return', 'Ä met', 'Ã§Ä¶ÅÃ¦Ä¢Ä£', 'Ã¤Â¸Â°Ã¥Â¯Ä®', 'Ã¦ÅÄµ', 'Ã¦Ä«Ä¢Ã¤Â»Â¥', 'Ã©Â¡Â»', 'Ä er', 'Ä fra', '30', 'Ã¨Äµ', 'Ã¢Ä¢Ä¶', 'Ä Ã¥Â½Äµ', 'ah', 'Ã¤Â¿Ä¥', 'Ä likely', 'Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä ', 'Ã¥ÄªÄ¿', 'Ä creating', 'Ä farm', 'Ä bal', 'Ä lives', 'Ã¥Â®Ä¥Ã§Ä¼Ä¦', 'Ä ability', 'Ã¤Â¸Ä¬Ã§Ä¼Ä¦', 'Ä sentence', 'Ã¥Ä¤Â¨', 'Ä rout', 'Ä provides', 'Ä again', 'Ã¥Â®Å‚Ã§Ä«Â©', 'Ã©Ä¢Ä²', 'Ä years', 'Ã¨Å€Ä¯', 'Ä physical', 'Python', 'Ä Ex', 'iting', 'Ã¨Â°Ä¥Ã¦Ä·Â´', 'Ã§Â½Ä³Ã§Â»Ä¾', 'Ã¦Ä§Â¢', 'Ã§Â©ÂºÃ©Ä¹Â´', 'Ã¥Ä½Â°', 'Ã¨Â±Ä¨', 'Ã¦Ä½Â´Ã¥Â¤Ä¼Ã§Ä¼Ä¦', 'Ä Ar', 'Ä maintain', 'Ã¥Â®Å€Ã©Ä»Ä§', 'Ä travel', 'Ä sat', 'pro', 'Ã§Ä¶ÂµÃ¥ÅƒÄ²', 'Ã¦Â±Â½', 'ex', 'Ã¥Ä£Ä©', 'Ã¦Ä²Åƒ', 'Ã©Ä¼Ä±Ã§Ä¿Ä¢', 'Ã¨Â¿ÄºÃ¦Ä¾Ä«', 'Ã§Â¤Â¼', 'ale', 'Ä consum', 'ÄŠÄ ', 'ncy', 'Ä questions', 'fort', 'making', 'Ä desc', '15', 'Ä involves', 'Ä stress', 'Ã¥ÅƒÄ¹Ã§Â¬Â¦', 'here', 'Ä impacts', 'Ä exercis', 'Ã¥Ä¿Ä¼', 'ledge', 'Ã§Â§Ä³Ã¦Ä¬Ä¢', 'oci', 'Ä effectively', 'Ã¦Â¶ÄªÃ¨Â´Â¹', 'Ä conclusion', 'Ã©ÄºÄ§', 'Ä stre', 'issions', 'Ã¦Â·Â»', 'It', 'Ã©Ä¿Ä»', 'Ä virtual', 'Ã¨Â¡Â£', 'Ä achieve', 'ource', 'Ã¨Â¿Å€', 'acks', 'Ã¨Â¡Â¨Ã¦Å‚Â¼', 'Ä importance', 'Ã¨Ä©ÂªÃ¦ÄªÄ³', 'These', 'num', 'Ã§Ä¼Ä¦Ã¦Å‚', 'Ä relationships', 'Ä workers', 'gical', 'orpor', 'erson', 'Ã¥Ä³Â¢', 'nds', 'Ã¦Ä°Â¨Ã¨Ä¯Ä²', 'ohn', 'Ã¥Â¿Ä§Ã©Â¡Â»', 'Ã¥Â®Â¹Ã¦ÄºÄµ', 'Ä Go', 'Ä tell', 'Ä Res', 'onom', 'Ä bec', 'Ã¦Â³Ä½', 'pos', 'Ä move', 'Ä story', 'Ã¦ÅƒÂ¢', 'Ä priorit', 'Ä industries', 'Ã¨Ä¾', 'Ä possible', 'Ä Man', 'Ä express', 'abilities', 'Ä integr', 'Ã¤Â»Â£Ã¨Â¡Â¨', 'Ä respond', 'Ã¥ÄªÄ¨Ã©Ä´Å', 'Ã¦Ä¾ÂºÃ¤Â¼Ä¼', 'Ä things', 'Ã¤ÂºÂ¤Ã¦ÂµÄ£', 'Ä meth', 'urther', 'Ä wide', 'Ã¨Ä³Ä¹', 'Ã¦ÄªÄ³Ã§Ä¼Ä¦', 'Ä¸Ã§Ä·Â¥', 'ides', 'ething', 'Ä While', 'pan', 'Ã§ÅƒÄ¸Ã§Ä·Â¥', 'Ä cent', 'Ä please', 'ology', 'uracy', 'Ã¥Â¾Âª', 'ward', 'nce', 'Ä then', 'Ã§ÂªÄ£', 'Ã¥Â¥Ä©', 'Ä blo', 'ai', 'Ã¦Å€Ä¹', 'Ã§Â®Ä¹Ã¦Â³Ä·', 'Ã§Â»Â¼', 'Ä print', 'aces', 'lu', 'ÂªÃ¦Ä¸Â½', 'pre', 'Ã§Ä¼Ä¦Ã¦Ä¦Ä±', 'Ä sol', 'Ä overall', 'hold', 'Ä es', 'Ã§Ä¼Ä¦Ã¤Â¸Ä¢', 'Ã©Ä£Ä©', 'Ä popul', 'Ã¥Â°Ä±Ã¨Â¯Â´', 'Ã¦Â³Â¢', 'Ã¥Ä¯Ä£', 'Ã¤Â¹ÅÃ¥Ä±Â¯Ã¤Â»Â¥', 'Ã©Â£ÅÃ¥ÄµÄ£', 'Ä content', 'Ã¥Â°Ä¦', 'Ä requires', 'Ã¦Â£Ä¢Ã¦ÅÂ¥', 'ÄŠÄ Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä ', 'Ä groups', 'Ä fair', 'Ä bl', 'Ã¥Â®Å€Ã©ÂªÄ®', 'Ã¦Ä®Ä«Ã§Ä§Â§', 'osp', 'str', 'Ã¤Â¸Ä¯Ã¨Ä¥Â½', 'Ä harm', 'Ä produ', 'Ã§Ä¼Ä¦Ã¦Ä¬Ä¢', 'Ã§Ä©', 'tle', 'Ä animals', 'Ã¨Â§Ä´Ã¨Ä«Â²', 'lev', 'Ã¦Â¸Ä²', 'Ã¥Â¤Ä¯Ã¦Ä¿Ä¤', 'Ä depend', 'Ã¦Ä®Ä³Ã¦ÄªÄº', 'Ã¥Ä®Ä§Ã¥Ä²Â«', 'Ä helps', 'Ä open', 'Ä net', 'Ä Ä Ä Ä Ä ', 'Ä strong', 'Ä jour', 'Ã¥Â¹Â¿Ã¦Â³Ä½', 'Ã¦Ä·Â´Ã¤Â¸Âª', 'Ä elect', 'Ä response', 'Ã¥Ä¯Ä·Ã¨Â¯Ä¯', 'Ã¦Ä¾Ä­', 'Ä <', 'Ã¥Ä®Ä¸Ã¥ÅƒÂ¦', 'Ã©Ä´Äª', 'Ä quick', 'ually', 'Ä something', 'Ä track', 'Ã¥ÂºÂ¦Ã¥Ä´Ä®', 'erences', 'Ã¦Å‚Ä³', 'Ä accuracy', 'Ä exc', 'Ã©Â£Å€', 'Ä field', 'Ã¥Â¯Â»Ã¦Ä«Â¾', 'Ã©Ä§Â¸', 'Ä hope', 'Ã§Ä³', 'Ä innov', 'Ã§Â»Âª', 'alk', 'Ä types', 'Ä did', 'Ã¥Ä¬Âª', 'Ä call', 'Ã¨Â¯Ä¹', 'Ä early', 'Ä One', 'app', 'Ä common', 'Ã¦Ä¾Ä¢Ã§Â»Äª', 'Ä check', 'Ä sym', 'Ã§Ä¤Ä´', 'Ã¦Ä¬Ä¢Ã¨Ä¥Â½', 'Ä enh', 'Ä agricult', 'Ä imm', 'Ã§Â»Ä©', 'Ã¦Â»Â¡Ã¨Â¶Â³', 'Ä school', 'bal', 'Ä following', 'based', 'Ä webs', 'Ä culture', 'Ä Com', 'way', 'Ã¤Â¸Ä¢Ã¥Â®Ä¼', 'Ã¥Ä·Ä¨Ã¥ÄµÄ£', 'ude', 'Ã§Ä¼Ä¦Ã¥Ä±Ä³Ã¥Â±Ä·', 'Ã§Ä¶ÅÃ¤ÂºÂ§', 'osystem', 'Ä plant', 'Ã¥Ä±Â¶', 'Ã¥Ä²Ä¥', 'Ã¤Â»Ä¸Ã§Ä¼Ä¦', 'der', 'Ã¨Â¯Â¢', 'Ã¥Â®Â¶Ã¥Ä§Â·', 'Ä free', 'Ã§Â§Â»', 'Ã¦Ä°Ä®', 'Ä body', 'Ä present', 'Ä particularly', 'Ä children', 'Ä student', ').', 'Ã§Ä«Â¹Ã¥Â¾Ä£', 'Ã¨Ä¶', 'Ã©ÄºÄ§Ã¨Â¯Â»', 'Ã¦Ä·ÄªÃ§Ä°Ä©', 'Ä program', 'Ã©Ä§Â±', 'Ã¥Ä±ÄºÃ¥Â¾Ä¹', 'ix', 'Ä come', 'Ã§Ä¼Ä¦Ã¦Â²', 'Ä Te', 'Ä To', 'Ã¥Ä§Â±Ã¥Ä²Ä®', 'Ä employees', 'Ã¨Â¯Â´Ã¦ÄºÄ°', 'Ä heart', 'Ä mot', 'Ã¦Ä¾Ä­Ã¥Ä±Ä­', 'eric', 'Ã¨Â¯Ä³', 'Ä current', 'Ã¦ÄªÄ²Ã¦Ä¾Â¬', 'Ä too', 'Ã§Ä°Â©Ã¥Â®Â¶', 'Ã¥ÄªÄ½Ã¦Ä¸Â°', 'Ä ecosystem', 'Ã¥Â¸Â¸Ã¨Â§Ä£', 'Ã¤Â¸Ä¢Ã¦ÅƒÂ¥', 'Ä pres', 'Ä multi', 'Ã¥Ä³Ä¬Ã¨Â¯Ä«', 'Ã¤Â¸Â¥', 'Ä mit', 'Ä action', 'Ã§Ä¨Å', 'Ä habit', 'Ã¥Ä±Â£Ã¦Ä¦Å', 'Ã§Â®Â±', 'Ä uses', 'Ã¥Â¢Å€Ã¥Â¼Âº', 'Ã§Â»Ä»Ã¥Ä©Âº', 'Ä 9', 'Ä dep', 'Ä economic', 'Ã¦Ä¢Â§Ã§Ä¼Ä¦', '18', 'Ã¥Ä¨Â°', 'Ä helped', 'Ã¥Ä²Â¸Ã¥Â¼Ä·', 'Ã§Ä°Ä­', 'Ä diagnos', 'Ã¥Å‚', 'Ã¨Ä£Ä¶Ã§Â³Â»', 'Ã§Â¾Â¤', 'Ã§Â»Ä¥Ã¤Â¹Å‚', 'Ã¦ÄªÄ²Ã©Ä·Â¿', 'Ä point', 'Ã¥Â®Ä¼Ã¦Ä¾Å', 'Ã¥Ä³Â¼', 'Ã¨Ä¯Â¯', 'Ã¦Ä¿Â¯', 'Ã¦Â¤Ä´', 'Ã¦Ä·ÄªÃ¦Å€Ä¾', 'Ä special', 'Ã¦Â·Â·', 'Ã¥Ä©Å‚Ã¤Â¸Âª', 'ause', 'Ã©Ä¨', 'Ã¦Â¯Ä¶Ã¨ÂµÄ½', 'Ã¨Â·Ä¿', 'What', 'Ä times', 'icles', 'Ä *', 'Ã§Â´Â§', 'Ã¥Â¦Ä¤Ã¦Å€Ä¾Ã¤Â½Å‚', 'Ã§Ä­Â¬Ã§Ä«Â¹', 'Ã§Ä£Âµ', 'Ã§Â¨Ä°', 'Ä carbon', 'Ä bias', 'Ã¥Ä¬Â©Ã¤ÂºÄ°', 'Ä const', 'Ã¨Ä©ÂªÃ§Ä¶Â±', 'Ã¦Ä¿Â¥Ã¨Â¯Â´', 'Ã¥Â°Â±Ã¦ÄºÂ¯', 'Ã¥Ä¯Â°', 'Ä meet', 'Ã¨Â§Ä¦Ã¥ÄªÄ´', 'Ã§Ä¼Ä¦Ã§Â¾', 'Ã¨Ä²Â¥Ã¥Ä§Â»', 'ators', 'Ã§Â¨Â³Ã¥Â®Ä¼', 'ode', 'Ã§Ä§Â®', 'Ä associ', 'Ã¥Â¿Ä¹', 'Ã¨Â¡Ä®Ã¦ÄºÅ', 'Ã¦Ä¿Ä°', 'Ä review', 'Ã¥Ä©Ä¢', 'Ä Ro', 'Ä knowledge', 'Ã¤Â»Â¥Ã¤Â¾Â¿', 'Ã¦ÂµÄ­Ã¨Â¯Ä·', 'Ã¥Ä²ÄªÃ©Ä¢Ä¤', 'sc', 'Ã¥Â½Â¢Ã¥Â¼Ä±', 'Ä friends', 'Ä nature', 'Ä critical', 'Ã¦Â´Ä­', 'Ä after', 'erve', 'Ä rece', 'Ã§Ä¼Ä¦Ã¦Åƒ', 'Ã¦Â±Â½Ã¨Â½Â¦', 'Ã§Ä·Ä®', 'Ä loss', 'Ä applications', 'Ã¥Â¤Ä¼Ã§Â§Ä¯', 'Ã©Ä¶Ä§', 'Ã¤Â¸Â²', 'Ä insp', '---', 'Ä Sh', 'Ä vol', 'lut', 'oks', 'sequ', 'Ä bir', 'Ã¥Ä²ÄªÃ§Ä²Ä¨', 'Ä necess', 'Ã¦ÄªÄ³Ã¦Ä¥Â³', 'Ã§ÅƒÄ«Ã¦Ä¸Â¹Ã©Ä¿Â¢', 'Ã©Â¼Äµ', 'Ä soft', 'Ä live', 'Ã¥Â°Ä±Ã¦ÄºÄ°', 'Ä Ind', 'Ä bring', 'Ã¦ÄºÂ¯Ã¦Ä®Ä©', 'Ä soil', 'ilar', 'Ã¤Â¸Ä¾', 'Ã¦Ä¿Â¡Ã¤Â»Â¶', 'Ä tri', 'Ã¤ÂºÂ®', 'Ä mom', 'Ã¦Ä±Â¡', 'Ã¤Â¼Â°', 'Å€Ã¤ÂºÄ«', 'Ã§Ä½Ä³', 'Ã¨Ä¤Â¤', 'Ã¨Â´Â¢Ã¥Ä¬Â¡', 'Ã¦Â·Â»Ã¥Ä¬Å‚', 'Ã©Â¥Â®Ã©Â£Å', 'Ä allowing', 'Ã¥ÂºÄ·', 'Ä right', 'Ä expert', 'Ä supp', 'Ä init', 'Ã§Ä¼Ä¦Ã¦Âµ', 'arget', 'Ä expect', 'Ä 19', 'Ä measures', 'olutions', 'just', 'arc', 'Ã¥Â°Ä¼', 'Ä practice', 'Ã¦Ä¾Ä«Ã¥Ä¬Â©Ã¤ÂºÄ°', 'Ã¥Â¤Â§Ã©Ä©Ä±', \"',\", 'iment', 'Ä continue', 'Ä discuss', '100', 'Ã©Ä¼Ä¾', 'Ã§Ä¼Ä¦Ã¦Ä¦Å', 'Ä reflect', 'itation', 'Ã¥Ä¯Â«', 'Ã¤ÂºÄ¨Ã¤Â¸Ä¢', 'ney', 'Ä Le', 'ised', 'Ã¨Â¶Ä­', 'Ã¤ÂºÄ¨Ã¤Â¸Ä¢Ã¤Â¸Âª', 'Ä increasing', 'Ã§Ä¼Ä¦Ã¦Ä®', 'Ä stru', 'Ã¦Ä¢Â»Ã§Â»Äµ', 'ely', 'Ã¥Â®Ä©', 'Ä author', 'Ã¨Â¡Â¨Ã©Ä¿Â¢', 'Ä x', 'Ã¦Ä·Ä§Ã¤ÂºÄ­', 'emic', 'Ä represent', 'ger', 'Ä increased', 'ones', 'ains', 'Ä trained', 'Ä fish', 'Ä state', 'Ã¥Ä¨Â·', 'Ã§Ä¶ÅÃ©Ä·Â¿', 'Ä renew', 'ording', 'Ã¥Ä®Ä¹', 'Ã¦Ä°ÂªÃ¦Ä¸Â½', 'Ã¥Â¹Â³Ã¨Â¡Â¡', 'Ä successful', 'Ã¤Â¸Ä­Ã©Ä¿Â¢', 'Ä activity', 'Ã¨Ä®Â¶', 'Ã©Ä¢Ä¤Ã¥ÂºÄ¶', 'Ã¨Ä¦Ä³', 'Ã¦Ä°Â¢Ã§Â´Â¢', 'ffic', 'Ã§Â»Ä¦Ã¦ÄªÄ²', 'atives', 'Ã¤ÂºÄ¼', 'Ä scen', 'Ã¦Â²Ä»', 'gress', 'Ã¤Â½Â¿Ã¥Â¾Ä¹', 'Ã¦Ä«Â¿', 'Ä discrim', 'Ä assistants', 'Ä exist', 'Ã§Ä·Ä»', 'Ä space', 'Ã¦Ä¾Ä¢Ã¨Â¿Ä³', 'Ä ideas', 'Ã©Ä©Ä©Ã¥Ä±Ä¸', 'light', 'Ã¦Â³Â¨Ã©Ä©Ä¯', 'Ã§Ä¼Ä¦Ã¦Ä¹Â¶Ã©Ä¹Â´', 'Ã¨Â¿Ä°', 'Ä comb', 'Ã©Ä¢Ä¤Ã¥Â½Äµ', 'Ä yourself', 'rite', 'ason', 'Ã¥Ä®Ä¢', 'Ã¥Ä±Â¯Ã¤Â»Â¥Ã¤Â½Â¿Ã§Ä¶Â¨', 'Ã¥Ä§Ä§Ã¦Â»Â¡', 'Ä values', 'Ã¦Â½', 'Ä biases', 'Ã¤Â¿Ä¥Ã¨Â¿Ä½', 'Ã¥Ä¾ÂºÃ¦Ä»Â¯', 'ross', 'Ã¥Ä¯Â³Ã¥Ä±Â¯', 'Ä cru', 'Ä number', 'Ä type', 'rast', 'Ã¥Ä©Ä¨Ã§Â¡Â®', 'This', 'Ä past', 'Ã§Ä£Â¯', 'Ã¥Â®Ä¼Ã¤Â¹Ä«', 'Ä solutions', 'Ä ter', 'Ã¤Â¿Ä¿Ã¨Â¯Ä£', 'Ã¨Ä¶Â¬', 'Ã¥Â¹Â¸', 'Ã¥Ä«Â§', 'Ã¥Ä§Â´Ã¨Â¶Â£', 'Ã¥Âª', 'ention', 'avor', 'Ä scient', 'Ã¥Ä¬ÂªÃ¥Ä¬Ä½', 'Ä providers', 'Ä policies', 'alu', 'Ä Im', 'Ä allows', 'Ä intelligence', 'Ã§Ä¼Ä¦Ã¦Ä¸Â¹Ã¦Â³Ä·', 'Ã¨Â¿Ä»Ã¦ÄºÂ¯', 'Ä `', 'Ä emissions', 'Ä Ã¥Â°Ä¨', 'Ä meaning', 'Ä style', 'Ã¥Ä°ÅÃ¥Ä½Å‚', 'Ä strugg', 'Ã§Ä¼Ä¦Ã§Â¾Ä°', 'iful', 'dition', 'Ã©Ä¥Â½Ã¦Ä¾Ä«', 'Ã§Â©ÂºÃ¦Â°Ä¶', 'Ã¥Â®Ä¥Ã¤Â»Â¬Ã§Ä¼Ä¦', 'Ã¤Â¼ÄºÃ¥Ä®Ä¸', 'Ä influ', 'Ã¥ÅÂºÃ¤ÂºÄ°', 'Ä details', 'Ä transparency', 'Ä mess', 'Ä Cl', 'Ä game', 'pri', 'Ã¨Â¶Ä­Ã¥Ä¬Â¿', 'Ã¥Â½Ä´', 'Ã§Â¿Â»Ã¨Â¯Ä³', 'Ã¦Ä·Â£', 'By', 'Ã©Åƒ', 'Ä Americ', 'Ä production', 'Ä incorpor', 'Ã¦Ä»Ä¼', 'Ä involve', 'Ä hot', 'Ã¦Ä»Â®', 'by', 'Ä flow', 'Ä emerg', 'Ã¥ÂºÂ§', 'Ä idea', 'Ã¥Ä°Ä­Ã¥Ä¬Ä½', 'Ã©Ä¿Ä´', 'oms', 'Ã¨Ä£Ä®Ã¤Â¸Ä¼', 'Ä report', 'Ä pap', 'Ä therap', 'Ä sal', 'Ã¥Ä±Ä¤Ã¤Â¸Ä°', 'Ã¦Ä¸Ä©Ã¥ÅƒÂ¦', 'Ã¦Ä²ÅƒÃ©Ä§Ä¯', 'oot', '),', 'Ä cr', 'Ä processes', 'gin', 'Ã¥Â¹Â³Ã¥Ä±Â°', 'Ã¥Â¯Å', 'Ä promoting', 'Ã¦Ä¼Ä¸', 'akehold', 'Ã§Â»Â§', 'iver', 'Ã¦Â¦Ä¤', 'Ä models', 'Ä dra', 'Ã¨Ä¸', 'Ä group', 'Ã¨Â¶Â³Ã¥Â¤Å', 'Ä green', 'Ä healthy', 'Ä comfort', 'Ä additional', 'Ã¤Â¸Ä¢Ã¦Â¬Â¡', 'Ã©Â¤Ä²Ã¥Ä°Ä§', 'Ä materials', 'Ä manage', 'Ã§Ä¼Ä¦Ã¦Â¯', 'Ã¤Â¼Â¤', 'Ã¥Ä±Ä¬Ã¦Ä¹Â¶', 'Ä glo', 'Ä stat', 'Ã¥Â¿Â«Ã©Ä¢Å', 'Ä monitoring', 'aily', 'rand', 'oice', 'resh', 'Ã§Â»Ä¦Ã§Â»Ä©', 'Ä under', 'Ä necessary', 'Ä helpful', 'Ä Col', 'Ã©Â»Ä³Ã¦Â´Å€', 'Ã¥Ä£Ä¼Ã¥Ä©Âº', 'Ä course', 'Ä mat', 'Ä leg', 'Ä face', 'Ã¤Â»Â¤', 'Ã¨Ä«Â¯Ã¥Â¥Â½Ã§Ä¼Ä¦', 'ock', 'Ã¥Ä®Â»Ã§Ä¸Ä¹', 'Ã§Ä½Ä¸', 'idence', 'Ä associated', 'Ä progress', 'Ã¥Ä¾Ä¨', 'Ä everyone', 'Ã§Â¼Äµ', 'Ä Eng', 'word', 'Ã¨ÄµÄ¿', 'Ã¥Â¤Â©Ã¦Â°Ä¶', 'Ä actions', 'ems', 'Ä Pl', 'Ã¥Â®Ä»', 'ush', 'Ã©Â¡Â¾', 'Ä costs', 'ator', 'Ã§Â©Â¿', 'Ä amounts', 'Ã¨Ä¶Â¬Ã¨Ä±Ä¾', '..', 'Ä manner', 'Ä consequ', 'Ã¦Â°Ä¶Ã¥Ä¢Ä»', 'Ä insights', 'being', 'atory', 'ener', 'lex', 'Ä means', 'Ä collaboration', 'Ä perspect', 'orm', 'priate', 'Ã¥Â°Ä¬Ã©Ä©Ä¯', 'Ä target', 'Ã¨Â®Â°Ã¥Â½Ä·', 'Ã¥Ä¢Ä´', 'Ä renewable', 'Ã¦Ä¦Â¿', 'Ã¨Ä¥Â½Ã¦ÂºÄ²', 'Ä input', 'Ã¥Â®Ä©Ã¥Â®Ä»', 'ape', 'Ä adjust', 'eries', 'Ä dire', 'Ã¤Â¾Ä¿', 'ustr', 'fect', 'Ä beautiful', 'Ä due', 'reci', 'Ã§Ä®Â®', 'Ã¨Ä¥Ä®Ã¦Ä»Â¯', 'Ã¨Ä¤Â¡', 'Ä dam', 'ik', 'Ä advanced', 'Ã§Ä½Â¸Ã¥Â¯Â¹', 'Ã¥Ä²Ä¯Ã§Â§Â°', 'Ä short', 'Ä object', 'Ã¨Â¿Ä»Ã©Ä©Ä®', 'Ã©Ä¢Å‚Ã¦ÄªÄ²', 'Ã¨Ä²Â¥Ã©Ä¶Ä¢', 'Ã§Ä¼Ä¦Ã¦Ä¥Ä§Ã¦Ä¦Å', 'Ã§Â¥Â¨', 'Ä countries', 'ining', 'istic', 'Ä plans', 'Ã¨Â´Â£Ã¤Â»Â»', 'Ä stakehold', 'the', 'Ä assess', 'Ã¦Ä¢Ä¿Ã¨Ä¢Ä¥', 'ech', 'Ã¦ÄªÄ²Ã¥Ä³Äº', '21', 'Ä daily', 'Ä comput', 'Ã§Ä¼Ä¦Ã¦Ä¥Ä§Ã¥Ä¨Âµ', 'Ã¦Ä±Ä²Ã¥Ä©Âº', 'Ä Ã¢Ä¢Ä¾', 'Ã¥ÂªÄ´', 'Ã¤Â¸ÅƒÃ¥Â¿Ä¥', 'ished', 'Ä Se', 'onomous', 'ern', 'Ã§Â»Â´Ã¦Ä¬Â¤', 'ames', 'Ä prioritize', 'Ã§ÂºÂ¸', 'Ã¨Ä¤Â¥', 'Ä temper', 'Ã¦Â¸Ä§Ã¦Â´Ä£', 'use', 'Ã¦Â±Â¡', 'Ä minim', 'Ã¦ÄºÂ¯Ã¥Ä¾Â¨', 'Ã¥Â¤Â§Ã¥Â°Ä±', 'Ã¥ÄµÂªÃ¤ÂºÄ½', 'Ä appreci', 'reng', 'Ä regulations', 'Ä Z', 'Ã©Ä¶Ä»Ã¨Â¯Â¯', 'rans', 'Ã¨Ä¢Ä®Ã¤Â¸Ä¶', 'Ã¨ÄªÂ¬', 'Ã¨Ä³Â±', 'Ã¨Ä¨', 'Ã¦Â°Â´Ã¥Â¹Â³', 'Ã¨Â´ÅƒÃ§Ä«Â©', 'Ã¥ÅƒÄ¹Ã§Â¬Â¦Ã¤Â¸Â²', 'Ã¥Â¯Â¹Ã¦Ä¸Â¹', 'Ä him', 'Ä consequences', 'Ã¥Â·Â´', 'Ã©Â¼ÄµÃ¥Ä¬Â±', 'Ä fil', 'Ã¤ÂºÂºÃ¥Ä³Äº', 'Ã¨Â·Ä¿Ã§Â¦Â»', 'Ä When', 'Ã§Ä¼Ä¦Ã¦Â°Â´', 'Ã§Ä«Â©Ã§Ä²Ä¨', 'Ã¥Ä²Ä®Ã¦Ä¹Â¶Ã¤Â¹Å', 'Ã¥Ä¾Â¨Ã¨Â¿Ä»Ã¤Â¸Âª', 'Ã¥Ä§Â¶Ã¦Â¬Â¡', ',\"', 'Ã¦Â¶Â²', 'Ã§Ä¶Â·', 'ival', 'Ã¥Ä±Â¯Ã¤Â»Â¥Ã¨Â®Â©', 'Ã¦Ä¥Â¯', 'Ä advance', 'Ä veh', 'Ã¥Â¦Ä¤Ã¦Å€Ä¾Ã¦Ä¤Â¨', 'Ä estab', 'ript', 'Ã§Â«Â¯', 'Ã¤Â¸Ä¯Ã¤Â¼Ä¼', 'Ä transparent', 'Ã¦Ä·Â°Ã©Ä©Ä±', 'Ã§Ä½Äº', 'Ä speak', 'Ä park', 'Ä stakeholders', 'Ã©Âº', 'Ä event', 'Ã§Ä¼Ä¦Ã¦Ä·Â°Ã¦Ä¯Â®', 'Ã¨Ä©ÂªÃ¥Ä¬Â¨', 'Ã§Â»Ä¨Ã¨Ä¬Ä¤', 'Ã¨Â¯Ä¦Ã¤Â¼Â°', 'Ã¦Â¶Â¦', 'Ä preferences', 'Ä veget', 'Ã¦Ä¯Å', 'equ', 'Ä gl', 'Ä pain', 'ogra', 'Ä traffic', 'Ä oce', 'Ã¤Â¹Äº', 'ext', 'Ã¢Ä¢Ä¿Ã¯Â¼Ä®', 'Ä another', 'Ã¥Â¤Ä¼Ã¥Â°Ä³', 'Ä against', 'Ã§Â»Ä±Ã¥Ä°Ä¨', 'Ã¨Â®Â¡Ã§Â®Ä¹Ã¦Ä¾Âº', 'Ã¨Ä¢Ä²', 'Ã¨Â½Â¯Ã¤Â»Â¶', 'Ä Pre', 'Ä plants', 'Ã§Ä½Â¸Ã¤ÂºÄ´', 'Ã©Â¢Ä³', '\\\\_', 'Ä same', 'rug', 'Ä valu', 'Ä occ', 'Ã§Ä¼Ä¦Ã§Â¤', 'Ä sustainability', 'Ä She', 'de', 'ote', 'Ä dig', 'NA', 'Ä crucial', 'Ã¦Ä«Â§', 'Ã¥Â±Ä¢', 'Ã¦Ä­Å', 'Ã¦Ä­Ä®', 'Ä non', 'Ä engaging', 'Ä intern', 'LP', 'Ã¦Â¸Â©Ã¥ÂºÂ¦', 'Ã¦Å‚Â¸', 'Ã¦Ä¬Â¥Ã¥Ä³Ä¬', 'Ã¦Ä¿Â¥Ã¨Â¶Ä¬', 'hood', 'Ã¤Â¸Ä«Ã¤Â¸Âª', 'Ã¥Â¦Ä¤Ã¤Â¸Ä­', 'Ã§Ä«Â©Ã¤Â½Äµ', 'force', 'Ä needed', 'Ä images', 'Ä building', 'icious', 'Ä Ã¦ÄªÄ³', 'Ã¨Â¶Ä¬Ã¦Ä¿Â¥Ã¨Â¶Ä¬', 'Ã¦Ä¶Â¾Ã¥Ä§Â¥', 'go', 'Ã©Ä»Ä¯Ã¤Â½Ä°', 'Ã¥Â½ÄµÃ¥Ä¾Â°', 'Ã¦Â¶ÄªÃ¨Â´Â¹Ã¨Ä¢Ä§', 'Ã§Â£', 'iversity', 'Ã©Â¢Ä¦Ã§Â®Ä¹', 'icle', 'Ã¦Â·Â·Ã¥Ä²Äª', 'Ä particip', 'Ä dishes', 'Ä throughout', 'Ä within', 'Ã¥Ä±Â³', 'Ã©Â«ÄºÃ§Ä¼Ä¦', 'Ä phot', 'Ä trust', 'Ã¦Ä¦Ä±Ã¨Â¯Ä¨', 'Ã¤Â»Â¥Ã§Â¡Â®Ã¤Â¿Ä¿', 'Ã§Ä¬Â¶Ã¦Ä¢Ä£', 'Ä automation', '11', 'Ä post', 'Ã¦Ä«Ä­Ã¦Ä¾Âº', 'works', 'Ã©Ä¢Ä±', 'Ã¥ÂºÄµ', 'Ä wind', 'Ä ==', 'Ä processing', 'Ã¨Ä®Ä¥Ã¥Ä½Â´', 'Ã¦Ä¦Ä±Ã¤Â¹Ä«', 'Ã¨Â¿Â½Ã¦Â±Ä¤', 'ÃƒÂ©', 'Ã¥Â¾Ä¦', 'Ã©Ä¿Å‚', 'Ã¤Â¸Ä¸', 'Ã¨Ä»Â½', 'Ã§Â«Å€Ã¤ÂºÄ«', 'Ä appropriate', 'Ã¦Ä½Â´Ã¥Â¥Â½Ã§Ä¼Ä¦', 'Ä character', 'cl', 'Ã§Â§Äº', 'itude', 'Ä teac', 'leep', 'Ä Develop', 'ince', 'Ã¥Â·Â¦', 'ground', 'Ã¨Â¡Ä®Ã¤Â¸Ä¼', 'Ã©Ä´ÄªÃ¥Â¯Â¹', 'Ã¥Â¿Ä§Ã¨Â¦Ä£', 'Ä determ', '----------------', 'Ä streng', 'do', 'Ä challenging', 'ork', 'Ä anx', 'Ã¨Ä«Â²Ã§Ä¼Ä¦', 'Ä hard', 'Ã¦ÄºÄ°Ã§Â¡Â®', 'Ã¥ÄªÄ¨Ã¤ÂºÂ«', 'Ã¦Ä¶Â¹Ã¥Ä±Äº', 'Ã¤Â½Â³', 'Ã¥Ä±ÂªÃ¦Ä¾Ä«', 'Ã¥Â±Ä·Ã§Â¤Âº', 'Ä camp', 'Ã§ÂºÂ³', 'aj', 'etic', 'ument', 'Ã¤Â½Å‚Ã¥Ä±Â¯Ã¤Â»Â¥', 'Ä pollut', 'Ä hig', 'pping', 'ead', 'Ã§Ä¦Â¶Ã¨Ä¢Ä®', 'Ã§Â¬Â¬Ã¤ÂºÄ®', 'Ã©Â¸Å', 'Ã§Ä«Â©Ã¥ÄµÄ£', 'Ã¤Â¸Â¾', 'Ä encourage', 'pecial', 'Ä across', 'elves', 'Ã¤ÂºÄ­Ã¤Â»Â¶', 'cle', 'Ã¦Â©', 'Ã¥ÂªÄ´Ã¤Â½Äµ', 'ners', 'Ä cal', 'Ã¨Ä»Â½Ã§Ä¦Â¶', 'Ã¥Ä½Âº', 'Ã¤Â¹Å‚Ã¦Ä¥Â¯', 'Ä safe', 'Ã¨Ä¥Â½Ã©Ä©Ä±', 'istics', 'Ã¤Â¹Ä­Ã¥Ä«Ä¯', 'Ä issue', 'Ã¥Â¤Ä¼Ã¤Â¸Âª', 'Ã¥Ä¨Â³Ã§ÅƒÄ¸', 'Ã¨Â¾Â¾Ã¥ÄªÂ°', 'Ã¦Ä¹Â©', 'Ã¤Â¸Ä¯Ã¥Ä±Â¯', 'Ã¤Â¸Ä¢Ã§Ä½Â´', 'Ã¥Â·Â¨', 'Ã¦Ä¦ÅÃ¨Â°Â¢', 'Ä New', 'Ã¤Â¸Ä¢Ã¦Â®Âµ', 'Ä machines', 'Ã¥Â°Ä¨Ã¥Ä§Â¶', 'Ã§Â»Â§Ã§Â»Åƒ', 'Ä word', 'Ã§Ä«Â¹Ã¥ÄªÂ«', 'Ä agriculture', 'Ã¦Ä¢Ä°', 'Ã©Ä¢Ä²Ã¦Â¸Ä²', 'Ã©ÄµÂ¾', 'Ã¨Â¯Â¾', 'Ä kind', 'Ã¥Â¢Ä»', 'Ã¨Â°Â¢Ã¨Â°Â¢', 'Ä algorithm', 'Ã¨Â£Ä§Ã©Â¥Â°', 'Ä along', 'Ä easy', 'Ã¤ÂºÄ³', 'Ã¨Â§Â£Ã¥Ä¨Â³Ã¦Ä¸Â¹Ã¦Â¡Äª', 'Ä awareness', \"'ve\", 'Ã¦Ä¸Â¹Ã¥Ä²Ä³', 'Ä never', 'Ä quickly', 'Ä respect', 'Ã§Ä¼Ä¦Ã¦Ä»', 'Ä among', 'Ä accountability', 'Ä law', 'ening', 'Ä defin', 'Ä surround', 'Ã©ÄµÄ£', 'Ä powerful', 'An', 'Ä cause', 'Ã¦Â¥', 'Ã¦Ä°Ä®Ã¦Ä±Â¡', 'Ã¨Â¿ÄºÃ¦ÄºÂ¯', 'Ä creative', 'Ã¨Â¡Ä¢', 'Ä located', 'unning', 'Ã¥Ä¾Â°Ã¥Ä®Âº', 'Ã©Ä¿Â¢Ã§Â§Â¯', 'Ã©Ä½Â¨', 'Ä near', 'Ä initi', 'ression', 'Ã¤Â¸Ä­Ã¦Ä¿Â¥', '25', 'Ã©Â©Â¶', 'Â¾Ã§Ä¹Ä§', 'ables', 'Ã¦Ä¾Ä«Ã¨Â¶Â£', 'Ã¥Â¾ÂªÃ§Ä°Â¯', 'Ã§ÅƒÄ¶Ã¦Â¡Äª', 'Ã§Å‚Â´', 'ication', 'Ã©Ä»Â¢', 'Ã¦Â²Â»Ã§Ä¸Ä¹', 'Ä addition', 'Ã¤ÂºÄ­Ã¦Ä¥Ä§', 'Ä because', 'Ã¥Ä±Äª', 'Ã¨Ä¤Ä®', 'Ã§ÂºÂª', 'side', 'Ã¦Ä­Ä§', 'Ã¦Â¹Â¿', 'Ã¥Ä¯Ä¬', 'Ã©Â¡Âº', 'Ä And', 'Ä restaurant', 'Ä vide', 'Ä problem', 'azing', 'Ä members', 'Ä nut', 'Ä cou', 'Ã¦ÂµÂª', 'Ä Ã¨Â¿Ä»', 'Ä helping', 'Ä Is', 'Ã¦Ä±Ä²Ã¥Ä¯Ä©', 'Ä Ä Ä Ä Ä Ä ', 'Ä sho', 'Ä relev', 'Ä arg', 'Ä balance', 'illed', 'Ã¦ÄºÂ¯Ã¤Â»Ä¢Ã¤Â¹Äª', 'Ã¥Ä¬Ä½Ã©Ä©Ä±', 'ired', 'Ã¥Â¤Ä¾', 'Ã¥Ä±Â¯Ã¦Ä®Ä£Ã§Â»Åƒ', 'Ä perfect', '**', 'ification', 'Ã¦Â¶Ä«', 'Ä wildlife', 'ane', 'Ä related', 'Ã¥Â®Â¤Ã¥Ä¨Ä§', 'Ã¥ÂºÄ¾', 'Ã¤ÂºÂ«Ã¥Ä±Ä¹', 'ours', 'Ã¨Â·Ä³', 'Ã¥Ä·Ä¨Ã¤Â¸Ä¼', 'aching', 'Ä sun', 'Ä recognition', 'elt', 'Ä order', 'Ã¥Â¹Â³Ã¥Ä¿Ä©', 'ging', 'Ã¤Â¸Â´', 'Ã§Ä¤Â¼', 'Ä going', 'Ã¥Ä³Â¼Ã¥Ä²Â¸', 'Ä software', 'Ä remot', 'Ã¨Ä³Ä¹Ã¥Ä²Ä¯', 'Ã¥Â¹Â¸Ã§Â¦Ä±', 'Ä enhance', 'Ã¨Ä»Ä¼', 'Ä now', 'Ä threat', 'Ä dest', 'Ã¥Ä¿Ä©Ã¥Ä®Ä¢', 'Ä acad', 'Ã¥ÂºÄ¶Ã¥Â¯Â¹', 'Ã§Ä¾Ä­Ã¥ÄªÂ°', 'cast', 'Ã¨Â¾Ä¨', 'ificial', 'Ä very', 'ook', 'Ã¥Ä®ÂºÃ¥ÅÅ', 'Â¹Ä£', 'Ã¦ÄªÂ¿Ã©Ä¹Â´', 'Ã¦Ä±Ä²Ã¤Â¾Ä½Ã¤ÂºÄ¨', 'Ä motiv', 'Ä accessible', 'Ã¥Ä¨Â³Ã¥Â®Ä¼', 'Ä hy', 'Ã¥Â®Äª', 'Ä flo', 'ug', 'Ä informed', 'Ã¥ÄµÄ£Ã¨Â´Â¨', 'Ã§Ä¼Ä¦Ã§Å', 'aves', 'arr', 'Ä With', 'let', 'Ã¨Â§Ä¤Ã§Ä¤Â¹', 'enge', 'Ã¨Â¡Ä®Ã¥Ä¬Â¨', 'friend', 'Ã§Â³Ä·', 'Ä further', 'Ä Ens', 'Ã§Â§Ä£', 'Ä ado', 'Ä clean', 'Ã§Ä½Â¸Ã¥ÂºÄ¶', 'Ä fre', 'pecially', 'Ã¨Ä¹', 'Ä capt', 'Ã§Ä¼Ä¦Ã§Ä¾', 'Ä someone', 'Ä cell', 'Ã¦Ä¶Â¾Ã¥Ä¾Â¨', 'Ã¦Â¬Â¢Ã¨Â¿Ä°', 'Ä Ã¢Ä¢', 'Ä devices', 'Ã§Ä¼Ä¦Ã¦Ä¸Â¹Ã¥Â¼Ä±', 'Ä jobs', 'augh', 'not', 'Ã¦Ä¾Ä«Ã¤ÂºÄ½', 'Ã¥Ä§Â¬Ã¥Ä§Â±', 'gest', 'Ã§Ä¼Ä¦Ã§Ä¶ÅÃ¦Â´Â»', 'Ã§Ä¾Â¼', 'Ã§Ä¼Ä¦Ã¤Â¿Â¡Ã¦Ä£Â¯', 'Ä Cons', 'Ã¦Ä°Ä´Ã¥ÂºÄ±', 'Ä benefit', 'rect', 'Ã¥Â¤Ä±', 'unte', 'Ã§Â¬Â¦Ã¥Ä²Äª', 'Ã¤Â¸Ä¢Ã¤Â½Ä¯', 'Ã¥Ä¨Ä§Ã©Ä¥Â¨', 'Ä looking', 'ding', 'Ã¦Ä¬Äº', 'Ã¨Â¾Ä³', 'Ã¨Â¿Ä»Ã¤Â¸ÂªÃ©Ä¹Â®Ã©Â¢Äº', 'Ä especially', 'Ã§Ä¾Å‚', 'Ã¢Ä¢Ä¿Ã£Ä¢Ä¤', 'Ã¥Â¥Ä±', 'ray', 'Ã¨Â¿ÄºÃ¥Ä±Â¯Ã¤Â»Â¥', 'Ã¥ÄªÄ½Ã¤Â½Ä¾', 'coming', 'Ä multiple', 'Ã©Ä¼Ä²', 'Ã¦Â³Â¡', 'Ã¦Å‚Ä©Ã¥Ä©Ä¨', 'Ä mil', 'Ã©Ä¾Ä¢Ã¨Â¦Ä£Ã¦Â³Â¨Ã¦Ä¦Ä±', 'Ä anxiety', 'Ã¦Ä¶Â¹Ã¨Â¿Ä½', 'Ã¥Â±Ä­', 'Ã¦Â±Â¡Ã¦ÅÄµ', 'Ã§Â¼Ä¸Ã§Â¨Ä­', 'Ã¨Â´Â¹Ã§Ä¶Â¨', 'Ä evalu', 'imately', 'Ä liter', 'ograph', 'Ä search', '16', 'enced', 'Ä methods', 'Ã§Ä¥Äª', 'Ã¦Â¨Â¡Ã¥Â¼Ä±', 'Ã§Ä¬Â¶Ã¥Ä¨Âµ', 'Ã¦Ä¶Â¹Ã¥Ä¸Ä¦', 'Ã¥Â¤Ä¼Ã¦Å‚Â·', 'cer', 'Ã¥Â¥Ä¸', 'Ä satis', 'Ä website', 'Ã¥Ä¬Å€', 'Ã¥Ä£Â¥Ã¨ÂºÂ«', 'Ä global', 'Ä ask', 'Ä platforms', 'Ä diseases', 'Ã§Ä°Â°Ã¨Â±Â¡', 'tics', 'Ã¦Â±Ä£', 'Ã¥ÄªÂ¤Ã¦Ä¸Åƒ', 'Ä convers', 'Ä relationship', 'Ã¨Â®Â¾Ã§Â½Â®', 'Ã¦Â³Ä·Ã¥Â¾Ä­', 'Ä mindful', 'Ã©Â¢Ä¦Ã¦ÂµÄ­', 'overy', 'Ã¥Ä£Ä¾', 'Ã§Ä¶ÂµÃ¨Â§Ä¨', 'Ã¨Â§Ä¦Ã¥ÄªÄ»', 'aken', 'Ä implementing', 'ising', 'Ã¥Ä±Ä¤Ã¥Ä¬Å‚', 'Ã¦Ä¥Ä§Ã§Â»Âª', 'Ä provided', 'Ã¦Â·Â±Ã¥Ä§Â¥', 'Ä programmed', 'Ä relevant', 'Ã§Ä¼Ä¦Ã§Ä¥', 'Ã§Ä¸Â¾Ã§Ä¹Ä§', 'Ã¥Ä®Â»Ã§Ä¶Å', 'Ã¥ÄªÄ½Ã¥Â»Âº', 'Ä generate', 'Ã¦Ä¶Â¶Ã¥Ä§Â¥', 'Ã¤Â¼Ä³', 'izes', 'Ä transform', 'Ã©Ä£Âµ', 'astic', 'Ã¥Ä³Äª', 'Ã¦Â¯Ä±Ã¤Â¸ÂªÃ¤ÂºÂº', 'Ã¨Â¿Ä¶', 'iet', 'Ä voice', 'Ã©Ä¢Ä¶', 'Ã¦Ä¶Â¾Ã¦Ä¿Â¾', 'Ã¥Ä¯Â´', 'Ã¨Ä¥Ä¾', 'Ä structure', 'Ã¦Ä¹Â¶Ã¥Â°Ä¼', 'Ä Q', 'Ä else', 'duc', 'Ä emp', 'Ã¨Ä£Ä¼', 'Ã¨Â´Â§', 'aches', 'Ã§Â§Ä¢', 'anks', 'Ä night', 'Ä professionals', 'Ä bas', 'Ã¨Â´Âµ', 'ec', 'Ä diversity', 'ites', 'dr', 'Ã¥Ä½Â°Ã©Ä¼Â¾', 'Ä¥Ã¥Ä¾', 'Ã¥Å€Ä¥Ã¥Ä¾', 'Ã¥Å€Ä¥Ã¥Ä¾Â¾', 'Ä drug', 'Ã§Â¢Â³', 'Ä name', 'Ã¥Ä®Ä¸Ã§Ä¼Ä¦', 'aid', 'Ã¦Ä¾Ä¢Ã¥Â¤Â§', 'Ã¦Ä³Ä¦', 'Ã§Â®Ä¢Ã¥Ä¯Ä·Ã§Ä¼Ä¦', 'Ä warm', 'Ä done', 'Ä function', 'asc', 'Ã¥Â¼ÂºÃ¨Â°Ä¥', 'Ä demand', 'Ä visual', 'Ä upd', 'Ã¦ÅƒÂ£Ã¥Ä¾Â¨', 'Ä similar', 'Ã©Ä¢Ä´', 'Ã¦Â¯Ä½', 'Ã©Ä¶Â»', 'ently', 'Ä valuable', 'Ä disaster', 'Ã¤Â¸Ä¢Ã¨ÄªÂ¬', 'Ã¦Â´Â²', 'Ä Reg', 'Ä discrimination', 'Ã¥Ä¨Ä»Ã¤Â¸Ä¢Ã§Â¯Ä©', 'Ä government', 'Ä Ã¥Â¥Â½Ã§Ä¼Ä¦', '500', 'lying', 'Ä prev', 'Ä prepare', 'Ä problems', 'Ã¨Â·Â³', 'Ä prom', 'Ã¥Ä¨Â²', 'Ã¥Â®Ä«Ã¨Â£Ä§', 'Ã©Ä¶Â»Ã§Ä¤Â¼', 'Ã¦ÂµÄµ', 'Ã¨Â¹', 'Ã¥ÂºÄ¶Ã§Ä¶Â¨Ã§Â¨Ä­Ã¥ÂºÄ±', 'ng', 'Ä compet', 'Ã¥ÄªÄ¨Ã¥ÄªÂ«', 'ological', 'Ã¥Â®Â¡', 'Ä transl', 'Ä direct', 'Ã¥Ä«Ä¤', 'Ä suggestions', 'Ä paper', 'Ä recognize', 'ton', 'Ä mitigate', 'Ã¨Â®Â¨Ã¨Â®Âº', 'Ã¤ÂºÄ´Ã¥Ä¬Â¨', 'Ä Ear', 'Ä amazing', 'cre', 'Ã©Â¦Äª', 'Ä involved', 'face', 'Ã¦Ä¾Ä«Ã¥Ä§Â³', '))', 'Ä exce', 'Ä productivity', 'Ã¨Åƒ', 'Ã©Â¦Ä¨', 'Ä sounds', 'Ä identifying', '],', 'Ã©Â¾Ä»', 'Ä fit', 'Ä contribute', 'ths', 'friendly', 'ele', 'ified', 'iveness', 'itely', 'Ä X', 'Ä led', 'Ã¥Ä¿Ä±', 'Ä histor', 'Ä dat', 'Ä journey', 'Ä }', 'Ä select', 'Ã¦Â¼Â«', 'Ä conduct', 'Ã¨Â¿Ä½Ã¤Â¸Ä¢Ã¦ÅƒÂ¥', 'Ã§Â»Ä»Ã¦ÄªÄ³', 'Ä lif', 'Ã¨Â£Ä§Ã¤Â¿Â®', 'Ã¤Â¸ÂºÃ¤Â»Ä¢Ã¤Â¹Äª', 'Ã¤ÂºÂ¬', 'Ä nav', 'Ä whole', 'Ã§Â¹Ä£', 'Ã¥Ä¨Ä¾', 'Ã¦Ä¶Â»', 'Ä breat', 'Ä miss', 'Ã©Â¾Ä¦', 'tt', 'sw', 'Ä bar', 'Ã¨Â¯Â·Ã©Ä¹Â®', 'Ã¨Ä£Ä¶Ã§Â½Ä³', 'Ä attract', 'Ã¦Ä¤Â¨Ã¥Ä±Â¯Ã¤Â»Â¥', 'One', 'Ã¥Ä§Ä§Ã¥ÄªÄ¨', 'ring', 'Ä Ã¥Â½ÄµÃ§Ä¦Â¶', 'ream', 'Ä evol', 'Ä sn', 'Ä Em', 'mosp', 'Ä choose', 'view', 'Ä arr', 'Ä sleep', 'ended', 'Ã¦Å€Â¶', 'Ä vehicles', 'Ä fresh', 'Ä organization', 'Ã¨Â¿Ä»Ã¦Â®Âµ', 'Ã¦Â±Â¤', 'Ä Int', 'Ä context', 'Ã¥Ä±Â¦Ã¥Â¤Ä¸', 'Ä ocean', 'Ã¦Ä¦ÅÃ¥Ä±Ä¹', 'Ä pollution', 'urb', 'Ã¦Ä«Â§Ã¨Â¡Ä®', 'ersonal', 'Ä Health', 'Ã¤Â¼ÄºÃ§Ä¤Â¹', 'Ä attention', 'Ã¦Ä¾Ä«Ã§Ä¿Ä¢', 'Ã©Â£ÅÃ¦Ä¿Ä²', 'Ä err', 'Ã§Ä¼Ä¦Ã¦Ä¿Â¥', 'Ã§Ä¼Ä¦Ã§Äª', 'Ã¨ÅƒÂ¦', 'Ã¨Â·Å', 'Ã¦Ä¹Ä§Ã¨Â¡Ä®', 'Ã¨Ä´Ä¾', 'Ã§Ä¼Ä¦Ã¦Ä¢Ä¿', 'Ä chatbot', 'Ã§Ä¼Ä¦Ã©Ä¾Ä¢Ã¦Â±Ä¤', 'Ã§Ä·Â¥', 'Ä feeling', 'Ä implemented', 'Ã§Â¤Â¾Ã¥Ä®Âº', 'Ã§Ä¼Ä¦Ã¥Â»ÂºÃ¨Â®Â®', 'Ã¦Ä²Ä§', 'Ã©Ä¹Â»', 'Ã¥Ä±Ä¯Ã©Â¦Äª', 'Ã§Ä½Â´Ã¦Ä°Â¥', 'Ã¦ÄºÂ¥', 'itable', 'Ã¦ÄªÄ³Ã¤Â¼Ä¼', 'Ã¥Ä¯Â±', 'Ã¨Ä«Â¯Ã¥Â¥Â½', 'Ä living', 'Ã¥Ä±ÄºÃ©Ä©Ä±', 'Ä But', 'Ä complete', 'Ä trends', 'Ä makes', 'Ã¤Â»Ä¬Ã¥Â¤Â©', 'Ä distribut', 'Ä commit', 'Ä atmosp', 'Ã¤Â¼Â´', 'Ä sensors', 'Ä sw', 'Ã¦Ä¹Å‚Ã¨Â®Âº', 'omen', 'Ã¦Ä¶Â¿Ã¥ÂºÄ¾', 'Ä challenge', 'Ä turn', 'Ã§Ä²Ä¨Ã¨Â®Âº', 'par', 'Ä write', 'Ã§Â»Ä±Ã¥Ä§Â¸', 'emember', 'Ã©Â¥Åƒ', 'Ã¦Ä¸Â¹Ã¤Â¾Â¿', 'Ä cu', 'Ä value', 'Ä fund', 'pose', 'Ã¨Â°Ä¥Ã¦ÅÂ¥', 'Ã§Ä¿Â¡', 'Ä communicate', 'Ä disease', 'Ä researc', 'Ä lack', 'arning', 'Ä Park', 'Ã§Ä¦Â¦', 'Ã©Â«ÄºÃ¥ÂºÂ¦', 'Ä rather', 'Ã¥Â®Â£', 'Ã§ÄªÂ¶', 'Ã©ÄºÂ¶', 'Ã¨Â®Â¢', 'Ã§Ä¥Â§', 'Ä higher', 'Ä summary', 'Ä Aut', 'Ã§Ä¼Ä¦Ã¦Â³', 'Ä ele', 'isms', 'Ä reli', 'Ã¤Â¹ÅÃ¤Â¼Ä¼', 'fra', 'Ã¥Ä³Ä¬Ã¨Â¯Ä«Ã¦ÄªÄ³', 'Ã¦Ä¬Â½', 'Ä situations', 'Ä marine', 'Ã¦Ä¥Â³Ã¨Â¦Ä£', 'inci', 'inal', 'Ä gain', 'Ä difference', 'Ã¦Ä¾ÂºÃ¥Ä»Â¨Ã¤ÂºÂº', 'Ã¦ÂµÄ£Ã§Â¨Ä­', 'Ä Chat', 'Ã§Â½Ä³Ã§Â«Ä»', 'Ã¦Ä¾Â«', 'Ä color', 'Ä aspect', 'Ã§Â½Ä¹', 'Ä Educ', 'Ä deploy', 'Ä beauty', 'Ã¦Ä¤Â£', 'ruction', 'itut', 'Ã¦Ä¿Å', 'Ã¨Â®Â©Ã¦ÄªÄ³Ã¤Â»Â¬', 'Ã©Ä·Â¿Ã¥ÂºÂ¦', 'ules', 'Ã¦Â¶Ä«Ã¥Ä±Ä¬', 'Ä digital', 'Ä existing', 'Ä Or', '\\\\_\\\\_', 'Ä background', 'Ã§Ä¹Ä©', 'Ã¦Â¯Ä±Ã¥Â¤Â©', 'python', 'Ä farmers', 'Ä continu', '\":', 'Ä given', 'Ã¥Â°Ä±Ã¦Ä¹Â¶', 'Ä moment', '200', 'John', 'Ã©Ä¿Â¢Ã¥Â¯Â¹', 'Ä intro', 'Ä therapy', 'Ã¨Â¿Ä¶Ã¥Ä½Å€', 'Ã¥Â¹Â¶Ã¥Ä¾Â¨', 'Ä z', 'Ä afford', 'Ã¤Â¸Ä¿', 'Ã¥Â®Â½', 'Ä Ãƒ', 'Ä National', 'Ã¨Ä¥Â¡', 'Ä exercise', 'Ã¦Ä²Ä§Ã¦Ä­Ä®', 'Ã¦Ä¶Â¯Ã¤Â»Äº', 'Ã©ÄºÂ³Ã¥Ä§Ä«', 'Ã¨Â¯Ä¼', 'Ä sect', 'Ä Su', 'Ã¥Â¢Å€Ã©Ä·Â¿', 'Ã§Â¾Ä°Ã¤Â¸Â½', 'Ä wa', 'Ã¤Â»Â¥Ã¤Â¸Ä­Ã¦ÄºÂ¯Ã¤Â¸Ä¢Ã¤ÂºÄ½', 'Ã¨Ä½Ä­Ã§Â³Ä·', 'Ä ill', 'Ã¦Â¸Ä§Ã¦Ä»', 'etry', 'Ã¦Â¢Â¦', 'Ã§Â¾Ä°Ã¥Ä½Â½', 'Ã¤Â»Ä¯', 'oney', 'Ä ecosystems', 'Ã¦Ä®Ä©Ã¥Â¯Â¼', 'def', '99', 'Ã¦ÅÄ¶', 'pped', 'Ä limit', 'Ã§Ä°Ä«', 'Ä academic', 'Ä restaurants', 'Ä head', 'Ã¤Â¿Â¡Ã¤Â»Â»', 'asters', 'Ã¥Â²Ä£', 'akers', '14', 'As', 'Ã¦Å‚Â¡', 'Ã©Â«ÄºÃ¦Ä·Äª', 'phas', 'yn', 'Ã§Â¨Ä­Ã¥ÂºÂ¦', 'Ã¨Â¾Â£', 'Ã¤Â¸Ä¬Ã©Ä¿Â¢', 'Ã¥Â®Â¶Ã¥Â±Ä§', 'term', 'Ã§Â¾Ä°Ã©Â£Å', 'Ä overs', 'Ã¥Â®Äº', 'Ä indic', 'Ä Your', 'St', 'Ã¥Â½Â¢Ã¨Â±Â¡', 'Ã¨Â´Â¡', 'Ã¥ÂºÄ¬', 'Ä Sc', 'agra', 'Ã§Ä¾ÅÃ¦ÅƒÂ£', 'oint', 'ids', 'arent', 'Ã©ÄµÂ¶', 'Ã¨Ä£Ä¬', 'Ä regular', 'Ã¤Â¼ÄºÃ§Â§Ä¢', 'Ä colle', 'Ã§Ä¸Ä³', 'Ä subject', 'Ä greater', 'Ä store', 'Ã¥ÅÂ¹Ã¨Â®Åƒ', 'Ä imag', 'Ä answ', 'Ã¤Â½Ä»', 'Ä spot', 'Ã¥ÄªÄ¨Ã¥ÅƒÄ²', 'Ä audience', 'pet', 'Ä vers', 'Ä trail', 'Ã¥Ä­Ä©', 'erous', 'Ä guidance', 'Ä speech', 'Ã¥ÄµÂ²', 'Ã¦ÄºÂ¯Ã§Ä¶Â±', 'Ã¨Â´Â¡Ã§Ä®Â®', 'Ã¥Ä²ÄªÃ©Ä¢Ä¤Ã§Ä¼Ä¦', 'Ã¨Â®Â¾Ã¦Ä¸Â½', 'Ã¤Â»Ä¸Ã¤ÂºÂº', 'ensive', 'Ã¥Ä¢Â¾', 'aling', 'Ä projects', 'Ã¥Â³', 'Ä takes', 'Ã§Â»Â©', 'That', 'Ä bro', 'ived', 'Ä &', 'Ã¥Ä¿Ä²', 'placement', 'Ã¨Â¿Å€Ã¦Ä°Â¥', 'Ã§Ä¼Ä¦Ã§Â¤Â¾', 'Ä Tra', 'Ä relax', 'ufact', 'Ã©Ä£Ä¯', 'Ä surv', 'Ã¥Ä±Â£Ã¥Ä³Â³', 'Ä creativity', 'of', 'Ã¥Â¨Ä£', 'Ã§Ä¼Ä¦Ã§Å‚', 'Ä breath', 'Ä places', 'Ä describ', 'Ã¨Ä­Â±Ã¨Â¯Åƒ', 'Ä damage', 'oration', 'Ã¤Â¸ÂºÃ¦Ä¤Â¨', 'ift', 'Ä case', 'Ã¥Â¹Â´Ã©Â¾Ä¦', 'Ä press', 'Ã§Ä¶Ä¾', 'Ã©Ä©Ä°', 'Ã¦Ä¹Ä§Ã¦Â¸Â¸', 'Ä taken', 'ined', 'Ä concept', 'Ã¦Ä´Åƒ', 'Ä interesting', 'Ã¨Â·Âµ', 'Ä sea', '60', 'Ä foot', 'Ä Name', 'Ä researchers', 'Ã©Ä¢Ä£', 'Ä wee', ');', 'Ã§Ä¼Ä¦Ã¥Ä§Â³Ã©Ä¶Â®', 'Ã¤Â¼Â½', 'elebr', 'Ã¥Â¡Ä³', 'We', 'Ã§Â»Ä±Ã¥Â¸Â¸', 'Ä populations', 'Ã¥Ä§Â¬Ã¥Â¼Ä±', 'orn', 'Ã§Ä©Ä¥', 'Ã¤ÂºÂºÃ§Ä¶Å', '17', 'Ã¦Ä°Â¥Ã¥Ä±Ä¹', 'Ä location', 'Ä inequ', 'Ä intervent', 'Ä interested', 'Ä definitely', 'Ä assistance', 'Ã¨Â¿Ä»Ã¤Â¸Ä¢', 'Ã¥Ä²ÄªÃ¥Ä²Ä®', 'Ã¤Â¼ÄºÃ¥Ä¬Â¿', 'Ã§Ä¼Ä¦Ã¥Â·Â¥Ã¤Â½Ä¾', 'Ä 12', 'Ä mov', 'Ã¥Ä£Ä±', 'Ã¥ÅƒÄºÃ¥Ä¤Â¨', 'usive', 'Ã¦Ä¹Ä±', 'Ã¯Â¼Ä«Ã¯Â¼Ä®', 'Ä gas', 'Ä interests', 'Ã¦Â¸Ä§Ã¦Ä»Â°', 'Ä gard', 'Ã§Ä¸Â«', 'Ä say', 'Ã¥Â¤Â«', 'ges', 'Ã¨Ä²Â¨', 'Ã¤Â¸Ä¼Ã¥Ä¬Â¡', 'Ã¤Â¸ÂªÃ¦Ä¢Â§', 'Ã¥Ä²Â¯', 'Ä engagement', 'Ä big', 'Ã©Ä¾Ä¢Ã¨Â¦Ä£Ã¨Ä¢Ä¥Ã¨Ä»Ä³', 'Ä princi', 'Ã¥Ä³Â¨Ã¥Ä½Â´', 'Ä opportunity', 'Ã§Ä£Â¾', 'Ã¨Ä¹Ä±', 'rel', 'Ã§Â¼ÂºÃ§Ä¤Â¹', 'Ä happy', 'Ã¥Ä´Ä®Ã¥Ä§Â¶Ã¤Â»Ä¸', 'ava', 'Ä establish', 'Ã©Â¸Â¡Ã¨Ä½Ä­', 'iking', 'Ä Trans', 'rastructure', 'forest', 'Ã¨Ä°Â·Ã¥Ä±Ä¸', 'Ã¨Ä¦Ä¼', 'inally', 'Ã¨ÂµÄ±', 'Ä delicious', 'Ä results', 'Ã¨Â§Ä¤Ã¥Â¯Å', 'Ã¥Â®Å€Ã¨Â·Âµ', 'Ä last', 'Ä polit', 'Ã¦Ä¢Â§Ã¨Ä¥Â½', 'For', 'bi', 'Ã§Ä½Â¸Ã¤Â¿Â¡', 'ffee', 'Ä phr', 'Ä forest', 'elling', 'Ã¦ÂµÄ£Ã¨Â¡Ä®', 'atic', 'Ã¥Â¤Â§Ã¥Â®Â¶', 'Ä Inst', 'Ã¦Ä·Â°Ã¥ÅƒÂ¦', 'Ã¦Ä«Â©', 'Ã¥Â®Ä®Ã¥Ä§Â¨', 'Ã¥Â¼Ä·Ã¨ÂµÂ·', 'ese', 'Ã¨Â½Â¬Ã¦Ä¯Â¢', 'Ä affected', 'Ä robotics', 'Ã§Â»Â¼Ã¤Â¸Ä¬', 'Ä prop', 'Ã¨Â®Â©Ã¤ÂºÂº', 'Ã¦Â²Â³', 'Ã¤Â¸ÅƒÃ¦Ä¾Ä¢', 'Ä autonomous', 'Ä having', 'Ä trip', 'ury', 'Ä biased', 'Ä considerations', 'Ä particular', 'Ã¥Ä¯Å‚', 'Ã¦Ä°Â¨Ã¥Â¹Â¿', 'Ä initiatives', 'ials', 'Ã¥Ä³Â³Ã©Ä£Äµ', 'Ä treatments', 'Ä emphas', 'Ã§Ä­Â¬Ã§Ä«Â¹Ã§Ä¼Ä¦', 'Ä lay', 'Ã¦Ä¶Â¿Ã§ÅƒÄ¸', 'Ã¦Ä¢Ä°Ã¤Â¹Äª', 'ronic', 'play', 'Ä cook', 'Ã¨Â¿Ä½Ã¥Ä§Â¥', 'Ã¨Â½Â®', 'Ä volunte', 'Ä rain', 'Ä Mon', 'Ä consumption', 'Ã¨Ä½Ä­Ã§Ä»Â½', 'Ä Soc', 'Ã¥Â£Â¤', 'Ä routine', 'Ä improved', 'To', 'Ã¤ÂºÂºÃ§Ä«Â©', 'Ã¨Â¯Â»Ã¨Ä¢Ä§', 'Ä goal', 'Ã¥Â¹Â¿Ã¥Ä³Ä¬', 'Ã©Ä·Â¿Ã¦Ä¾Å', 'Ä ey', 'He', 'Ä outdo', 'Ä cuis', 'Ä away', 'Ä books', 'Ä topic', 'Ã¥Â¤Â§Ã¥ÄªÂ©', 'house', 'Ä ones', 'Ã§Â§Å', \"':\", 'Ã¦ÄªÂ¿Ã¥Â±Ä­', 'Ã§Â§Â»Ã¥Ä¬Â¨', 'Ä disasters', 'ests', 'illing', 'Ã§Â»Â¿Ã¨Ä«Â²', 'Ã¥ÄµÂ²Ã¥ÅƒÂ¦', 'Ã¦ÄªÄ²Ã¥ÄªÄ¨', 'Ä occur', 'Ä¾Ã¤Â¼Â½', 'Ã¥Ä¾ÅÃ¥Â£Â¤', 'Ã§Ä¼Ä¦Ã¤Â¸Â»Ã¨Â¦Ä£', 'Ã§Ä°Â°Ã¥Â®Å€', 'Ä animal', 'Ã©Â¢Ä¨Ã¥Â¯Â¼', 'Ä views', 'Ã©Ä¤Â®', 'Ã¦Â°Â§Ã¥Ä®Ä¸', 'athy', 'Ã©Ä£ÄµÃ¥Â¾Â·', 'Ã§Â¤Â¾Ã¤ÂºÂ¤Ã¥ÂªÄ´Ã¤Â½Äµ', 'Ä Personal', 'Ä½Ã¥Ä½Â´', 'Ä purch', 'Ä country', 'Ä remind', 'Ã¥Â¯Â¸', 'Ä rights', 'Ã§Ä¼Ä¦Ã§Ä°Â¯Ã¥Â¢Ä¥', 'Ä Pr', 'Ä line', 'ibr', 'Ã©Â©Â¾', 'Ä maj', 'Ä overcome', 'Ä next', 'Ã¦Ä«Ä¢Ã¨Â¿Â°', 'Ã¨Â§Ä¦Ã¥Â®Ä¼', 'Ä interactions', 'Ä conflic', 'Ä why', 'Ã§Â³Â»Ã¥ÄªÄ¹', 'Ã¥Â°Â¼', 'ibly', 'Ã§Ä«Ä½Ã¥Â¥Â¶', 'Ä responses', 'ses', 'Ã¥ÅƒÂ¦Ã¤Â¼Ä¼', 'bol', 'Ä standards', 'ulner', 'Ã¥Â¯Â¹Ã¨Â¯Ä¿Ã¥Ä¨Ä§Ã¥Â®Â¹', 'lished', 'Ã§Ä¼Ä¦Ã¦Ä¢Â§', 'Ã§Ä¶ÅÃ¦Ä¢Ä£Ã§Â³Â»Ã§Â»Å', 'ann', 'Ã¦Ä¥Ä§Ã¥Ä¨ÂµÃ¤Â¸Ä­', 'Ã¥Â¯Â»Ã¦Â±Ä¤', 'Ä hold', 'den', 'Ã¥Ä¯Ä¥', 'Ä mention', 'Ä Many', 'Ã§Ä½Â´Ã¥ÄªÂ°', 'Ã©Ä£Ä¹', 'hel', 'Ä believe', 'aries', 'Ã¦Ä¾Ä«Ã¤Â¸Ä¢Ã¤Â¸Âª', '13', 'Ä atmosphere', 'Ä mor', 'Ã¦Ä¹Â¥Ã¦Ä¾Å', 'Ã¤Â¹Ä§', 'Ã¤Â½Å‚Ã¥Â¥Â½', 'Ä addressing', 'Ä Ã¢Ä¢Äµ', 'Ã§Ä¼Ä¦Ã¥Ä¾Â°Ã¦Ä¸Â¹', 'ming', 'Ä cannot', 'Ä manufact', 'Ä pie', 'icing', 'Ä studies', 'Ã§Â¾Ä°Ã¥Ä³Â³', 'Ä American', 'Ä NLP', 'Ä according', 'mselves', 'Ã¨Ä¦Ä¤', 'Ã¨Ä©ÂªÃ¤Â¿Â¡', 'Ã¦Ä«Ä¢Ã©Ä¾Ä¢', 'Ä themselves', 'Ä remote', 'Ã¥ÅÂ¹Ã¥Ä§Â»', 'Ã¥Â®Ä«Ã¦Ä°Ä´', 'Ã¤Â½Å‚Ã©Ä¾Ä¢Ã¨Â¦Ä£', 'Ä regard', 'iring', 'Ã¨Â¯Ä¨Ã¥ÄªÂ«', 'Ä article', 'Ã¦Ä£Ä´', 'Ã¦Ä¢Â»Ã§Ä¼Ä¦Ã¦Ä¿Â¥', 'Ä align', 'Ã¦Â±Å‚', 'tenance', 'faction', 'Ã¥Ä¬Â¨Ã¤Â½Ä¾', 'Ã§Ä¼Ä¦Ã§Â©', 'Ã§Â¼Â©', 'Ã¦Ä¢Â¥', 'Ä 100', 'Ä testing', 'Ã¥ÅƒÄ¹Ã¦Â¯Ä¯', 'Ã¥Â¹Â´Ã¨Â½Â»', 'Ã¥ÄªÂ¶Ã©Ä¢Å‚', 'Ä swe', 'Ã¥Â°Âº', 'hens', 'Ã¦Â°Â´Ã¦Å€Ä¾', 'Ä infrastructure', 'Ã¨Ä«Â²Ã¥Â½Â©', 'Ã¦Ä¢Â»Ã§Ä¼Ä¦Ã¦Ä¿Â¥Ã¨Â¯Â´', 'Ã¦Ä¾Ä«Ã¤Â»Ä¢Ã¤Â¹Äª', 'text', 'Ã¨Â½Â¦Ã¨Â¾Ä¨', 'Ä pay', 'rop', 'ÄŠÄ Ä ', 'Ä caused', 'Ä correct', 'Ä Ã¬', 'Ã¨Ä¥Å€', 'Ä Med', 'Ã§Â²Â¾Ã§Â¥Å€', 'Ã¦Â°Ä¶Ã¥Ä¢Ä»Ã¥Ä±ÄºÃ¥Ä®Ä¸', 'Ä Red', 'Ã¤ÂºÄ´Ã¨Ä£Ä¶Ã§Â½Ä³', 'Ä engage', 'Ã¥ÄªÄ¨Ã¤Â¸Âº', 'Ä Data', 'Ä full', 'enc', 'Ã©Ä©Ä¯Ã¦Ä¸Â°', 'Ã¦ÅƒÂ£Ã§Â¡Â®Ã§Ä¼Ä¦', 'Ã§Ä¼Ä¦Ã¦Â°Ä¶', 'Ã¥Ä±Ä®Ã¦Ä¸Â¹', 'Ä comes', 'Ã¥Ä±Â¤Ã¤Â»Â£', 'Ã¦ÅÄ²Ã¤ÂºÄ½', 'Ã¥Ä³ÄªÃ§Ä°Â°', 'Ä today', 'aged', 'Ã¦ÄªÄ³Ã¥Ä±Â¯Ã¤Â»Â¥', 'Ã¦Ä¹Â¥Ã¥Â¸Â¸', 'Ã¦Â»Ä³', 'Ä clin', 'Ä \\\\', 'Ä obs', 'Ä artificial', 'Ä excell', 'Ã§Ä¼Ä¦Ã§Â¬', 'alls', 'Ä produce', 'Ä Des', 'oss', 'Ã¨Â¹Äª', 'Ä draw', 'Ä letter', 'Ä advice', 'Ä highly', 'Ã§Ä¬Â¯', 'Ã§Â»Â¼Ã¤Â¸Ä¬Ã¦Ä«Ä¢Ã¨Â¿Â°', 'Ã¦Â»Â¡Ã¦Ä¦Ä±', 'Ä principles', 'Ã¨Ä®Ä¦', 'Ä feelings', 'Ã§Ä¼Ä¦Ã¦Â´', 'Ä hom', 'Ä fail', 'Ä crop', 'Ã¥Â§Ä¾', 'Ä question', 'Ä disabilities', 'Ã¨ÄªÅ€Ã¨Â¹Äª', 'Ä implications', 'ral', 'Ä sing', '40', 'Ä famil', 'Ä governments', 'Ä record', 'Ã¥Â½Â¢Ã§Ä¬Â¶', 'Ä begin', 'ises', 'Ã§Ä¼Ä¦Ã¦Ä¥Â³', 'achine', 'Ã¨Â°Â±', 'Ä vulner', 'Ä proper', 'Ä oversight', 'Ã¨Â´ÅÃ©Ä¿Â¢', 'Ä email', 'Ä news', 'Ä exploring', 'Ä favor', 'Ã¦Â¥Â¼', 'Ã¥Â®Ä¾', 'Ä univers', 'Ã¥Â·Â®Ã¥Â¼Ä¤', 'Ã¯Â¼Ä«Ã£Ä¢Ä¤', 'Ã¨Â§Â£Ã¥Ä¨Â³Ã©Ä¹Â®Ã©Â¢Äº', 'Ä famous', 'gn', 'Ä message', 'atitude', 'Ä cra', 'Ä cover', 'Ã¦Â·Â±Ã¥ÄªÂ»', 'Ã¥Ä±Â¯Ã¤Â»Â¥Ã©Ä¢Ä«Ã¦Ä­Â©', 'Ã§Ä¶ÅÃ¦Â´Â»Ã¤Â¸Åƒ', 'Ã§Â§Ä¯Ã§Â±Â»', 'Ä smart', 'onstr', 'vey', 'Ã§Ä¶Â²', 'Ä regularly', 'Ä Sm', 'Ã¦Ä¦ÅÃ¨Â§Ä«', 'Ä thought', 'Ä exh', 'cure', 'Ã§Â»Äº', 'Ã¨Â®Â¤Ã¨Â¯Ä¨', 'Ä old', 'Ã¦Ä¦Ä«', 'Ã§Â§Â°Ã¤Â¸Âº', 'Ä fields', 'Ä consist', 'Ã£Ä£', 'Ã§Â»Ä¨Ã¨Ä¥Å€', 'Ä hours', '80', 'alking', 'Ã¨Â§Ä«Ã¥Â¾Ä¹', 'Ã§Â»Ä¿', 'Ã¤Â½Å‚Ã¤Â»Â¬', 'Ä English', 'Ä significantly', 'Ä source', 'Ä ant', 'Ä educational', 'Ä task', 'Ä handle', 'Ã¦Ä²Ä¾', 'Ä Sp', 'Ä called', 'Ä terms', 'Ã¦Â²Ä«', 'Ä win', 'duction', 'Ä modern', 'Ä cuisine', 'Ã¥Â¥Ä¹', 'Ã¨Â§Â¦', 'olutely', 'Ã§Â«Â¥', 'pite', 'Ä felt', 'Ä compre', 'Ä wond', 'Ã¨Â¿Ä²Ã¨Â¡Ä®', 'Ä resil', 'Ã§Ä½Â¸Ã¤Â¼Â¼', 'Ã©Ä©Ä³Ã¨Å€Ä¯', 'Ã§ÄªÂ±Ã¦Ä¥Ä§', 'Ã§Â¬Ä¶', 'Ã¨ÄªÂª', 'Ã¨Â°Äª', 'Ã¥Ä¬Ä½Ã§Ä¼Ä¦', 'Ã¦Ä¾Ä«Ã¦Ä«Ä¢', 'Ã¦Â½Ä¾', 'ulate', 'Ä detection', 'Ã¥Â®Â£Ã¤Â¼Å‚', 'Ä matter', 'Ã©Ä©Ä±Ã¥ÅƒÄ²', 'Write', 'Ã§Â»ÄµÃ¥Ä²Äª', 'Ã§Â»Ä±Ã¨Â¿Ä©', 'Ä developers', 'Ã¨Âª', 'Ä ---', 'Ã¤ÂºÂºÃ©Ä»Ä§', 'Ã§ÅƒÂ¾', 'Ã¯Â¼Ä¼Ã¢Ä¢Ä¾', 'Ä innovative', 'Ã£Ä¢Ä¤Ã¢Ä¢Ä¿', 'Ã¥Â½Â¼', 'Ã©Â¥Â¼', 'Ã¨Â¿Ä©Ã¥ÂºÂ¦', 'Ä planet', 'Ã¥Ä§Â°', 'Ã¥Â¸Ä£', 'Ã¦Ä·Â¬', 'Ä legal', 'Ä lot', 'Ã¦ÄªÄ²Ã¤Â¸ÂºÃ¤ÂºÄ¨', 'iate', 'Ä mis', 'Ã¥Ä£Ä©Ã¨Â®Â¾', 'Ã§Ä¼Ä¦Ã¦Ä¸Ä©Ã§Â«Å‚', 'Ä Compan', 'Ä doc', 'Ä careful', 'Ä ever', 'Ã¦ÄªÄ³Ã¤Â»Â¬Ã¥Â°Ä¨', 'Ã¤Â¾Ä­Ã¥ÅƒÄ²', 'Ã¤Â¹Â³', 'Ã¤Â½Ä¾Ã¨Ä¢Ä§', 'Ã¥Ä²Â§', 'Ã¦Ä¼Â´', 'Ä remember', 'Ã§Ä½Â®Ã§Ä¼Ä¦', 'Ä put', 'Ã¥Â¸Â¸Ã¨Â§Ä£Ã§Ä¼Ä¦', 'Ä fest', 'Ã¥Â»ÂºÃ¨Â®Â¾', 'Ã¥Â®Å€Ã§Ä¶Â¨', 'Ä active', 'Ã§ÂªÄ¹', 'outh', 'Ã¥Ä°ÅÃ§Ä²Ä¨', 'Ä trying', 'Ã¨Â¿Â·', 'Ã§Ä½Â¸Ã¥Ä²Ä®', 'Ã©Ä§Ä´Ã¥ÂºÄ¹', 'Another', 'Ã¦Ä¾Ä¢Ã¤Â½Â³', 'Ä analytics', 'Ä perpet', 'ipment', 'Ä Ã¥Â¦Ä¤Ã¦Å€Ä¾', 'Ã¨Â§Ä¤Ã¤Â¼Ä¹', 'Ä celebr', 'Ä heav', 'Ä meditation', 'Ã¥Â¤Â§Ã¦Â°Ä¶', 'And', 'Ã¤Â¸Ä¯Ã©Ä¶Ä»', 'Ä whether', 'set', 'Ä demonstr', 'Ã¤Â¸Ä¢Ã¦Â¬Â¾', 'Ã¦Ä¶Â¶Ã©Ä½Ä¨', 'Ã©Ä»Ä²Ã¥ÄªÂ¶', 'Ä ing', 'Ä revolution', 'Ã§Ä¾Ä£', 'Ä science', 'Ã§Ä½Â®Ã¥Ä«Ä¯', 'Ä thinking', 'Â±Ã¤Â¹Ä²', 'Ã¨Â¯Â¾Ã§Â¨Ä­', 'Ä pack', 'Ä image', 'loc', 'Ä stories', 'uck', 'Ä satisfaction', 'Ä collection', 'ho', 'Ã¨ÂµÅ€', 'Ã©Ä¿Â¢Ã¤Â¸Â´', 'Ä la', 'Ä symbol', 'Ä emb', 'Ä habitats', 'Ä lower', 'Ä continues', 'Ã©Ä¾Ä©', 'Ã¥ÄµÄª', 'Ä Take', 'Ä environments', 'Ä three', 'Ä enc', 'Ä Acc', 'Ã¦Ä¦Ä±Ã¥Ä³Â³', 'Ã¥Ä°Â¨', 'chan', 'Ä Hum', 'Ä true', 'Ã¥ÄªÄ©Ã¦ÄªÄ²', 'sing', 'Ã¢Ä¢Ä¶Ã¢Ä¢Ä¶', 'Ã¥Ä©ÂºÃ¦Ä¿Â¥', 'Ä region', 'Ä interpre', 'Ä diagnosis', 'Ã©Å€', 'Ä doing', 'Ä run', 'Ä coffee', 'Ä major', 'Ä mindfulness', 'Ä affordable', 'Ã§Ä»Â¾', 'Ä detailed', 'Ã©Ä¿Å€Ã¥Â¸Â¸Ã©Ä©Ä¯Ã¨Â¦Ä£Ã§Ä¼Ä¦', 'Ã§Ä¼Ä¦Ã¦Â²ÅÃ©Ä¢Ä¼', 'Ã§Ä¼Ä¦Ã¦Ä·Ä§', 'Ã¥Ä¢Ä´Ã¥Ä§Â¥', 'Ä themes', 'Ä network', 'Ã¯Â¼Ä«Ã¯Â¼Ä¼', 'Ä United', 'Ã§Ä¼Ä¦Ã¦Ä®Ä©', 'orts', 'Ã¥Ä¯Â«Ã§Ä¶Å', 'Ä planning', 'Ã¦Ä¥Å‚', 'Ã¥Ä«Âª', 'Ä Prov', 'Ã§Ä¼Ä¦Ã¥ÂºÄ¶Ã§Ä¶Â¨', 'Ä peri', 'Ä accountable', 'Ã§Ä«Ä»', 'Ã§Ä¼Ä¦Ã§Ä£', 'Ä choice', 'Ä Comm', 'idents', 'Ã§Ä¼Ä¦Ã¥Â®Ä«Ã¥Ä§Â¨', 'Ã¥Â¹Â¶Ã¤Â¸Ä¯', 'Ã¥Â¤ÂªÃ©ÄºÂ³Ã§Â³Â»', 'Ä receive', 'Ä close', 'Ã§Ä¼Ä¦Ã¦Ä¹Â¶Ã¥Ä¢Ä»', 'Ä changing', 'Ã¤Â»Â·Ã¥Ä¢Â¼Ã¨Â§Ä¤', 'Ä perpetu', 'Ä season', 'Ä men', 'Ä learned', 'Ä situation', 'Ä replace', 'head', 'Ã¨Â®Â©Ã¦ÄªÄ³', 'Ã¥Ä¾Â¨Ã¤Â¸Ä¢Ã¨ÂµÂ·', 'Ã§Ä¼Ä¦Ã§Â©Âº', 'Ã©Ä¾Â²', 'Ä enough', 'Ã¥Â±Ä·Ã§Ä°Â°', 'Ä leaders', 'ancing', 'Ä temperature', 'Ã¥Ä±Â«', 'Ä 30', 'Ã¦Ä¦Ä±Ã¥Ä³Â³Ã§Ä¿Ä¢', 'Ã¦Â±Ä©', 'Ä Govern', 'Ä focused', 'uro', 'Ä simple', 'Ä hiking', 'Ã¦Â¯Ä´', 'Ä comprehens', 'Ã¤ÂºÄª', 'Ä created', 'cond', 'Ã©Â¡Âµ', 'Ä Wor', 'Ã¨Â¯Ä£Ã¦Ä¯Â®', 'Ä workplace', 'Ä characters', 'Ã§Ä¼Ä¦Ã¨Â®Â¾Ã¨Â®Â¡', 'Ä mechan', 'Ä Dis', 'Ã§Â¥Å€Ã§Â§Äº', 'Ã¥Â·Å€', 'Ä On', '</', 'Ã§Â§Ä¯Ã¦Â¤Ä¯', 'Ä path', 'Ä limited', 'Ä solar', 'Ã§Ä¼Ä¦Ã¦Ä±', '22', 'Ä appreciate', 'Ã¥Â¿Â«Ã¤Â¹Ä²', 'Ã¦Ä¦ÅÃ¥Ä±Ä¹Ã¥ÄªÂ°', 'Ã¨Ä¢Ä¹', 'med', 'icine', 'Ä note', 'Ã¥Â½ÄµÃ¥Ä«Ä¯', 'Ã¦ÄªÄ³Ã¤Â»Â¬Ã¥ÂºÄ¶Ã¨Â¯Â¥', 'Ä seen', 'Ã¤Â¸Ä¢Ã¥Ä²Ä¯', 'Ã¥Â°Â½Ã¥Ä±Â¯Ã¨Ä¥Â½', 'Ã¨Â¿Ä²Ã§Â®Ä¹', 'Ã¨Â§Ä´Ã¥ÂºÂ¦', 'Ä equipment', 'Ä spread', 'Ã¨Â¸', 'Ã¨Â®Â¿', 'Ã¥Ä±Â¥Ã¨Â¯Ä¿', 'Ã¦Ä®Â¥', 'Ä purpose', 'Ã¨Â¯Â·Ã¤Â½Å‚', 'Your', 'arian', 'Ã¤Â»Âª', 'Ä perspectives', 'Ã¥Ä©ÂºÃ¤ÂºÄ¨', 'Ã¥Â©Ä¼Ã§Â¤Â¼', 'Ä excellent', 'Ä Ensuring', 'Ä reach', 'Ã©ÄºÂ¶Ã¦Â®Âµ', 'Ã¤Â¿Ä¿Ã©Ä¼Ä¾', 'Ä empathy', 'Ä My', 'Ã§Ä³Ä¾Ã¤Â¼Â½', 'Ä ver', 'abel', 'Ä Predict', 'Ä maintenance', 'Ã¨Â¯Ä¦Ã¤Â»Â·', 'Ä ult', 'Ã¥Ä´Â¨', 'ox', 'Ã¥Ä´Â¨Ã¨Â¯Â¢', 'Ä shared', 'ina', 'list', 'Ä outdoor', 'Ä thoughts', 'inating', 'Ã©Ä´Â±', 'Ä frame', 'Ã©ÄºÂ¿', 'Ã¥ÄªÂ©Ã¦Â¶Â¦', 'Ã§Ä¼Ä¦Ã¦Ä°Â¨', 'Ã¥Ä¯Ä¼', 'Ä recent', 'Ä altern', 'ared', '==', 'Ä road', 'Ã¤ÂºÄ­Ã©Â¡Â¹', 'ged', 'ynt', 'Ä spend', 'Ã§Â½Âª', 'Ã¥Ä±Ä¸Ã¥Â¾Ä¹', 'Ã©Â¹', 'li', 'Ã¦Ä¹Â¶Ã¦Ä¾Å', 'Ã¤Â¸Â¥Ã©Ä©Ä¯', 'Ã¥Â¿Ä¨', 'Ã¥Â©Â´', 'Ã¦Ä°Â¥Ã¤Â¸Ä­Ã¦Ä¿Â¥', 'Ä Earth', 'Ä Chatbots', 'Ä setting', 'Ã§Â¥Ä¿', 'Ã©Ä¶Ä¢Ã¥Ä¶Â®Ã©Â¢Ä¿', 'Ã¤Â¼Â¦', 'Ä reading', 'Ã¦Ä°Â¢Ã¨Â®Â¨', 'aign', 'Ã©Å€Ä­', 'Ä young', 'Ä career', 'Ä teachers', 'Ã§Ä¼Ä¦Ã¨Â´Â¨Ã©Ä©Ä±', 'Ã¥Â±Å€Ã¤ÂºÄ°', 'Ä easier', 'Ä scientific', 'Ã§Â¾Ä°Ã¥Ä§Ä¥', 'Ä spir', 'Ã¥Ä¬Â³', 'Ã§Ä¼Ä¦Ã¦Ä¶Â¯', 'rist', 'Ã¨ÂµÄ¦Ã¤ÂºÂ§', 'Ã§Ä¶ÅÃ¥ÅƒÄº', 'Ã¨Ä©Â³Ã¥Â°Ä³', 'Ã¥Â§Â¿', 'Ä video', 'Ä aim', 'Ã¥Â®Ä¿Ã¥Â®Ä¿', 'Ã§ÄªÂ¶Ã¦Â¯Ä¯', '________________', 'alities', 'Ä bud', 'Ä street', 'Ä Ã¦ÄºÂ¯', 'Ã¦Ä¸Â¹Ã§Â¨Ä­', 'Ã¤Â¸Ä¸Ã§ÂºÂª', 'ches', 'earch', 'Ã¦Ä´Â°', 'Ä engine', 'Ä displacement', 'Ä Robots', 'ervised', 'Ã©Â¡Â¶', 'oud', 'Ä walk', 'Ä emergency', 'Ã¨Ä£Äº', 'nal', 'Ä datas', 'Ã¥Ä¢Âº', 'Ã¥Ä²Ä°Ã§Ä¼Ä¦', 'Ã¥Â¾ÄªÃ¥Â¥Â½', 'Ä myself', 'Ã§Ä¼Ä¦Ã¦Ä«Ä­', 'Ä usage', 'Ä shown', 'Ã¦Â®Ä¬', 'Ä typically', 'uly', 'Ã¦Ä¸Â°Ã©Ä¹Â»', 'Ã¦Ä½Â¿', 'Ä orig', 'Ã¨Â½Â»Ã¦Ä¿Â¾', 'Ã¦ÄºÂ¾Ã§Â¤Âº', 'Ä adopt', 'Ã¨Ä¤Â¡Ã§Â¥Â¨', 'Ä parent', 'aps', 'Ã¦Ä¢Ä¿Ã¦Ä¥Â³', 'Ä marketing', 'Ã¨Ä»Â«', 'Ã©Ä¥Â¨Ã©Ä¹Â¨', 'Ã§Ä¼Ä¦Ã¦Ä·Äª', 'Ä comfortable', 'Ã¥ÅƒÂ¦Ã¤Â¹Å‚Ã¥Ä´Ä®', 'Ä forecast', 'iction', 'Ä getting', 'Ä trees', 'aving', 'Ã§Ä¼Ä¦Ã¥ÅÂºÃ§Â¡Ä¢', 'ready', 'Ã¦Ä¸Â°Ã©Â²Ä¾', 'going', 'Â¹Ã©Â¥', 'Ä evidence', 'Â¹Ã©Â¥Âª', 'Ã§Â§Ä­', 'Ã¦Ä¾Ä«Ã¥Â¾ÄªÃ¥Â¤Ä¼', 'Ã©Ä¿Â¢Ã¨Â¯Ä·', 'Ã©Ä£Ä©Ã¥ÄªÂ°', 'Ã§Â»Ä»Ã¥Â®Ä¼', 'irc', 'Ã¥Ä±Â¯Ã¤Â»Â¥Ã¦Å‚Â¹Ã¦Ä¯Â®', 'Ã©Â©Â¾Ã©Â©Â¶', 'Ã¥Â·Â§Ã¥Ä§Ä­', 'Ä stunning', 'Ã§Ä¼Ä¦Ã¦Â¦Ä¤', 'Ã¦Â¡Ä®', 'Ä John', 'ulation', 'Ã¥Ä±Ä¤Ã¨Ä¢Ä¥', 'Ä flex', 'Ã§Ä¦Â¦Ã¨Ä»Ä³', 'ymakers', 'Ä forms', 'sh', 'val', 'Ä So', 'co', 'Ã¦Ä°Â¨Ã¥Ä¬Â¨', 'Ã¨Ä§Â¿', 'Ã§Ä«Â¹Ã¦Â®Ä¬', 'Ä enab', 'Ã¥Â°Ä¨Ã¤Â¼Ä¼', 'Ã¦Ä¶Â¯Ã¥Ä©Âº', 'Ã¥Ä¿Ä¼Ã¦Ä®Ä£', 'Ã§ÂºÂ¢Ã¨Ä«Â²', 'Ä option', 'Ä started', 'ration', 'Ä poetry', 'Ä port', 'gen', 'Ã¨ÂªÄ«', 'Ä deliv', 'Ã§Ä¶Ä¼', 'Ã©Ä¢Â»', 'Ã©Ä¢Ä«Ã©Â¡Â¹', 'Ä ground', 'Ã¥Â½Â¼Ã¦ÅƒÂ¤', 'ana', 'Ã§Ä¼Ä¦Ã¦Ä¹Â¥', 'Ã¥Ä¾Â¨Ã§ÂºÂ¿', 'Ä secure', 'Ä Ã¦Å‚Â¹Ã¦Ä¯Â®', 'Ã©Â¥Â®Ã¦Ä¸Ä»', 'Ä gratitude', 'Ã§Â¬Â¬Ã¤Â¸Ä«', 'Ä song', 'Ä points', 'Ä already', 'Ã§Ä¼Ä¦Ã§ÄªÂ±', 'Ä Techn', 'Ä reality', 'Ã§Ä±Åƒ', 'Ä since', 'Ä population', 'yond', 'bor', 'Ä Social', 'Ã¦Ä±Ä²Ã¥Ä±Ä¸', 'Ã¥Â·Â¥Ã§Â¨Ä­', 'aff', 'Ã¤ÂºÂ¤Ã¦ÄºÄµ', 'Ä worth', 'Ã¥Â¡Â«', 'Ã¥Â¨Â±Ã¤Â¹Ä²', 'Ä dog', 'Ä Art', 'Ã§Â¡Â¬', 'Ã¦ÂµÂ·Ã¦Â´Ä­', 'Ã¥Ä¨Ä´', 'Ã§Ä«Äª', 'Ä programming', 'Ä Ass', 'Ä Machine', 'Ã¥Ä¢Â¼Ã¥Â¾Ä¹', 'Ã¨Â¯Â·Ã¨Â¾ÄµÃ¥Ä§Â¥', 'Ã¥Â£Â°Ã©ÅÂ³', 'Ä exercises', 'Ã¥Ä§Ä«Ã§ÂºÂ¿', 'Ã¦Â³Ä·Ã¥Ä´Ä®', 'Ä feature', 'eff', 'Ã¨Â¿Ä½Ã¦ÅƒÂ¥', 'Ã¥Â¥Â³Ã¦Ä¢Â§', 'Ä efficiently', 'Ã§Ä¼Ä¦Ã¦Ä¬Ä¢Ã¦Ä¾Â¯', 'Ä genetic', 'Ã¤Â»Â¤Ã¤ÂºÂº', 'Ã¨Â´Â¦', 'Ã§Ä¼Ä¦Ã¤ÂºÂ§Ã¥ÄµÄ£', 'Ã¥Ä°Ä¼', 'Ã¥Ä´Ä®Ã¦Ä¸Ä©Ã¥Ä®Ä¸', 'Ã©Ä»Ä¦', 'Ä mob', 'Ã§Â»Â¼Ã¥Ä²Äª', 'ters', 'Ã¦Ä¾Ä«Ã¤Â¸Ä¢', 'Ã¥Â¦Ä¨', 'Ã¥Ä¯Äª', 'Ä outside', 'Ä propert', 'Ã©Ä¤Â®Ã¤Â»Â¶', 'Ã¤Â¸Â»Ã¤Â¹Ä«', 'Ä policy', 'Ã¨Ä©ÂªÃ¨ÂºÂ«', 'Ä navigate', 'Ä sty', 'Ã§Ä¶ÂµÃ¨Ä¦Ä³', 'Ä abilities', 'Ä faced', 'Ã§Ä¼Ä¦Ã§Â¼', 'Ã§Ä¼Ä¦Ã¥Â°Ä±', 'Ã¨Ä·', 'Ä tone', 'igation', 'Ã¥Ä±Ä¤Ã¦Ä·Â°', 'Ã¨Ä½Ä­Ã§Ä»Â½Ã¨Â´Â¨', 'Ã¤Â½Ä½', 'Ã§Ä¶Ä¼Ã¨Ä©Â³', 'Ä skin', 'Ã¨Ä´Â¸', 'Ã¦Ä­Ä½', 'Ã©ÅƒÄ¶', 'ashion', 'Ä ingred', 'Ã¦Ä¹Ä­', 'Ä campaign', 'Ä mount', 'Ä consid', 'Ä muse', 'nter', 'water', 'Ã¤Â¼Ä¼Ã¨Â®Â®', 'Ä protection', 'Ã¤Â¿Ä¿Ã©Ä»Â©', 'Ä crops', 'ogle', 'Ã©Ä¼Ä±Ã¦Ä¹Â¶', 'Ã¦Ä¼Ä¹', 'ium', 'Ã¤Â¹Ä±', 'Ä diet', 'lies', 'Ã§Ä¶Â¨Ã¦Ä¿Â¥', 'Ä Encoura', 'Ã¦Ä¬Ä¹', 'apan', 'Ã©ÄºÂ²Ã¦ÅƒÂ¢', 'Wow', 'Ã§Ä¼Ä¦Ã¥ÅÂºÃ¦Ä¾Â¬', 'Ã¥Â¹Â³Ã¦Ä¸Â¹', 'Ä step', 'Ã¥Ä±Â¯Ã©Ä¿Å‚', 'Ã¨Â¡Â¨Ã¦ÄºÄ°', 'Ä predictions', 'Ä sympt', 'Ä diagnoses', 'Ã¥Ä§Â¬Ã¥Ä½Åƒ', 'Ä supply', 'Ä previous', 'Ã§Â»Ä¦Ã¥Ä²Äª', '.,', 'Ã§Ä¼Ä¦Ã¨Â¿Ä©Ã§Â¨Ä­', 'Ã¦Ä·Ä±', 'su', 'aris', 'Ã§Ä·Ä§', 'ocol', 'Ã¦Ä²Ä¾Ã§Â´Â¢', 'itle', 'Ã©Ä¨Ä´', 'Ã©Â¡Â¾Ã¥Â®Â¢', 'Ã©Ä¢Â»Ã¨Â¾Ä³', 'Ã©Ä¿Å€Ã¥Â¸Â¸Ã©Ä©Ä¯Ã¨Â¦Ä£', 'Ä Bi', 'Ã¥Â·Â¦Ã¥Ä±Â³', 'amm', 'Ä everything', 'Ã¦ÄºÅ‚', 'Ä incred', 'Ä peace', 'Ã¨Ä¾Ä¾', 'Ä museum', 'Ã§Ä­Â¬Ã§Â«Ä­', 'Ä comprehensive', 'Ä rates', '//', 'Ä rad', 'Ã¥Ä¦Â¿Ã§Â«Â¥', 'Ã§Ä«Â¹Ã¨Ä«Â²', 'Ä Predictive', 'Ã¥Â¼Ä·Ã¥Ä¬Ä½', 'ler', 'Ã¥Â°Â¤', 'icro', 'Ã¨Â¡Â¥', 'Ä determine', 'Ã§Ä¼Ä¦Ã¥Ä¨Ä§Ã¥Â®Â¹', 'Ä compl', 'Ä greenhouse', 'Ã¨Ä§Ä²', 'Ä highlight', 'Ä partners', 'Ä doct', 'Ã§Ä¼Ä¦Ã¤Â½Â¿Ã§Ä¶Â¨', 'Ã¦ÅƒÄ®Ã¦Ä½Â²', 'Ã¦Ä®Ä©Ã¥Ä¯Ä¹', 'Ä Af', 'Ã¦Ä¾ÂºÃ¦Å€Ä¦', 'Ã©Ä¢Ä¢', 'Ä poems', 'Ã¥Â¿Ä¥Ã¥Ä´Ä®', 'Ä attend', 'Ã§Ä¼Ä¦Ã¦Â¸Â¸', 'Ä side', 'ales', 'Ä mentioned', 'Ä Abs', 'Ä historical', 'Ä left', 'Ã¤Â»Â¥Ã¤Â¸Ä­Ã¥Ä©Å‚Ã¤Â¸Âª', 'Ã¥Ä±Ä¹Ã¦Â¬Â¢Ã¨Â¿Ä°', 'Ã¨Ä±Ä¾Ã¥ÄµÄ£', 'Ä remain', 'Ã¦Ä©', 'Ä tours', 'Å‚Ã©Ä£Äµ', 'Ä errors', 'Ã¦Ä¾ÂºÃ¥ÄªÂ¶', 'Ã¦Â¦', 'Ã¦Ä¤Â£Ã¨Ä¢Ä§', 'more', 'Ä experts', 'Ã§Ä¼Ä¦Ã§Å‚Ä¶Ã§Â©Â¶', 'Ã§Â»ÄµÃ¦Ä¿Å', 'Ä written', 'Ã§Å‚Ä¶', 'Ä et', 'input', 'Ã¦Â°Ä¶Ã¤Â½Äµ', 'Ã¨Ä¼', 'Ã¦Ä¥Ä¬', 'Ä age', 'Ã©Ä©Ä¯Ã¥Â¤Ä¯', 'Ã¥Â¼Â¹', 'Ã¥ÅƒÂ¤', 'Ä symptoms', 'Ä belief', \"'d\", 'iol', 'Ä 18', 'Ã¥Ä§Ä§Ã¨Â¶Â³', 'Ã§Ä±Ä¯', 'forcement', 'Ã¦Ä¸Ä¹', 'ÂªÃ¨Ä®Ä¦', 'Ä 15', 'Ã¤Â¸Ä¢Ã¤Â¸ÂªÃ¤ÂºÂº', 'Ä applic', 'Ã¨Â´Â¥', 'Ã¤Â½Ä¯Ã¤ÂºÄ°', 'Ã©Ä»Â¤Ã¤ÂºÄ¨', '=\"', 'Ã¤Â¸Ä«Ã¨Â§Ä´', 'Ã¦Ä¢Ä¿Ã§Â»Â´', 'Ã¥Ä¯Â·', 'Ä fru', 'Ä Collabor', 'Ä prim', 'Ä required', 'Ä watch', 'Ã¨Â°Ä¥Ã¥Ä³Â³', 'Ã§Â»ÄµÃ¨Â®Âº', 'ony', 'Ä guide', 'Ä max', 'Ä Could', 'Ä advent', 'Ä Overall', 'Ã§Ä¼Ä¦Ã¦Ä¬Ä·', 'Ä exper', 'Ã¥Äº', 'icial', 'oster', 'Ã§Ä¼Ä¦Ã©Â¢Ä¾Ã¨Ä«Â²', 'Ä operations', 'Ã©Ä¥Ä£', 'Ä money', 'ley', 'cling', 'Ä oil', 'Ã§Ä¼Â®Ã¨Ä¤Â¤', 'Ä ge', 'Ä bat', 'Ä Ph', 'Ä sche', 'Ä electric', 'vest', 'Ä chain', 'Ä capabilities', 'ird', 'Ã¨Â¯Ä£Ã¦ÄºÄ°', 'Ã¦Ä¾Ä¢Ã¥Â¥Â½', 'ivil', 'Ä depending', 'Ä save', 'Ä practical', 'Ä cultures', 'Ã§Ä½Â¸Ã¥ÂºÄ¶Ã§Ä¼Ä¦', 'sy', 'Ã§Ä¼Ä¦Ã§Â²', 'Ä behind', 'Ã¦Ä¹Â¶Ã©Ä¹Â´Ã¥Ä´Ä®', 'Ã¥Â¹Ä§', 'Ä Ag', 'Ä effectiveness', 'Ad', 'Ä Of', 'Ä anything', 'Ã¥Â·Â§Ã¥Ä§Ä­Ã¥Ä¬Ä½', 'Ä mist', 'Ä languages', 'Ä Make', 'Ã¥Â«', 'Ã¦Â£Â®', 'Ä Cont', 'Ä Absolutely', 'Ä investment', 'mat', 'Ã§Ä¼Ä¦Ã¦Ä·Ä§Ã¤ÂºÄ­', 'Ã¦Â¬Â§', 'Ä speed', 'Ã§Ä¼Ä¦Ã¦Â¸Â©', 'Ä cities', 'Ã¥Ä¨Ä»Ã¤Â½Ä¾', 'Thanks', 'Ä ded', 'Ã¥ÄªÄ¨Ã©Ä§Ä¯', 'Ä dark', 'Ä supporting', 'Ã¥Â¹Ä·', 'Ä Ke', 'Ã©Ä½Â¶', 'Ä sharing', 'Ä house', 'Ã¨Â®Â¤Ã§ÅÂ¥', 'Ä surrounding', 'Ä reduced', 'Ä fu', 'Ä stor', 'Ä abs', 'Tom', 'cent', 'Ä Education', 'Ä thr', 'ott', 'Ä That', 'Ä hear', 'ung', 'Ä beyond', 'Ä Co', 'room', 'Ã¨Â¯Ä¹Ã¦ÅƒÄ®', 'reme', 'Ä little', 'Ä games', 'Ã¤Â¹Ä­Ã¥Ä²Ä°', 'Ã©Ä¥Â½Ã¤Â¼Ä¼', 'Ã¨Â¯ÅƒÃ©ÅÂ³', 'Ã§Â¬Ä³', 'Ã§Ä«Â¹Ã¥Â®Ä¼', 'Ã§Â¬Â¬Ã¤Â¸Ä¢', 'Ä depression', 'Ä innovation', 'Ä Fr', 'Ä computer', 'can', 'Ã¥Â³Â°', 'Ã§Â¼Ä¸Ã¥Ä¨Ä»Ã¤Â¸Ä¢Ã¤Â¸Âª', 'Ä international', 'Ä cancer', 'Ã¥ÅƒÂ¦Ã¨Ä¢Ä§', 'Ä discover', 'het', 'Ä compos', 'Ä recy', 'Ä 200', 'Ã¥Ä²Â«Ã¦Ä¾Ä«', 'Ã§Ä¹Ä½', 'Ã§Â¼ÄµÃ¨Â§Â£', 'Ä frequ', 'Ã§Ä¶Â³', 'Ä Mar', 'Ã§Ä¼Ä¦Ã©Ä¢Ä«Ã¦Ä­Â©', 'Ä unt', 'Ä regions', 'Ä opin', 'Ä Governments', 'Ã¦Â¶Ä¤', 'Ã¥Ä¨Ä§Ã¥Â¿Ä¥', 'Ã¤Â¸Ä¬Ã¦Ä¾Ä¢', 'Ã¤Â»Ä¯Ã§Ä¦Â¶', 'lier', 'Ã¦Â³Â³', 'Ã¤ÂºÄ´Ã§Ä½Â¸', 'Ä Stud', 'azon', 'Ä arch', 'Ä chem', 'Ã§Ä¼Ä¦Ã¨Ä¥Â½Ã¥Ä¬Ä½', 'Ã§Ä¼Ä¦Ã¤Â¸Ä¢Ã¤Â¸Âª', 'Ä ap', 'Ä red', 'Ä women', 'Ä prote', 'Ä finding', 'Ã¥Â§Â»', 'Ã©Ä¢Ä¤Ã¥Â½ÄµÃ§Ä¼Ä¦', 'Ä forward', 'Ã¥Â¯Â¹Ã¨Â±Â¡', 'Ä wait', 'Ä considered', 'dule', 'backs', 'Ä clinical', 'Ã¥Ä§Â·Ã¥Â¤Ä©', 'Ã©ÂºÂ¦', 'Ä ongoing', 'Ã¥Ä¨Ä½', 'Ä far', 'Ã¥Ä´Ä®Ã¨Â°', 'XXX', 'Ä political', 'Ä camer', 'Ã§Ä¼Ä¦Ã¨Â¡Ä®Ã¤Â¸Âº', 'Ã¦Ä¦Ä±Ã¥Â¤Â§Ã¥ÄªÂ©', 'Ä apps', 'Ã¥Ä©Ä±Ã¨Â½Â»', 'Ä readers', 'Ã¥Â©Ä¼Ã¥Â§Â»', 'Ã¦Â°Â¸', 'ores', 'Ã¥Ä§Â¨Ã©Ä¿Â¢', 'Ä Afric', 'Ä favorite', 'Ä mill', 'Ä dang', 'Ä States', 'Ã¥Ä¢Å', 'Ã¥Â¯Â¿', 'Ä lat', 'Ã¨Â¿Ä©Ã¥Ä°Â»', 'Ä truly', 'Ã¥Ä½Å€Ã§ÅƒÄ¶Ã©Ä¹Â®Ã©Â¢Äº', 'Ä cogn', 'Ã¤Â»Â°', 'Ä Japan', 'izz', 'Ã§Ä¼Ä¦Ã¦Ä¿Ä²', 'xx', 'Ã©Â¢ÄºÃ§Ä½Â®', 'ription', 'Ã©Ä¤Â£Ã¤ÂºÄ½', 'Ä budget', 'Ä vast', 'Ã©Ä¼Ä²Ã§Â§Ä£', 'Ä policymakers', 'Ã¨Â¿ÄºÃ©Ä¾Ä¢Ã¨Â¦Ä£', 'Ã¥Â¹Â¶Ã¦Ä±Ä²Ã¤Â¾Ä½', 'Ä sweet', 'Ä general', 'Ã¦Â»Â¤', 'Ä birds', 'Ä plastic', 'ÄŠÄ‰', 'Ã¥ÄªÂº', 'mental', 'Ä inclusive', 'Ä topics', 'Ä slow', 'Ã¤Â½Å‚Ã¨Ä¥Â½', 'Ã¨Â¶Â³Ã¥Â¤ÅÃ§Ä¼Ä¦', 'Ã¨Â§Ä¨Ã¨Â§Ä«', 'ww', 'Ä Ã¤Â½Â¿Ã§Ä¶Â¨', 'Ã¦Ä«Â¹', 'Ã¦Â¦Ä¤Ã¥Â¿Âµ', 'Ã©Â£ÅÃ§Ä¶Â¨', 'Ã¨Ä¢Â³', 'cks', 'Ä fraud', 'Ä ingredients', 'Ä fasc', 'Ã¥Ä®Ä¹Ã¤ÂºÂ¬', 'Ä fr', 'Ä manufacturing', 'Ä Ã¤Â½Ä¾Ã¤Â¸Âº', 'Ä beach', 'Ã©Â¡Â¿', 'erious', 'Ã¥Â¤Ä¸Ã¨Â§Ä¤', 'Ã©Â¢Ä¦Ã©ÄºÂ²', 'Ã¦Ä¿Â¥Ã¨Ä©Âª', 'Ã¨Ä¤Ä®Ã¨Ä¤Ä«', 'Ä days', 'Ä assign', 'Ä advant', 'Ä teams', 'Ã©Â¢Ä¹', 'nown', 'Ä Po', '}{', 'Ä minut', 'itions', 'Ä easily', 'Ä Bl', 'name', 'Ã¥ÅƒÂ¦Ã¦Å‚Â¡', 'Ä responsibility', 'Ã¥Ä±Ä³Ã¦Ä®Â¥', 'Ä sensitive', 'Ã§ÅƒÄ«Ã¤ÂºÄ°', 'cious', 'Ä sou', 'Ã¥Â±Ä±', 'Ä rich', 'Ã¥Â½ÄµÃ§Ä¦Â¶', 'man', 'Ä interpret', '24', 'Ä shows', 'Ã¨Ä£Ä®Ã¥Ä¾Âº', 'Ä fall', 'Ã¨Â½Â½', 'Ã¤Â¸Â°Ã¥Â¯Ä®Ã§Ä¼Ä¦', \"('\", 'Ã¤Â¿Â®Ã¦Ä¶Â¹', 'Ã¦Ä½Â´Ã¦Ä¯Â¢', 'Al', 'Ã¥Ä±Â¯Ã¨Ä¥Â½Ã¦ÄºÂ¯', 'Ä rate', 'Ä protecting', 'fit', 'Ä 50', 'Ä movement', 'Ã¨Â§Äª', 'Ä employee', 'Ä disord', 'Ã¥ÄªÄ½Ã¦Ä¦Ä±', 'Ã¤ÂºÂ§Ã¥ÄµÄ£Ã§Ä¼Ä¦', 'Ã¦Ä¾Ä¿', 'ÄŠÄ Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä Ä ', 'Ä pred', 'Ä offering', 'Ã¥Ä¯Ä£Ã¥ÄªÄ¨', 'Ã¨Ä¢Ä®Ã¤Â¸Ä¯Ã¦ÄºÂ¯', 'Thank', 'Ã¦Ä½Â¾', 'Ä elements', 'Ã§Â²Ä´', 'Ä courses', 'Ä integrated', 'Ä Car', 'agraph', 'Ã¥ÅÂºÃ¥Ä½Å‚', 'Ä instead', 'Ã¨Ä¦Â±', 'Ã¥Ä±Â¦Ã¤Â¸Ä¢Ã¤Â¸Âª', 'Ã¥Â¯Ä¨Ã§Å‚Ä£', 'Ä allowed', 'Ã©Ä¿Â¢Ã¥Ä®Ä§', 'Ã§Ä·ÂªÃ¨Ä®Ä¦', 'Ã¥Ä´Ä®Ã¥Ä±Ä³Ã¥Â±Ä·', 'Ã¥Â°Ä£', 'Ä connection', 'Ã¥Ä¾Â¨Ã¤Â¸Ä¢Ã¤Â¸Âª', 'Ä useful', 'Ã¨Â¯ÅƒÃ¥Ä±Â¥', 'Ã¥ÄªÄ¨Ã¥Â¸Ä¥', 'Ã¨Â¡Â¨Ã¦Â¼Ä¶', 'Ã¦Ä¾Ä«Ã¦Ä¹Â¶', 'Ã§Ä¼Ä¦Ã¦Ä¹Ä§', 'Ã§Ä¼Ä¦Ã¦Ä¢Â»', 'Ä fashion', 'Ã¨Ä­Â¦', 'Ã¨Â¦Ä£Ã¦Â³Â¨Ã¦Ä¦Ä±', 'Ã§Ä¶ÅÃ§Â´Å‚', 'Ä nutri', 'Ã¨Ä©ÂªÃ¨Â¡Ä®', 'Ã§Ä¼Ä¦Ã§Ä­', 'Ã§Ä²Ä¨Ã¨Â§Â£Ã¥Ä´Ä®', 'Ä cat', 'Ã¦Ä¾ÂºÃ¥Ä»Â¨Ã¥ÅƒÂ¦Ã¤Â¹Å‚', 'Ä exhib', 'Ã¥Ä´Ä®Ã¦Ä¾Ä¯Ã¥Ä¬Â¡', 'frac', 'epend', 'Ä impacted', 'Ä ut', 'Ã¦Ä·Â°Ã§Â»Ä¦', 'Ä World', 'Ä answer', 'erse', 'Ã©ÂªÂ¨', 'Ä artists', 'Ã¥ÅƒÂ©Ã¥ÅƒÄ²Ã§Ä¼Ä¦', 'Ã¤Â»Ä¶', 'Ã§Ä»Â»', 'Ä Are', 'Ä cool', 'Ä cognitive', 'Ã¥Ä²Ä¦Ã¤Â¸Âª', 'like', 'Ã¥Â©Â´Ã¥Ä¦Â¿', 'Ã¥ÄªÄ¹Ã¥Ä©Âº', 'Ã¥Â¹Â»', 'ront', 'Ã¥Â®Â¶Ã©Ä·Â¿', 'Ã§Â¼ÂºÃ¤Â¹Ä±', 'Ä cyber', 'ilt', 'Ä capture', 'Ã¥Ä¹', 'Ã¥Ä¾Â¨Ã¤ÂºÄ°', 'Ä threats', 'Ã¥Ä´Ä®Ã§Â¤Â¾Ã¤Â¼Ä¼', 'Ä cells', 'Ã¦Â¸Ä§Ã¥Ä¯Ä·', 'Ä Vis', 'Ã¦Ä°Ä«', 'Ä hol', 'Ã¥ÅƒÄ²Ã§Ä¼Ä¦', 'Ch', 'Ã¨Ä¿', 'Ä said', 'Ä dream', 'unch', 'une', 'Ä Don', 'Ã¥Â®Â¶Ã¤ÂºÂº', 'Ã§Â±Ä¯', 'Ã¦Ä¦ÅÃ¥Ä´Ä®', 'Ä experienced', 'Ã§Ä¼Ä¦Ã©Ä©Ä¯Ã¨Â¦Ä£Ã¦Ä¢Â§', 'Ã¥Â¼Ä¥', 'ump', 'Ã©ÄºÄ²', 'Ä habitat', 'Ã¨Â¢Ä­', 'Ä jo', 'Ã§Â®Ä¢Ã¦Â´Ä£', 'Ä bur', 'Ä visitors', 'Ã©Ä½Ä§', 'Ã§Ä¼Ä¦Ã§ÅÂ¥', 'Ä entire', 'Ã¨Â®Â²Ã¨Â¿Â°', 'Ã¤ÂºÄ¨Ã¤Â¸Ä¢Ã¤ÂºÄ½', 'Ã¥Ä¯Ä±Ã¤Â½Ä¾', 'Ä Bus', 'Ã¥Â°Â¾', 'Ã§Ä¼Ä¦Ã¦Ä·Ä»', 'olog', 'Ä signs', 'Ä speaker', 'Ã§Ä¼Ä¦Ã©ÅÂ³Ã¤Â¹Ä²', 'Ä novel', 'Ã¥Â±Ä§Ã¦Â°Ä³', 'Ã§Ä¼Ä¦Ã¥Ä±ÄºÃ¥Ä®Ä¸', 'Ã¥Â°Â½Ã©Ä©Ä±', 'Ä spirit', 'Ã¥Â®Ä®Ã§Â¾Ä°', 'Ã¨Â´Â·', 'Ã¥Â¿Ä§Ã¨Â¦Ä£Ã§Ä¼Ä¦', 'ief', 'Ã§Â¤ÂºÃ¤Â¾Ä­', 'Ä div', 'Ã¦Ä·Â´Ã¦Ä·Â°', 'Ä economy', 'Ä ethically', 'Ã©Ä»Äª', 'Ä schools', 'Ä networks']\n"
     ]
    }
   ],
   "source": [
    "vocab=tokenizer.get_vocab()\n",
    "print(sorted(vocab,key=lambda x:vocab[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0977649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniMindæ¨¡å‹å‚æ•°: 25.83 M(illion)\n",
      "trainable params: 10,240 || all params: 25,840,128 || trainable%: 0.0396\n"
     ]
    }
   ],
   "source": [
    "from peft import PromptEmbedding, PromptTuningConfig,get_peft_model\n",
    "import torch.nn as nn\n",
    "model, tokenizer = init_model(args)\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    num_virtual_tokens=20,\n",
    "    prompt_tuning_init=\"TEXT\",\n",
    "    num_transformer_submodules=1,\n",
    "    token_dim=512,\n",
    "    prompt_tuning_init_text=\"Classify the passage:\",\n",
    "    tokenizer_name_or_path='./model'\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # åº”æ˜¾ç¤º ~15K å¯è®­ç»ƒå‚æ•°\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cf7e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.7488, -0.0540,  0.1875,  ..., -4.4258,  0.8613, -0.9082],\n",
      "         [-0.4753,  0.7772,  0.0536,  ..., -0.0755,  1.1915, -0.8722],\n",
      "         [ 0.3381,  2.9204,  1.4160,  ..., -1.0023,  1.0548, -0.8825],\n",
      "         ...,\n",
      "         [ 0.8371,  1.6740, -1.3006,  ..., -0.7171, -3.4437, -0.4352],\n",
      "         [-0.3711,  1.4281,  0.0099,  ..., -0.6652, -2.6028, -0.7404],\n",
      "         [-0.6661,  0.9061, -0.4236,  ..., -2.7374, -0.9953, -0.8959]]],\n",
      "       device='cuda:0'), [None, None, None, None, None, None, None, None], 0)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m inputs = tokenizer(input_prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(model.model(inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m prediction = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(prediction)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\peft\\peft_model.py:2050\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2048\u001b[39m             outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(*args, **kwargs)\n\u001b[32m   2049\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2050\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\transformers\\generation\\utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Git\\minimind\\model\\model_minimind.py:450\u001b[39m, in \u001b[36mMiniMindForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, past_key_values, use_cache, logits_to_keep, **args)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    444\u001b[39m             input_ids: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    445\u001b[39m             attention_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    448\u001b[39m             logits_to_keep: Union[\u001b[38;5;28mint\u001b[39m, torch.Tensor] = \u001b[32m0\u001b[39m,\n\u001b[32m    449\u001b[39m             **args):\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m--------------------------------\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    452\u001b[39m     hidden_states, past_key_values, aux_loss = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    453\u001b[39m         input_ids=input_ids,\n\u001b[32m    454\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m         **args\n\u001b[32m    458\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "input_prompt = [\"Classify the sentiment of the following review as positive or negative.\\nReview: I love this film!\\nSentiment:\"]\n",
    "inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(device)\n",
    "print(model.model(inputs['input_ids']))\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_new_tokens=5,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b86dc140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token '<|im_start|>' çš„ ID: 1\n",
      "å½“å‰embeddingå½¢çŠ¶: torch.Size([512])\n",
      "å½“å‰embeddingå‰5ä¸ªå€¼: tensor([ 0.0036, -0.0128, -0.0255,  0.0096, -0.0007], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "ä¿®æ”¹åembeddingå‰5ä¸ªå€¼: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1. è·å–è¦ä¿®æ”¹çš„tokençš„id\n",
    "token_str = \"<|im_start|>\"  # æˆ–è€…ä»»ä½•ä½ æƒ³ä¿®æ”¹çš„token\n",
    "token_id = tokenizer.convert_tokens_to_ids(token_str)\n",
    "print(f\"Token '{token_str}' çš„ ID: {token_id}\")\n",
    "\n",
    "# 2. è®¿é—®embeddingå±‚\n",
    "# æ³¨æ„ï¼šMiniMindä¸­embed_tokenså’Œlm_headå…±äº«æƒé‡\n",
    "embedding_layer = model.model.embed_tokens  # æˆ–è€… model.lm_head (å®ƒä»¬å…±äº«æƒé‡)\n",
    "\n",
    "# 3. æŸ¥çœ‹å½“å‰embedding\n",
    "current_embedding = embedding_layer.weight[token_id]\n",
    "print(f\"å½“å‰embeddingå½¢çŠ¶: {current_embedding.shape}\")\n",
    "print(f\"å½“å‰embeddingå‰5ä¸ªå€¼: {current_embedding[:5]}\")\n",
    "\n",
    "# # 4. ä¿®æ”¹embeddingï¼ˆéœ€è¦å…ˆè®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼æˆ–ä½¿ç”¨requires_grad_ï¼‰\n",
    "model.model.embed_tokens.weight.requires_grad_(True)\n",
    "\n",
    "# # æ–¹æ³•A: ç›´æ¥èµ‹å€¼æ–°çš„embeddingå‘é‡\n",
    "# new_embedding = torch.randn(model.config.hidden_size)  # éšæœºåˆå§‹åŒ–\n",
    "# # æˆ–è€…ä»å…¶ä»–tokenå¤åˆ¶\n",
    "# # new_embedding = embedding_layer.weight[other_token_id].clone()\n",
    "\n",
    "# embedding_layer.weight.data[token_id] = new_embedding\n",
    "\n",
    "# # æ–¹æ³•B: åœ¨ç°æœ‰embeddingåŸºç¡€ä¸Šè°ƒæ•´\n",
    "# # embedding_layer.weight.data[token_id] += 0.1 * torch.randn(model.config.hidden_size)\n",
    "\n",
    "# # æ–¹æ³•C: è®¾ç½®ä¸ºç‰¹å®šå€¼\n",
    "embedding_layer.weight.data[token_id] = torch.zeros(model.config.hidden_size)\n",
    "\n",
    "# # 5. éªŒè¯ä¿®æ”¹\n",
    "modified_embedding = embedding_layer.weight[token_id]\n",
    "print(f\"ä¿®æ”¹åembeddingå‰5ä¸ªå€¼: {modified_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a610db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68e94ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… '<cls>' ä¸åœ¨vocabä¸­ï¼Œå¯ä»¥æ·»åŠ \n",
      "\n",
      "æˆåŠŸæ·»åŠ  1 ä¸ªæ–°è¯åˆ°tokenizer\n",
      "\n",
      "æ–°æ·»åŠ çš„è¯åŠå…¶ID:\n",
      "  '<cls>' -> ID: 6400\n",
      "\n",
      "âš ï¸  æ³¨æ„ï¼šæ¨¡å‹åµŒå…¥å±‚éœ€è¦è°ƒæ•´å¤§å°ä»¥æ”¯æŒæ–°è¯\n",
      "å½“å‰vocabå¤§å°: 6401\n",
      "æ¨¡å‹embeddingå±‚å¤§å°: 6400\n",
      "æ˜¯å¦ç›¸åŒï¼š True\n",
      "è°ƒæ•´åæ¨¡å‹embeddingå±‚å¤§å°: 6401\n",
      "âœ… å·²åˆå§‹åŒ– 1 ä¸ªæ–°tokençš„embedding\n",
      "\n",
      "éªŒè¯æ–°è¯çš„ä½¿ç”¨:\n",
      "  åŸæ–‡: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•: <cls> åº”è¯¥è¢«æ­£ç¡®è¯†åˆ«\n",
      "  ç¼–ç å: [434, 1589, 3560, 28, 223, 6400, 223, 1406, 1183, 2826, 5159]\n",
      "  è§£ç å: è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•: <cls> åº”è¯¥è¢«æ­£ç¡®è¯†åˆ«\n"
     ]
    }
   ],
   "source": [
    "# ç»™tokenizerçš„vocabæ·»åŠ æ–°è¯\n",
    "# æ–¹æ³•ï¼šä½¿ç”¨ tokenizer.add_tokens() æ·»åŠ æ–°è¯\n",
    "\n",
    "# 1. å®šä¹‰è¦æ·»åŠ çš„æ–°è¯\n",
    "new_tokens = [\"<cls>\"]\n",
    "\n",
    "# 2. æ£€æŸ¥è¿™äº›è¯æ˜¯å¦å·²ç»å­˜åœ¨äºvocabä¸­\n",
    "existing_tokens = []\n",
    "for token in new_tokens:\n",
    "    if token in tokenizer.get_vocab():\n",
    "        existing_tokens.append(token)\n",
    "        print(f\"âš ï¸  '{token}' å·²ç»å­˜åœ¨äºvocabä¸­ï¼ŒID: {tokenizer.convert_tokens_to_ids(token)}\")\n",
    "    else:\n",
    "        print(f\"âœ… '{token}' ä¸åœ¨vocabä¸­ï¼Œå¯ä»¥æ·»åŠ \")\n",
    "\n",
    "# 3. è¿‡æ»¤æ‰å·²å­˜åœ¨çš„è¯\n",
    "tokens_to_add = [token for token in new_tokens if token not in existing_tokens]\n",
    "\n",
    "if tokens_to_add:\n",
    "    # 4. æ·»åŠ æ–°è¯åˆ°tokenizer\n",
    "    num_added = tokenizer.add_tokens(tokens_to_add)\n",
    "    print(f\"\\næˆåŠŸæ·»åŠ  {num_added} ä¸ªæ–°è¯åˆ°tokenizer\")\n",
    "    \n",
    "    # 5. æŸ¥çœ‹æ–°æ·»åŠ çš„è¯åŠå…¶ID\n",
    "    print(\"\\næ–°æ·»åŠ çš„è¯åŠå…¶ID:\")\n",
    "    for token in tokens_to_add:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        print(f\"  '{token}' -> ID: {token_id}\")\n",
    "    \n",
    "    # 6. å¦‚æœæ¨¡å‹éœ€è¦æ”¯æŒè¿™äº›æ–°è¯ï¼Œéœ€è¦è°ƒæ•´æ¨¡å‹åµŒå…¥å±‚å¤§å°\n",
    "    # æ³¨æ„ï¼šè¿™éœ€è¦é‡æ–°åˆå§‹åŒ–æ–°tokençš„embeddingæƒé‡\n",
    "    if num_added > 0:\n",
    "        print(f\"\\nâš ï¸  æ³¨æ„ï¼šæ¨¡å‹åµŒå…¥å±‚éœ€è¦è°ƒæ•´å¤§å°ä»¥æ”¯æŒæ–°è¯\")\n",
    "        print(f\"å½“å‰vocabå¤§å°: {len(tokenizer)}\")\n",
    "        print(f\"æ¨¡å‹embeddingå±‚å¤§å°: {model.get_input_embeddings().weight.shape[0]}\")\n",
    "        # åœ¨ resize ä¹‹å‰ä¿å­˜åŸæœ‰ embedding\n",
    "        old_embedding = model.get_input_embeddings().weight.data[:6400].clone()\n",
    "\n",
    "        # æ‰§è¡Œ resize\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # æ¯”è¾ƒ resize åçš„å‰ 6400 ä¸ª embedding\n",
    "        new_embedding = model.get_input_embeddings().weight.data[:6400]\n",
    "\n",
    "        # æ£€æŸ¥æ˜¯å¦ç›¸åŒ\n",
    "        print('æ˜¯å¦ç›¸åŒï¼š',torch.allclose(old_embedding, new_embedding))  # åº”è¯¥è¿”å› True\n",
    "        # è°ƒæ•´æ¨¡å‹åµŒå…¥å±‚å¤§å°\n",
    "        # model.resize_token_embeddings(len(tokenizer))\n",
    "        print(f\"è°ƒæ•´åæ¨¡å‹embeddingå±‚å¤§å°: {model.get_input_embeddings().weight.shape[0]}\")\n",
    "        \n",
    "        # 7. å¯é€‰ï¼šåˆå§‹åŒ–æ–°tokençš„embeddingï¼ˆå¯ä»¥éšæœºåˆå§‹åŒ–æˆ–ä»å…¶ä»–tokenå¤åˆ¶ï¼‰\n",
    "        # è·å–æ–°æ·»åŠ çš„token IDs\n",
    "        new_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens_to_add]\n",
    "        \n",
    "        # æ–¹æ³•A: éšæœºåˆå§‹åŒ–ï¼ˆä½¿ç”¨æ­£æ€åˆ†å¸ƒï¼‰\n",
    "        embedding_dim = model.get_input_embeddings().weight.shape[1]\n",
    "        for token_id in new_token_ids:\n",
    "            # ä»æ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºåˆå§‹åŒ–\n",
    "            new_embedding = torch.randn(embedding_dim) * 0.02  # å°æ–¹å·®åˆå§‹åŒ–\n",
    "            model.get_input_embeddings().weight.data[token_id] = new_embedding\n",
    "            # å¦‚æœlm_headå’Œembed_tokenså…±äº«æƒé‡ï¼Œåªéœ€è¦ä¿®æ”¹ä¸€å¤„\n",
    "            if hasattr(model, 'lm_head') and model.lm_head.weight is model.get_input_embeddings().weight:\n",
    "                pass  # å·²ç»å…±äº«ï¼Œæ— éœ€é¢å¤–æ“ä½œ\n",
    "        \n",
    "        print(f\"âœ… å·²åˆå§‹åŒ– {len(new_token_ids)} ä¸ªæ–°tokençš„embedding\")\n",
    "        \n",
    "        # 8. éªŒè¯æ–°è¯å¯ä»¥æ­£å¸¸ä½¿ç”¨\n",
    "        print(\"\\néªŒè¯æ–°è¯çš„ä½¿ç”¨:\")\n",
    "        test_text = f\"è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•: {tokens_to_add[0]} åº”è¯¥è¢«æ­£ç¡®è¯†åˆ«\"\n",
    "        encoded = tokenizer.encode(test_text, add_special_tokens=False)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        print(f\"  åŸæ–‡: {test_text}\")\n",
    "        print(f\"  ç¼–ç å: {encoded}\")\n",
    "        print(f\"  è§£ç å: {decoded}\")\n",
    "        \n",
    "        # 9. å¯é€‰ï¼šä¿å­˜æ›´æ–°åçš„tokenizer\n",
    "        # tokenizer.save_pretrained('./model_updated/')  # å–æ¶ˆæ³¨é‡Šä»¥ä¿å­˜\n",
    "        # print(\"\\nâœ… Tokenizerå·²ä¿å­˜åˆ° './model_updated/'\")\n",
    "else:\n",
    "    print(\"\\næ²¡æœ‰éœ€è¦æ·»åŠ çš„æ–°è¯\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83221d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "ä¿å­˜tokenizer...\n",
      "================================================================================\n",
      "âœ… Tokenizerå·²ä¿å­˜åˆ°: ./test/\n",
      "  âœ“ tokenizer_config.json\n",
      "  âœ— vocab.json (å¯èƒ½ä¸å­˜åœ¨ï¼Œå–å†³äºtokenizerç±»å‹)\n",
      "  âœ— merges.txt (å¯èƒ½ä¸å­˜åœ¨ï¼Œå–å†³äºtokenizerç±»å‹)\n",
      "  âœ“ special_tokens_map.json\n",
      "\n",
      "================================================================================\n",
      "ä¿å­˜æ¨¡å‹æƒé‡...\n",
      "================================================================================\n",
      "âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: ./test\\full_sft_cls_512.pth\n",
      "   æƒé‡åç§°: full_sft_cls\n",
      "   éšè—å±‚å¤§å°: 512\n",
      "   MoEåç¼€: \n",
      "   å‚æ•°é‡: 29.11M\n",
      "âœ… é…ç½®ä¿¡æ¯å·²ä¿å­˜åˆ°: ./test\\model_config_info.json\n",
      "\n",
      "================================================================================\n",
      "ä¿å­˜éªŒè¯:\n",
      "================================================================================\n",
      "âœ… Tokenizerå¯ä»¥æ­£å¸¸åŠ è½½ (vocabå¤§å°: 6401)\n",
      "âœ… æ–°æ·»åŠ çš„tokenä»ç„¶å­˜åœ¨äºvocabä¸­\n",
      "âœ… æ¨¡å‹æƒé‡æ–‡ä»¶å¯ä»¥æ­£å¸¸åŠ è½½ (åŒ…å« 75 ä¸ªå‚æ•°é”®)\n",
      "\n",
      "================================================================================\n",
      "ä¿å­˜æ¨¡å‹é…ç½®...\n",
      "================================================================================\n",
      "âœ… æ¨¡å‹é…ç½®å·²ä¿å­˜åˆ°: ./test\\config.json\n",
      "âœ… æ¨¡å‹å®šä¹‰æ–‡ä»¶å·²åˆ›å»º: ./test\\modeling_minimind.py\n",
      "âœ… config.jsonéªŒè¯é€šè¿‡\n",
      "   æ¨¡å‹ç±»å‹: minimind\n",
      "   vocabå¤§å°: 6401\n",
      "   éšè—å±‚å¤§å°: 512\n",
      "\n",
      "================================================================================\n",
      "ä¿å­˜å®Œæˆï¼\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ é‡æ–°åŠ è½½æ—¶ä½¿ç”¨ä»¥ä¸‹å‚æ•°:\n",
      "    args.load_from = './test'\n",
      "    args.save_dir = './test'\n",
      "    args.weight = 'full_sft_cls'\n",
      "    args.hidden_size = 512\n",
      "    args.use_moe = 0\n",
      "\n",
      "âœ… ç°åœ¨å¯ä»¥ä½¿ç”¨ AutoModelForCausalLM.from_pretrained('./test', trust_remote_code=True) åŠ è½½æ¨¡å‹\n"
     ]
    }
   ],
   "source": [
    "# åœ¨Cell 14çš„æœ€åï¼Œæ·»åŠ ä¿å­˜ä»£ç ï¼ˆåœ¨tokenizerå’Œmodeléƒ½å·²æ›´æ–°ä¹‹åï¼‰\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# ========== ä¿å­˜æ›´æ–°åçš„tokenizerå’Œæ¨¡å‹ ==========\n",
    "\n",
    "# 1. è®¾ç½®ä¿å­˜è·¯å¾„\n",
    "save_base_dir = \"./test\"  # tokenizerä¿å­˜ç›®å½•\n",
    "save_model_dir = \"./test\"  # æ¨¡å‹æƒé‡ä¿å­˜ç›®å½•ï¼ˆä¸init_modelçš„save_dirä¿æŒä¸€è‡´ï¼‰\n",
    "os.makedirs(save_base_dir, exist_ok=True)\n",
    "os.makedirs(save_model_dir, exist_ok=True)\n",
    "\n",
    "# 2. ä¿å­˜tokenizeråˆ°ç›®å½•\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ä¿å­˜tokenizer...\")\n",
    "print(\"=\" * 80)\n",
    "tokenizer.save_pretrained(save_base_dir)\n",
    "print(f\"âœ… Tokenizerå·²ä¿å­˜åˆ°: {save_base_dir}/\")\n",
    "\n",
    "# éªŒè¯tokenizeræ–‡ä»¶\n",
    "tokenizer_files = ['tokenizer_config.json', 'vocab.json', 'merges.txt', 'special_tokens_map.json']\n",
    "for file in tokenizer_files:\n",
    "    file_path = os.path.join(save_base_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"  âœ“ {file}\")\n",
    "    else:\n",
    "        print(f\"  âœ— {file} (å¯èƒ½ä¸å­˜åœ¨ï¼Œå–å†³äºtokenizerç±»å‹)\")\n",
    "\n",
    "# 3. ä¿å­˜æ¨¡å‹æƒé‡åˆ°.pthæ–‡ä»¶\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ä¿å­˜æ¨¡å‹æƒé‡...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# è·å–æ¨¡å‹é…ç½®å‚æ•°ï¼ˆç”¨äºæ„å»ºæ–‡ä»¶åï¼‰\n",
    "# è¿™äº›å‚æ•°éœ€è¦ä¸init_modelæ—¶ä½¿ç”¨çš„å‚æ•°ä¸€è‡´\n",
    "model_weight_name = \"full_sft_cls\"  # æƒé‡æ–‡ä»¶åï¼ˆå¦‚ 'pretrain', 'full_sft' ç­‰ï¼‰\n",
    "hidden_size = model.config.hidden_size if hasattr(model, 'config') else 512  # ä»æ¨¡å‹é…ç½®è·å–\n",
    "use_moe = getattr(model.config, 'use_moe', False) if hasattr(model, 'config') else False\n",
    "\n",
    "# æ„å»ºæƒé‡æ–‡ä»¶åï¼ˆæ ¼å¼ä¸init_modelæœŸæœ›çš„ä¸€è‡´ï¼‰\n",
    "moe_suffix = '_moe' if use_moe else ''\n",
    "weight_filename = f'{model_weight_name}_{hidden_size}{moe_suffix}.pth'\n",
    "weight_path = os.path.join(save_model_dir, weight_filename)\n",
    "\n",
    "# è·å–æ¨¡å‹çŠ¶æ€å­—å…¸\n",
    "if hasattr(model, 'module'):  # å¦‚æœæ¨¡å‹è¢«åŒ…è£…åœ¨DataParallelä¸­\n",
    "    state_dict = model.module.state_dict()\n",
    "else:\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "# è½¬æ¢ä¸ºåŠç²¾åº¦ä»¥èŠ‚çœç©ºé—´ï¼ˆä¸è®­ç»ƒè„šæœ¬ä¿æŒä¸€è‡´ï¼‰\n",
    "state_dict_to_save = {k: v.half() if v.dtype == torch.float32 else v \n",
    "                      for k, v in state_dict.items()}\n",
    "\n",
    "# ä¿å­˜æƒé‡\n",
    "torch.save(state_dict_to_save, weight_path)\n",
    "print(f\"âœ… æ¨¡å‹æƒé‡å·²ä¿å­˜åˆ°: {weight_path}\")\n",
    "print(f\"   æƒé‡åç§°: {model_weight_name}\")\n",
    "print(f\"   éšè—å±‚å¤§å°: {hidden_size}\")\n",
    "print(f\"   MoEåç¼€: {moe_suffix}\")\n",
    "print(f\"   å‚æ•°é‡: {sum(p.numel() for p in state_dict.values()) / 1e6:.2f}M\")\n",
    "\n",
    "# 4. ä¿å­˜æ¨¡å‹é…ç½®ä¿¡æ¯ï¼ˆå¯é€‰ï¼Œç”¨äºè®°å½•ï¼‰\n",
    "config_info = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'hidden_size': hidden_size,\n",
    "    'use_moe': use_moe,\n",
    "    'new_tokens': tokens_to_add if 'tokens_to_add' in locals() else [],\n",
    "    'tokenizer_path': save_base_dir,\n",
    "    'model_weight_path': weight_path,\n",
    "    'model_weight_name': model_weight_name\n",
    "}\n",
    "\n",
    "import json\n",
    "config_path = os.path.join(save_base_dir, 'model_config_info.json')\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_info, f, indent=2, ensure_ascii=False)\n",
    "print(f\"âœ… é…ç½®ä¿¡æ¯å·²ä¿å­˜åˆ°: {config_path}\")\n",
    "\n",
    "# 5. éªŒè¯ä¿å­˜ç»“æœ\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ä¿å­˜éªŒè¯:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# éªŒè¯tokenizerå¯ä»¥é‡æ–°åŠ è½½\n",
    "try:\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(save_base_dir)\n",
    "    print(f\"âœ… Tokenizerå¯ä»¥æ­£å¸¸åŠ è½½ (vocabå¤§å°: {len(test_tokenizer)})\")\n",
    "    # éªŒè¯æ–°æ·»åŠ çš„tokenæ˜¯å¦è¿˜åœ¨\n",
    "    if tokens_to_add and all(token in test_tokenizer.get_vocab() for token in tokens_to_add):\n",
    "        print(f\"âœ… æ–°æ·»åŠ çš„tokenä»ç„¶å­˜åœ¨äºvocabä¸­\")\n",
    "    del test_tokenizer\n",
    "except Exception as e:\n",
    "    print(f\"âŒ TokenizeråŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "# éªŒè¯æ¨¡å‹æƒé‡å¯ä»¥åŠ è½½\n",
    "try:\n",
    "    test_weights = torch.load(weight_path, map_location='cpu')\n",
    "    print(f\"âœ… æ¨¡å‹æƒé‡æ–‡ä»¶å¯ä»¥æ­£å¸¸åŠ è½½ (åŒ…å« {len(test_weights)} ä¸ªå‚æ•°é”®)\")\n",
    "    del test_weights\n",
    "except Exception as e:\n",
    "    print(f\"âŒ æ¨¡å‹æƒé‡åŠ è½½å¤±è´¥: {e}\")\n",
    "\n",
    "# 6. ä¿å­˜æ¨¡å‹é…ç½®åˆ°config.jsonï¼ˆä½¿å¾—AutoModelForCausalLM.from_pretrained()å¯ä»¥åŠ è½½ï¼‰\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ä¿å­˜æ¨¡å‹é…ç½®...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# æ³¨å†ŒMiniMindConfigå’ŒMiniMindForCausalLMåˆ°AutoClass\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "MiniMindConfig.register_for_auto_class()\n",
    "MiniMindForCausalLM.register_for_auto_class(\"AutoModelForCausalLM\")\n",
    "\n",
    "# è·å–æ¨¡å‹é…ç½®\n",
    "if hasattr(model, 'config'):\n",
    "    model_config = model.config\n",
    "else:\n",
    "    # å¦‚æœæ²¡æœ‰configï¼Œåˆ›å»ºä¸€ä¸ªæ–°çš„MiniMindConfig\n",
    "    model_config = MiniMindConfig(\n",
    "        vocab_size=len(tokenizer),\n",
    "        hidden_size=hidden_size,\n",
    "        num_hidden_layers=getattr(model, 'num_hidden_layers', 12),\n",
    "        use_moe=use_moe\n",
    "    )\n",
    "\n",
    "# æ›´æ–°vocab_sizeä»¥åŒ¹é…å®é™…çš„tokenizerå¤§å°\n",
    "model_config.vocab_size = len(tokenizer)\n",
    "\n",
    "# ä¿å­˜é…ç½®åˆ°config.json\n",
    "config_path = os.path.join(save_base_dir, 'config.json')\n",
    "model_config.save_pretrained(save_base_dir)\n",
    "print(f\"âœ… æ¨¡å‹é…ç½®å·²ä¿å­˜åˆ°: {config_path}\")\n",
    "\n",
    "# åˆ›å»ºmodeling_minimind.pyæ–‡ä»¶ï¼Œç”¨äºAutoModelForCausalLM.from_pretrained()åŠ è½½\n",
    "modeling_file_path = os.path.join(save_base_dir, 'modeling_minimind.py')\n",
    "modeling_code = '''# This file is auto-generated for loading MiniMind model with AutoModelForCausalLM\n",
    "# The actual model implementation is in the project's model/model_minimind.py\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to path so we can import the actual model\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import and re-export the model classes\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "\n",
    "# Re-export for transformers\n",
    "__all__ = ['MiniMindConfig', 'MiniMindForCausalLM']\n",
    "'''\n",
    "\n",
    "with open(modeling_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(modeling_code)\n",
    "print(f\"âœ… æ¨¡å‹å®šä¹‰æ–‡ä»¶å·²åˆ›å»º: {modeling_file_path}\")\n",
    "\n",
    "# éªŒè¯config.jsonæ˜¯å¦å­˜åœ¨ä¸”æ ¼å¼æ­£ç¡®\n",
    "try:\n",
    "    import json\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config_dict = json.load(f)\n",
    "    print(f\"âœ… config.jsonéªŒè¯é€šè¿‡\")\n",
    "    print(f\"   æ¨¡å‹ç±»å‹: {config_dict.get('model_type', 'N/A')}\")\n",
    "    print(f\"   vocabå¤§å°: {config_dict.get('vocab_size', 'N/A')}\")\n",
    "    print(f\"   éšè—å±‚å¤§å°: {config_dict.get('hidden_size', 'N/A')}\")\n",
    "    \n",
    "    # ç¡®ä¿model_typeå­—æ®µå­˜åœ¨\n",
    "    if 'model_type' not in config_dict:\n",
    "        print(f\"âš ï¸  config.jsonç¼ºå°‘model_typeå­—æ®µï¼Œæ­£åœ¨ä¿®å¤...\")\n",
    "        config_dict['model_type'] = 'minimind'\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_dict, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"âœ… å·²ä¿®å¤config.json\")\n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸  config.jsonéªŒè¯å¤±è´¥: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"ä¿å­˜å®Œæˆï¼\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nğŸ“Œ é‡æ–°åŠ è½½æ—¶ä½¿ç”¨ä»¥ä¸‹å‚æ•°:\")\n",
    "print(f\"    args.load_from = '{save_base_dir}'\")\n",
    "print(f\"    args.save_dir = '{save_model_dir}'\")\n",
    "print(f\"    args.weight = '{model_weight_name}'\")\n",
    "print(f\"    args.hidden_size = {hidden_size}\")\n",
    "print(f\"    args.use_moe = {1 if use_moe else 0}\")\n",
    "print(f\"\\nâœ… ç°åœ¨å¯ä»¥ä½¿ç”¨ AutoModelForCausalLM.from_pretrained('{save_base_dir}', trust_remote_code=True) åŠ è½½æ¨¡å‹\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6debd8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.test.model_minimind.MiniMindConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m args = Args()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# é‡æ–°åŠ è½½tokenizerå’Œæ¨¡å‹\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m model_reloaded, tokenizer_reloaded = \u001b[43minit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mé‡æ–°åŠ è½½å®Œæˆï¼\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36minit_model\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     31\u001b[39m         load_lora(model, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.save_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lora/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.lora_weight\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.hidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMiniMindæ¨¡å‹å‚æ•°: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters())\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m M(illion)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model.eval().to(device), tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:607\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    605\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    606\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized configuration class <class 'transformers_modules.test.model_minimind.MiniMindConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "source": [
    "# ========== é‡æ–°åŠ è½½ä¿å­˜çš„tokenizerå’Œæ¨¡å‹ ==========\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "import argparse\n",
    "\n",
    "# è®¾ç½®åŠ è½½å‚æ•°ï¼ˆä¸ä¿å­˜æ—¶ä¿æŒä¸€è‡´ï¼‰\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.load_from = \"./test\"  # tokenizerç›®å½•\n",
    "        self.save_dir = \"./test\"  # æ¨¡å‹æƒé‡ç›®å½•\n",
    "        self.weight = \"full_sft_cls\"  # æƒé‡æ–‡ä»¶å\n",
    "        self.hidden_size = 512  # æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´\n",
    "        self.num_hidden_layers = 12  # æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´\n",
    "        self.use_moe = 0  # æ ¹æ®å®é™…æ¨¡å‹è°ƒæ•´\n",
    "        self.lora_weight = \"None\"\n",
    "        self.inference_rope_scaling = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# ä½¿ç”¨notebookä¸­çš„init_modelå‡½æ•°é‡æ–°åŠ è½½\n",
    "args = Args()\n",
    "\n",
    "# é‡æ–°åŠ è½½tokenizerå’Œæ¨¡å‹\n",
    "model_reloaded, tokenizer_reloaded = init_model(args)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"é‡æ–°åŠ è½½å®Œæˆï¼\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Tokenizer vocabå¤§å°: {len(tokenizer_reloaded)}\")\n",
    "print(f\"æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model_reloaded.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# éªŒè¯æ–°æ·»åŠ çš„tokenæ˜¯å¦è¿˜åœ¨\n",
    "if 'tokens_to_add' in locals() and tokens_to_add:\n",
    "    print(f\"\\néªŒè¯æ–°æ·»åŠ çš„token:\")\n",
    "    for token in tokens_to_add:\n",
    "        if token in tokenizer_reloaded.get_vocab():\n",
    "            token_id = tokenizer_reloaded.convert_tokens_to_ids(token)\n",
    "            print(f\"  âœ… '{token}' -> ID: {token_id}\")\n",
    "        else:\n",
    "            print(f\"  âŒ '{token}' ä¸åœ¨vocabä¸­\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51821c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ­£åœ¨å¤„ç†æ–‡ä»¶: dataset/bbc_news_data.jsonl\n",
      "================================================================================\n",
      "å·²å¤„ç† 100 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 1266\n",
      "å·²å¤„ç† 200 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 1525\n",
      "å·²å¤„ç† 300 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 1702\n",
      "å·²å¤„ç† 400 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 1708\n",
      "å·²å¤„ç† 500 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 1708\n",
      "å·²å¤„ç† 600 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 1708\n",
      "å·²å¤„ç† 700 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 2217\n",
      "å·²å¤„ç† 800 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 6270\n",
      "å·²å¤„ç† 900 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 6270\n",
      "å·²å¤„ç† 1000 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 6270\n",
      "å·²å¤„ç† 1100 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 6270\n",
      "å·²å¤„ç† 1200 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 1300 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 1400 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 1500 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 1600 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 1700 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 1800 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 1900 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 2000 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 2100 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "å·²å¤„ç† 2200 æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: 7710\n",
      "\n",
      "================================================================================\n",
      "ç»“æœç»Ÿè®¡:\n",
      "================================================================================\n",
      "æ€»æ ·æœ¬æ•°: 2225\n",
      "æœ€é•¿è®­ç»ƒæ–‡æœ¬é•¿åº¦: 7710 tokens\n",
      "æœ€é•¿æ–‡æœ¬çš„æ ·æœ¬ç´¢å¼•: 1185 (è¡Œå·: 1186)\n",
      "\n",
      "é•¿åº¦ç»Ÿè®¡:\n",
      "  å¹³å‡é•¿åº¦: 800.13 tokens\n",
      "  æœ€çŸ­é•¿åº¦: 259 tokens\n",
      "  æœ€é•¿é•¿åº¦: 7710 tokens\n",
      "  ä¸­ä½æ•°é•¿åº¦: 708 tokens\n",
      "\n",
      "æœ€é•¿æ ·æœ¬è¯¦æƒ…:\n",
      "  ç´¢å¼•: 1185\n",
      "  è¡Œå·: 1186\n",
      "  é•¿åº¦: 7710 tokens\n",
      "  Userå†…å®¹é¢„è§ˆ: æ ‡é¢˜ï¼šTerror powers expose 'tyranny'\n",
      "å†…å®¹ï¼š The Lord Chancellor has defended government plans to introduce...\n",
      "  Assistantå†…å®¹: politics\n",
      "  è®­ç»ƒæ¨¡æ¿é¢„è§ˆ: <|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "æ ‡é¢˜ï¼šTerror powers expose 'tyranny'\n",
      "å†…å®¹ï¼š The Lord Chancellor has defended government plans to introduce control orders to keep fo...\n",
      "\n",
      "é•¿åº¦åˆ†å¸ƒ:\n",
      "  0-1024    : 1797 æ¡ (80.76%)\n",
      "  0-100     :    0 æ¡ ( 0.00%)\n",
      "  100-200   :    0 æ¡ ( 0.00%)\n",
      "  200-300   :    1 æ¡ ( 0.04%)\n",
      "  300-400   :  100 æ¡ ( 4.49%)\n",
      "  400-500   :  243 æ¡ (10.92%)\n",
      "  500-600   :  382 æ¡ (17.17%)\n",
      "  600-700   :  363 æ¡ (16.31%)\n",
      "  700-800   :  253 æ¡ (11.37%)\n",
      "  800-900   :  210 æ¡ ( 9.44%)\n",
      "  900-1000  :  199 æ¡ ( 8.94%)\n",
      "  1000-1100 :  136 æ¡ ( 6.11%)\n",
      "  1100-1200 :  102 æ¡ ( 4.58%)\n",
      "  1200-1300 :   71 æ¡ ( 3.19%)\n",
      "  1300-1400 :   51 æ¡ ( 2.29%)\n",
      "  1400-1500 :   42 æ¡ ( 1.89%)\n",
      "  1500-1600 :   22 æ¡ ( 0.99%)\n",
      "  1600-1700 :   17 æ¡ ( 0.76%)\n",
      "  1700-1800 :    5 æ¡ ( 0.22%)\n",
      "  1800-1900 :    6 æ¡ ( 0.27%)\n",
      "  1900-2000 :    4 æ¡ ( 0.18%)\n",
      "  2000+     :   18 æ¡ ( 0.81%)\n"
     ]
    }
   ],
   "source": [
    "# è®¡ç®—bbc_news_train.jsonlä¸­æ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ¨¡æ¿åçš„æœ€é•¿æ–‡æœ¬é•¿åº¦\n",
    "import json\n",
    "\n",
    "def get_max_training_length(jsonl_path, tokenizer):\n",
    "    \"\"\"\n",
    "    è®¡ç®—è®­ç»ƒæ•°æ®è½¬æ¢ä¸ºè®­ç»ƒæ¨¡æ¿åçš„æœ€é•¿æ–‡æœ¬é•¿åº¦\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path: JSONLæ–‡ä»¶è·¯å¾„\n",
    "        tokenizer: tokenizerå¯¹è±¡\n",
    "    \n",
    "    Returns:\n",
    "        max_length: æœ€é•¿æ–‡æœ¬çš„tokené•¿åº¦\n",
    "        max_index: æœ€é•¿æ–‡æœ¬çš„æ ·æœ¬ç´¢å¼•\n",
    "        max_sample: æœ€é•¿æ–‡æœ¬çš„æ ·æœ¬å†…å®¹\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    max_index = -1\n",
    "    max_sample = None\n",
    "    all_lengths = []\n",
    "    \n",
    "    print(f\"æ­£åœ¨å¤„ç†æ–‡ä»¶: {jsonl_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                # æ£€æŸ¥æ•°æ®æ ¼å¼\n",
    "                if 'conversations' not in data:\n",
    "                    print(f\"âš ï¸  è¡Œ {line_num}: ç¼ºå°‘ 'conversations' å­—æ®µï¼Œè·³è¿‡\")\n",
    "                    continue\n",
    "                \n",
    "                conversations = data['conversations']\n",
    "                if not conversations or len(conversations) == 0:\n",
    "                    print(f\"âš ï¸  è¡Œ {line_num}: conversations ä¸ºç©ºï¼Œè·³è¿‡\")\n",
    "                    continue\n",
    "                \n",
    "                # ä½¿ç”¨ä¸SFTDatasetç›¸åŒçš„æ–¹å¼åˆ›å»ºè®­ç»ƒæ¨¡æ¿\n",
    "                # å‚è€ƒ SFTDataset._create_chat_prompt æ–¹æ³•\n",
    "                messages = conversations.copy()\n",
    "                tools = conversations[0][\"functions\"] if (conversations and conversations[0][\"role\"] == \"system\" and conversations[0].get(\"functions\")) else None\n",
    "                \n",
    "                # åº”ç”¨chat_templateï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰\n",
    "                prompt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False,\n",
    "                    tools=tools\n",
    "                )\n",
    "                \n",
    "                # å¯¹è½¬æ¢åçš„æ–‡æœ¬è¿›è¡Œtokenize\n",
    "                input_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "                length = len(input_ids)\n",
    "                all_lengths.append(length)\n",
    "                \n",
    "                # æ›´æ–°æœ€å¤§å€¼\n",
    "                if length > max_length:\n",
    "                    max_length = length\n",
    "                    max_index = line_num - 1  # è½¬æ¢ä¸º0-basedç´¢å¼•\n",
    "                    max_sample = {\n",
    "                        'index': max_index,\n",
    "                        'line_num': line_num,\n",
    "                        'length': length,\n",
    "                        'conversations': conversations,\n",
    "                        'prompt_preview': prompt[:200] + '...' if len(prompt) > 200 else prompt\n",
    "                    }\n",
    "                \n",
    "                # æ¯å¤„ç†100æ¡æ•°æ®æ‰“å°ä¸€æ¬¡è¿›åº¦\n",
    "                if line_num % 100 == 0:\n",
    "                    print(f\"å·²å¤„ç† {line_num} æ¡æ•°æ®ï¼Œå½“å‰æœ€é•¿é•¿åº¦: {max_length}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"âš ï¸  è¡Œ {line_num}: JSONè§£æé”™è¯¯ - {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸  è¡Œ {line_num}: å¤„ç†é”™è¯¯ - {e}\")\n",
    "                continue\n",
    "    \n",
    "    return max_length, max_index, max_sample, all_lengths\n",
    "\n",
    "# æ‰§è¡Œè®¡ç®—ï¼ˆéœ€è¦å…ˆè¿è¡ŒCell 10åˆå§‹åŒ–tokenizerï¼‰\n",
    "if 'tokenizer' in globals():\n",
    "    max_length, max_index, max_sample, all_lengths = get_max_training_length('dataset/bbc_news_data.jsonl', tokenizer)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ç»“æœç»Ÿè®¡:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"æ€»æ ·æœ¬æ•°: {len(all_lengths)}\")\n",
    "    print(f\"æœ€é•¿è®­ç»ƒæ–‡æœ¬é•¿åº¦: {max_length} tokens\")\n",
    "    print(f\"æœ€é•¿æ–‡æœ¬çš„æ ·æœ¬ç´¢å¼•: {max_index} (è¡Œå·: {max_sample['line_num']})\")\n",
    "    print(f\"\\né•¿åº¦ç»Ÿè®¡:\")\n",
    "    print(f\"  å¹³å‡é•¿åº¦: {sum(all_lengths) / len(all_lengths):.2f} tokens\")\n",
    "    print(f\"  æœ€çŸ­é•¿åº¦: {min(all_lengths)} tokens\")\n",
    "    print(f\"  æœ€é•¿é•¿åº¦: {max(all_lengths)} tokens\")\n",
    "    print(f\"  ä¸­ä½æ•°é•¿åº¦: {sorted(all_lengths)[len(all_lengths)//2]} tokens\")\n",
    "    \n",
    "    # æ˜¾ç¤ºæœ€é•¿æ ·æœ¬çš„è¯¦ç»†ä¿¡æ¯\n",
    "    if max_sample:\n",
    "        print(f\"\\næœ€é•¿æ ·æœ¬è¯¦æƒ…:\")\n",
    "        print(f\"  ç´¢å¼•: {max_sample['index']}\")\n",
    "        print(f\"  è¡Œå·: {max_sample['line_num']}\")\n",
    "        print(f\"  é•¿åº¦: {max_sample['length']} tokens\")\n",
    "        print(f\"  Userå†…å®¹é¢„è§ˆ: {max_sample['conversations'][0]['content'][:100]}...\")\n",
    "        print(f\"  Assistantå†…å®¹: {max_sample['conversations'][1]['content']}\")\n",
    "        print(f\"  è®­ç»ƒæ¨¡æ¿é¢„è§ˆ: {max_sample['prompt_preview']}\")\n",
    "    \n",
    "    # é•¿åº¦åˆ†å¸ƒç»Ÿè®¡\n",
    "    print(f\"\\né•¿åº¦åˆ†å¸ƒ:\")\n",
    "    # ä»0-2000æ¯éš”100è®¾ç½®ä¸€ä¸ªåŒºé—´\n",
    "    length_ranges = [(0,1024,'0-1024')]\n",
    "    for start in range(0, 2000, 100):\n",
    "        end = start + 100\n",
    "        label = f\"{start}-{end}\"\n",
    "        length_ranges.append((start, end, label))\n",
    "    # æ·»åŠ æœ€åä¸€ä¸ªåŒºé—´ï¼š2000+\n",
    "    length_ranges.append((2000, float('inf'), \"2000+\"))\n",
    "    \n",
    "    for min_len, max_len, label in length_ranges:\n",
    "        count = sum(1 for l in all_lengths if min_len <= l < max_len)\n",
    "        percentage = count / len(all_lengths) * 100 if all_lengths else 0\n",
    "        print(f\"  {label:10s}: {count:4d} æ¡ ({percentage:5.2f}%)\")\n",
    "else:\n",
    "    print(\"âš ï¸  è¯·å…ˆè¿è¡Œ Cell 10 åˆå§‹åŒ– tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b8ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312pt291cu128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
