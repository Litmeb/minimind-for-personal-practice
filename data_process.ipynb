{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "adfc117c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import random\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "from model.model_lora import *\n",
    "from trainer.trainer_utils import setup_seed\n",
    "from torch.utils.data import Dataset\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78e38b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_csv('dataset/bbc-news-data.csv',on_bad_lines='warn',encoding='utf-8',sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "30c8277b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'business', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'entertainment', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'politics', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'sport', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech', 'tech']\n"
     ]
    }
   ],
   "source": [
    "data=[]\n",
    "for index,row in df.iterrows():\n",
    "    data.append(row[df.columns[0]])\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c8d73418",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "index = [i for i in range(len(df))]\n",
    "random.shuffle(index)\n",
    "df = df.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0b1cfc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      "5\n",
      "2\n",
      "9\n",
      "3\n",
      "4\n",
      "3\n",
      "4\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "11\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "5\n",
      "4\n",
      "3\n",
      "5\n",
      "7\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "4\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "8\n",
      "3\n",
      "10\n",
      "4\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "8\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "5\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "5\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "7\n",
      "1\n",
      "5\n",
      "2\n",
      "7\n",
      "1\n",
      "2\n",
      "5\n",
      "9\n",
      "5\n",
      "4\n",
      "7\n",
      "4\n",
      "10\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "3\n",
      "5\n",
      "2\n",
      "6\n",
      "4\n",
      "5\n",
      "3\n",
      "2\n",
      "4\n",
      "7\n",
      "3\n",
      "6\n",
      "9\n",
      "1\n",
      "2\n",
      "6\n",
      "2\n",
      "9\n",
      "3\n",
      "3\n",
      "6\n",
      "9\n",
      "5\n",
      "3\n",
      "3\n",
      "1\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "5\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "1\n",
      "7\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "5\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "9\n",
      "2\n",
      "2\n",
      "6\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "5\n",
      "2\n",
      "5\n",
      "3\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "5\n",
      "3\n",
      "2\n",
      "5\n",
      "5\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "4\n",
      "2\n",
      "7\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "5\n",
      "8\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "2\n",
      "1\n",
      "4\n",
      "1\n",
      "3\n",
      "1\n",
      "6\n",
      "2\n",
      "4\n",
      "5\n",
      "1\n",
      "2\n",
      "2\n",
      "1\n",
      "4\n",
      "3\n",
      "4\n",
      "5\n",
      "3\n",
      "2\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "10\n",
      "3\n",
      "3\n",
      "7\n",
      "3\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "1\n",
      "5\n",
      "1\n",
      "3\n",
      "3\n",
      "5\n",
      "1\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "4\n",
      "1\n",
      "5\n",
      "4\n",
      "2\n",
      "6\n",
      "16\n",
      "8\n",
      "6\n",
      "2\n",
      "5\n",
      "4\n",
      "4\n",
      "20\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "6\n",
      "3\n",
      "1\n",
      "1\n",
      "4\n",
      "5\n",
      "2\n",
      "6\n",
      "4\n",
      "6\n",
      "5\n",
      "2\n",
      "3\n",
      "12\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "1\n",
      "2\n",
      "1\n",
      "7\n",
      "1\n",
      "3\n",
      "4\n",
      "2\n",
      "1\n",
      "2\n",
      "5\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "2\n",
      "5\n",
      "1\n",
      "4\n",
      "5\n",
      "4\n",
      "9\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "5\n",
      "10\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "7\n",
      "4\n",
      "5\n",
      "2\n",
      "4\n",
      "2\n",
      "4\n",
      "3\n",
      "1\n",
      "6\n",
      "2\n",
      "9\n",
      "8\n",
      "2\n",
      "3\n",
      "4\n",
      "10\n",
      "5\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "5\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "6\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "6\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "3\n",
      "9\n",
      "4\n",
      "9\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "4\n",
      "8\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "5\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "1\n",
      "1\n",
      "6\n",
      "3\n",
      "5\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "5\n",
      "26\n",
      "3\n",
      "3\n",
      "6\n",
      "11\n",
      "3\n",
      "4\n",
      "8\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "1\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "6\n",
      "3\n",
      "5\n",
      "2\n",
      "7\n",
      "3\n",
      "2\n",
      "9\n",
      "3\n",
      "4\n",
      "3\n",
      "7\n",
      "3\n",
      "6\n",
      "5\n",
      "2\n",
      "7\n",
      "3\n",
      "15\n",
      "3\n",
      "3\n",
      "6\n",
      "5\n",
      "3\n",
      "1\n",
      "4\n",
      "1\n",
      "5\n",
      "3\n",
      "4\n",
      "7\n",
      "5\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "2\n",
      "3\n",
      "7\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "3\n",
      "3\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "5\n",
      "2\n",
      "3\n",
      "4\n",
      "2\n",
      "3\n",
      "2\n",
      "4\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "7\n",
      "5\n",
      "3\n",
      "5\n",
      "4\n",
      "5\n",
      "2\n",
      "2\n",
      "2\n",
      "1\n",
      "9\n",
      "2\n",
      "3\n",
      "8\n",
      "4\n",
      "2\n",
      "2\n",
      "4\n",
      "4\n",
      "6\n",
      "1\n",
      "8\n",
      "3\n",
      "1\n",
      "6\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "7\n",
      "1\n",
      "3\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "3\n",
      "4\n",
      "9\n",
      "3\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "3\n",
      "33\n",
      "3\n",
      "3\n",
      "2\n",
      "5\n",
      "11\n",
      "6\n",
      "2\n",
      "3\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "6\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "4\n",
      "2\n",
      "6\n",
      "3\n",
      "2\n",
      "4\n",
      "6\n",
      "10\n",
      "10\n",
      "1\n",
      "3\n",
      "3\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "6\n",
      "6\n",
      "5\n",
      "1\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "6\n",
      "7\n",
      "5\n",
      "2\n",
      "1\n",
      "27\n",
      "4\n",
      "7\n",
      "3\n",
      "2\n",
      "6\n",
      "3\n",
      "4\n",
      "1\n",
      "4\n",
      "4\n",
      "6\n",
      "2\n",
      "5\n",
      "3\n",
      "2\n",
      "4\n",
      "4\n",
      "4\n",
      "1\n",
      "3\n",
      "2\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "3\n",
      "4\n",
      "3\n",
      "6\n",
      "4\n",
      "4\n",
      "7\n",
      "2\n",
      "7\n",
      "6\n",
      "5\n",
      "2\n",
      "4\n",
      "4\n",
      "3\n",
      "4\n",
      "1\n",
      "4\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "5\n",
      "2\n",
      "4\n",
      "4\n",
      "2\n",
      "1\n",
      "2\n",
      "5\n",
      "6\n",
      "1\n",
      "8\n",
      "4\n",
      "4\n",
      "3\n",
      "5\n",
      "6\n",
      "1\n",
      "6\n",
      "4\n",
      "3\n",
      "1\n",
      "2\n",
      "1\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "1\n",
      "5\n",
      "5\n",
      "2\n",
      "1\n",
      "9\n",
      "1\n",
      "4\n",
      "2\n",
      "5\n",
      "5\n",
      "2\n",
      "6\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "3\n",
      "2\n",
      "8\n",
      "7\n",
      "1\n",
      "5\n",
      "5\n",
      "2\n",
      "4\n",
      "3\n",
      "1\n",
      "7\n",
      "3\n",
      "5\n",
      "7\n",
      "6\n",
      "1\n",
      "5\n",
      "6\n",
      "1\n",
      "6\n",
      "1\n",
      "1\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "6\n",
      "5\n",
      "6\n",
      "8\n",
      "3\n",
      "3\n",
      "4\n",
      "3\n",
      "2\n",
      "4\n",
      "4\n",
      "6\n",
      "5\n",
      "6\n",
      "4\n",
      "6\n",
      "7\n",
      "2\n",
      "3\n",
      "3\n",
      "4\n",
      "1\n",
      "1\n",
      "1\n",
      "3\n",
      "6\n",
      "2\n",
      "6\n",
      "1\n",
      "3\n",
      "5\n",
      "4\n",
      "4\n",
      "1\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "6\n",
      "7\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "4\n",
      "2\n",
      "4\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "1\n",
      "3\n",
      "4\n",
      "2\n",
      "2\n",
      "3\n",
      "7\n",
      "5\n",
      "3\n",
      "5\n",
      "2\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "1\n",
      "2\n",
      "4\n",
      "1\n",
      "3\n",
      "6\n",
      "4\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "3\n",
      "6\n",
      "2\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "5\n",
      "3\n",
      "3\n",
      "3\n",
      "4\n",
      "6\n",
      "4\n",
      "3\n",
      "4\n",
      "3\n",
      "6\n",
      "1\n",
      "1\n",
      "1\n",
      "2\n",
      "5\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "5\n",
      "8\n",
      "6\n",
      "4\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "5\n",
      "1\n",
      "2\n",
      "9\n",
      "1\n",
      "3\n",
      "3\n",
      "5\n",
      "3\n",
      "2\n",
      "2\n",
      "7\n",
      "6\n",
      "1\n",
      "1\n",
      "4\n",
      "10\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "4\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "5\n",
      "8\n",
      "2\n",
      "4\n",
      "5\n",
      "9\n",
      "3\n",
      "3\n",
      "49\n",
      "1\n",
      "2\n",
      "6\n",
      "4\n",
      "2\n",
      "3\n",
      "1\n",
      "6\n",
      "1\n",
      "4\n",
      "3\n",
      "1\n",
      "1\n",
      "1\n",
      "4\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "4\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "2\n",
      "1\n",
      "5\n",
      "4\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "5\n",
      "6\n",
      "3\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "6\n",
      "6\n",
      "1\n",
      "5\n",
      "5\n",
      "1\n",
      "8\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "5\n",
      "5\n",
      "5\n",
      "4\n",
      "4\n",
      "5\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "7\n",
      "7\n",
      "5\n",
      "5\n",
      "8\n",
      "2\n",
      "1\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "8\n",
      "3\n",
      "2\n",
      "4\n",
      "6\n",
      "2\n",
      "3\n",
      "3\n",
      "5\n",
      "7\n",
      "4\n",
      "2\n",
      "2\n",
      "2\n",
      "4\n",
      "2\n",
      "1\n",
      "2\n",
      "4\n",
      "3\n",
      "4\n",
      "2\n",
      "7\n",
      "2\n",
      "1\n",
      "6\n",
      "2\n",
      "8\n",
      "11\n",
      "4\n",
      "3\n",
      "8\n",
      "5\n",
      "2\n",
      "2\n",
      "1\n",
      "7\n",
      "4\n",
      "5\n",
      "4\n",
      "4\n",
      "1\n",
      "2\n",
      "3\n",
      "3\n",
      "1\n",
      "2\n",
      "2\n",
      "5\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "4\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "3\n",
      "1\n",
      "4\n",
      "4\n",
      "3\n",
      "7\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "3\n",
      "4\n",
      "4\n",
      "3\n",
      "3\n",
      "6\n",
      "6\n",
      "3\n",
      "3\n",
      "1\n",
      "3\n",
      "2\n",
      "4\n",
      "6\n",
      "2\n",
      "2\n",
      "2\n",
      "3\n",
      "1\n",
      "3\n",
      "4\n",
      "6\n",
      "2\n",
      "9\n",
      "1\n",
      "3\n",
      "3\n",
      "7\n",
      "4\n",
      "1\n",
      "8\n",
      "4\n",
      "2\n",
      "4\n",
      "7\n",
      "2\n",
      "6\n",
      "2\n",
      "1\n",
      "4\n",
      "3\n",
      "1\n",
      "1\n",
      "5\n",
      "10\n",
      "2\n",
      "4\n",
      "4\n",
      "3\n",
      "1\n",
      "3\n",
      "1\n",
      "3\n",
      "7\n",
      "1\n",
      "6\n",
      "8\n",
      "6\n",
      "5\n",
      "3\n",
      "5\n",
      "2\n",
      "5\n",
      "2\n",
      "2\n",
      "3\n",
      "3\n",
      "2\n",
      "5\n",
      "4\n",
      "7\n",
      "1\n",
      "3\n",
      "1\n",
      "4\n",
      "2\n",
      "4\n",
      "3\n",
      "2\n",
      "7\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "def split_text_into_chunks(text, target_words=700, overlap_words=50):\n",
    "    \"\"\"\n",
    "    将文本切分成指定单词数的chunks，尽量在句子边界处切分\n",
    "    \n",
    "    Args:\n",
    "        text: 要切分的文本\n",
    "        target_words: 目标单词数（约）\n",
    "        overlap_words: chunk之间的重叠单词数\n",
    "    \n",
    "    Returns:\n",
    "        chunks列表\n",
    "    \"\"\"\n",
    "    if not text or not text.strip():\n",
    "        return []\n",
    "    \n",
    "    # 按空格分割单词\n",
    "    words = text.split()\n",
    "    if len(words) <= target_words:\n",
    "        return [text]\n",
    "    \n",
    "    chunks = []\n",
    "    start_idx = 0\n",
    "    \n",
    "    while start_idx < len(words):\n",
    "        # 计算当前chunk的结束位置\n",
    "        end_idx = min(start_idx + target_words, len(words))\n",
    "        \n",
    "        # 如果还没到文本末尾，尝试在句子边界处切分\n",
    "        if end_idx < len(words):\n",
    "            # 向后查找句子结束符（. ! ? 后跟空格或换行）\n",
    "            # 在目标位置前后100个单词范围内查找\n",
    "            search_start = max(start_idx + target_words - 100, start_idx)\n",
    "            search_end = min(end_idx + 100, len(words))\n",
    "            \n",
    "            best_split = end_idx\n",
    "            # 从目标位置向前查找句子边界\n",
    "            for i in range(end_idx, search_start, -1):\n",
    "                if i < len(words):\n",
    "                    # 检查前一个单词是否以句子结束符结尾\n",
    "                    prev_word = words[i-1] if i > 0 else \"\"\n",
    "                    if prev_word and re.search(r'[.!?]$', prev_word):\n",
    "                        best_split = i\n",
    "                        break\n",
    "            \n",
    "            end_idx = best_split\n",
    "        \n",
    "        # 提取chunk\n",
    "        chunk_words = words[start_idx:end_idx]\n",
    "        chunk_text = \" \".join(chunk_words)\n",
    "        \n",
    "        if chunk_text.strip():\n",
    "            chunks.append(chunk_text.strip())\n",
    "        \n",
    "        # 移动到下一个chunk的起始位置（考虑重叠）\n",
    "        if end_idx >= len(words):\n",
    "            break\n",
    "        start_idx = max(end_idx - overlap_words, start_idx + 1)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def generate_prompt(title,content):\n",
    "    chunks=split_text_into_chunks(content,200,100)\n",
    "    print(len(chunks))\n",
    "    response=[]\n",
    "    for i in chunks:\n",
    "        response.append(f\"title:{title}\\nbody:{i}\\naccording to the content, please classify the article into one of the following categories: business, entertainment, politics, sport, technology.\\nanswer: \")\n",
    "    return response\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data=data\n",
    "        self.prompt = []\n",
    "        self.label=[]\n",
    "        for i in range(len(data)):\n",
    "            response=generate_prompt(data.at[i,'title'],data.at[i,'content'])\n",
    "            self.prompt.extend(response)\n",
    "            self.label.extend([data.at[i,'category']]*len(response))\n",
    "    def __len__(self):\n",
    "        return len(self.prompt)\n",
    "df1=df[:1113]\n",
    "df2=df[1113:]\n",
    "df1 = df1.reset_index(drop=True)\n",
    "df2 = df2.reset_index(drop=True)\n",
    "train_dataset=dataset(df1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "333a2f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(title,content):\n",
    "    # chunks=split_text_into_chunks(content,200,100)\n",
    "    # print(len(chunks))\n",
    "    # response=[]\n",
    "    # for i in chunks:\n",
    "    #     response.append(f\"title:{title}\\nbody:{i}\\naccording to the content, please classify the article into one of the following categories: business, entertainment, politics, sport, technology.\\nanswer: \")\n",
    "    return f\"title:{title}\\nbody:{content}\\naccording to the content, please classify the article into one of the following categories: business, entertainment, politics, sport, technology.\\nanswer: \"\n",
    "class dataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data=data\n",
    "        self.prompt = []\n",
    "        self.label=[]\n",
    "        for i in range(len(data)):\n",
    "            response=generate_prompt(data.at[i,'title'],data.at[i,'content'])\n",
    "            self.prompt.append(response)\n",
    "            self.label.append(data.at[i,'category'])\n",
    "    def __len__(self):\n",
    "        return len(self.prompt)\n",
    "test_dataset=dataset(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1bbcf1cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1112\n"
     ]
    }
   ],
   "source": [
    "print(len(test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1ede7605",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "已保存 1112 条数据到 dataset/bbc_news_test_en.jsonl\n"
     ]
    }
   ],
   "source": [
    "# 将训练集转换为jsonl格式（参考lora_identity.jsonl的结构）\n",
    "import json\n",
    "\n",
    "def save_dataset_to_jsonl(dataset, output_file):\n",
    "    \"\"\"\n",
    "    将dataset转换为jsonl格式并保存\n",
    "    \n",
    "    Args:\n",
    "        dataset: dataset对象，包含prompt和label属性\n",
    "        output_file: 输出文件路径\n",
    "    \"\"\"\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for i in range(len(dataset)):\n",
    "            # 构建conversations格式\n",
    "            conversation = {'text':dataset.prompt[i],'label':dataset.label[i]}\n",
    "            # 写入jsonl文件（每行一个JSON对象）\n",
    "            f.write(json.dumps(conversation, ensure_ascii=False) + '\\n')\n",
    "    print(f\"已保存 {len(dataset)} 条数据到 {output_file}\")\n",
    "# save_dataset_to_jsonl(full_dataset, 'dataset/bbc_news_data_no_instruction.jsonl')\n",
    "# 保存训练集\n",
    "save_dataset_to_jsonl(test_dataset, 'dataset/bbc_news_test_en.jsonl')\n",
    "\n",
    "# # 保存测试集（可选）\n",
    "# save_dataset_to_jsonl(test_dataset, 'dataset/bbc_news_test_no_instruction.jsonl')\n",
    "\n",
    "# # 验证：读取前几行查看格式\n",
    "# print(\"\\n验证格式（前3条）：\")\n",
    "# with open('dataset/bbc_news_train.jsonl', 'r', encoding='utf-8') as f:\n",
    "#     for i, line in enumerate(f):\n",
    "#         if i >= 3:\n",
    "#             break\n",
    "#         data = json.loads(line)\n",
    "#         print(f\"\\n第{i+1}条:\")\n",
    "#         print(f\"  User: {data['conversations'][0]['content'][:100]}...\")\n",
    "#         print(f\"  Assistant: {data['conversations'][1]['content']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23f8537d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在检查数据文件: dataset/bbc_news_train.jsonl\n",
      "================================================================================\n",
      "\n",
      "[基础检查] JSON格式和基本结构...\n",
      "  总样本数: 1113\n",
      "  JSON格式错误: 0\n",
      "  空conversations: 0\n",
      "  结构错误: 0\n",
      "  空内容: 0\n",
      "\n",
      "✅ 基础检查通过！\n"
     ]
    }
   ],
   "source": [
    "# 基础检查：不需要tokenizer，只检查JSON格式和结构\n",
    "import json\n",
    "\n",
    "def check_training_data_basic(jsonl_path):\n",
    "    \"\"\"\n",
    "    基础检查：不需要tokenizer，只检查JSON格式和结构\n",
    "    \"\"\"\n",
    "    print(f\"正在检查数据文件: {jsonl_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    stats = {\n",
    "        'total_samples': 0,\n",
    "        'json_errors': [],\n",
    "        'empty_conversations': [],\n",
    "        'invalid_structure': [],\n",
    "        'empty_content': []\n",
    "    }\n",
    "    \n",
    "    print(\"\\n[基础检查] JSON格式和基本结构...\")\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            stats['total_samples'] += 1\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                if 'conversations' not in data:\n",
    "                    stats['invalid_structure'].append({\n",
    "                        'line': line_num,\n",
    "                        'error': 'Missing \"conversations\" field'\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                if not data['conversations'] or len(data['conversations']) == 0:\n",
    "                    stats['empty_conversations'].append({'line': line_num})\n",
    "                    continue\n",
    "                \n",
    "                for i, conv in enumerate(data['conversations']):\n",
    "                    if 'role' not in conv or 'content' not in conv:\n",
    "                        stats['invalid_structure'].append({\n",
    "                            'line': line_num,\n",
    "                            'error': f'Conversation {i} missing \"role\" or \"content\"'\n",
    "                        })\n",
    "                        break\n",
    "                    if not conv['content'] or len(conv['content'].strip()) == 0:\n",
    "                        stats['empty_content'].append({\n",
    "                            'line': line_num,\n",
    "                            'conversation_index': i,\n",
    "                            'role': conv.get('role', 'unknown')\n",
    "                        })\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                stats['json_errors'].append({\n",
    "                    'line': line_num,\n",
    "                    'error': str(e)\n",
    "                })\n",
    "    \n",
    "    print(f\"  总样本数: {stats['total_samples']}\")\n",
    "    print(f\"  JSON格式错误: {len(stats['json_errors'])}\")\n",
    "    print(f\"  空conversations: {len(stats['empty_conversations'])}\")\n",
    "    print(f\"  结构错误: {len(stats['invalid_structure'])}\")\n",
    "    print(f\"  空内容: {len(stats['empty_content'])}\")\n",
    "    \n",
    "    if stats['json_errors'] or stats['invalid_structure'] or stats['empty_conversations']:\n",
    "        print(\"\\n⚠️  发现基础问题，请先修复！\")\n",
    "    else:\n",
    "        print(\"\\n✅ 基础检查通过！\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "# 先执行基础检查（不需要tokenizer）\n",
    "basic_stats = check_training_data_basic('dataset/bbc_news_train.jsonl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a78a058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在完整检查数据文件: dataset/bbc_news_train.jsonl\n",
      "================================================================================\n",
      "\n",
      "[1] 检查JSON格式和基本结构...\n",
      "  总样本数: 1113\n",
      "  JSON格式错误: 0\n",
      "  空conversations: 0\n",
      "  结构错误: 0\n",
      "  空内容: 0\n",
      "\n",
      "[2] 检查loss_mask（这是导致nan的关键）...\n",
      "\n",
      "================================================================================\n",
      "详细异常报告:\n",
      "================================================================================\n",
      "\n",
      "⚠️  关键问题: loss_mask全为0 (901 个)\n",
      "   这会导致训练时除零错误，产生nan！\n",
      "  样本索引 0 (行 1)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：Fockers fuel festive film chart\n",
      "内容： Comedy Meet...\n",
      "      [1] assistant: entertainment\n",
      "  样本索引 2 (行 3)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：'Hitler' row over Welsh arts cash\n",
      "内容： An artist...\n",
      "      [1] assistant: politics\n",
      "  样本索引 3 (行 4)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：Davenport hits out at Wimbledon\n",
      "内容： World numbe...\n",
      "      [1] assistant: sport\n",
      "  样本索引 5 (行 6)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：Straw to attend Auschwitz service\n",
      "内容： Foreign S...\n",
      "      [1] assistant: politics\n",
      "  样本索引 6 (行 7)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：US state acts to stop 'spammers'\n",
      "内容： US state T...\n",
      "      [1] assistant: tech\n",
      "  样本索引 7 (行 8)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：A-listers flock to Gervais sitcom\n",
      "内容： Hollywood...\n",
      "      [1] assistant: entertainment\n",
      "  样本索引 8 (行 9)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：Row brewing over peer-to-peer ads\n",
      "内容： Music dow...\n",
      "      [1] assistant: tech\n",
      "  样本索引 9 (行 10)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：Young debut cut short by Ginepri\n",
      "内容： Fifteen-ye...\n",
      "      [1] assistant: sport\n",
      "  样本索引 10 (行 11)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：Reboot ordered for EU patent law\n",
      "内容： A European...\n",
      "      [1] assistant: tech\n",
      "  样本索引 11 (行 12)\n",
      "    conversations数量: 2\n",
      "      [0] user: 标题：Blair looks to election campaign\n",
      "内容： Tony Blair...\n",
      "      [1] assistant: politics\n",
      "\n",
      "⚠️  序列过长 (1113 个):\n",
      "  样本索引 0 (行 1): 长度 511\n",
      "  样本索引 1 (行 2): 长度 511\n",
      "  样本索引 2 (行 3): 长度 511\n",
      "  样本索引 3 (行 4): 长度 511\n",
      "  样本索引 4 (行 5): 长度 511\n",
      "\n",
      "================================================================================\n",
      "检查总结:\n",
      "================================================================================\n",
      "⚠️  发现 901 个潜在问题\n",
      "\n",
      "🔴 严重: 有 901 个样本的loss_mask全为0\n",
      "   这些样本会导致训练时出现nan，建议修复或过滤！\n"
     ]
    }
   ],
   "source": [
    "# 完整检查：需要tokenizer，检查loss_mask（这是导致nan的关键）\n",
    "# 注意：需要先运行Cell 10初始化model和tokenizer\n",
    "from dataset.lm_dataset import SFTDataset\n",
    "\n",
    "def check_training_data_full(jsonl_path, tokenizer, max_length=10240):\n",
    "    \"\"\"\n",
    "    完整检查：需要tokenizer，检查loss_mask（这是导致nan的关键）\n",
    "    \n",
    "    检查项：\n",
    "    1. JSON格式是否正确\n",
    "    2. conversations结构是否完整\n",
    "    3. loss_mask是否全为0（会导致除零错误）\n",
    "    4. tokenized后的长度是否异常\n",
    "    5. 是否有空内容\n",
    "    \"\"\"\n",
    "    print(f\"正在完整检查数据文件: {jsonl_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # 统计信息\n",
    "    stats = {\n",
    "        'total_samples': 0,\n",
    "        'json_errors': [],\n",
    "        'empty_conversations': [],\n",
    "        'invalid_structure': [],\n",
    "        'zero_loss_mask': [],\n",
    "        'too_long': [],\n",
    "        'too_short': [],\n",
    "        'empty_content': [],\n",
    "        'special_chars_issues': []\n",
    "    }\n",
    "    \n",
    "    # 1. 检查JSON格式和基本结构\n",
    "    print(\"\\n[1] 检查JSON格式和基本结构...\")\n",
    "    valid_samples = []\n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            stats['total_samples'] += 1\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                if 'conversations' not in data:\n",
    "                    stats['invalid_structure'].append({\n",
    "                        'line': line_num,\n",
    "                        'error': 'Missing \"conversations\" field'\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                if not data['conversations'] or len(data['conversations']) == 0:\n",
    "                    stats['empty_conversations'].append({\n",
    "                        'line': line_num,\n",
    "                        'data': data\n",
    "                    })\n",
    "                    continue\n",
    "                \n",
    "                for i, conv in enumerate(data['conversations']):\n",
    "                    if 'role' not in conv or 'content' not in conv:\n",
    "                        stats['invalid_structure'].append({\n",
    "                            'line': line_num,\n",
    "                            'error': f'Conversation {i} missing \"role\" or \"content\"'\n",
    "                        })\n",
    "                        break\n",
    "                    if not conv['content'] or len(conv['content'].strip()) == 0:\n",
    "                        stats['empty_content'].append({\n",
    "                            'line': line_num,\n",
    "                            'conversation_index': i,\n",
    "                            'role': conv.get('role', 'unknown')\n",
    "                        })\n",
    "                \n",
    "                valid_samples.append((line_num, data))\n",
    "                \n",
    "            except json.JSONDecodeError as e:\n",
    "                stats['json_errors'].append({\n",
    "                    'line': line_num,\n",
    "                    'error': str(e),\n",
    "                    'content': line[:100]\n",
    "                })\n",
    "    \n",
    "    print(f\"  总样本数: {stats['total_samples']}\")\n",
    "    print(f\"  JSON格式错误: {len(stats['json_errors'])}\")\n",
    "    print(f\"  空conversations: {len(stats['empty_conversations'])}\")\n",
    "    print(f\"  结构错误: {len(stats['invalid_structure'])}\")\n",
    "    print(f\"  空内容: {len(stats['empty_content'])}\")\n",
    "    \n",
    "    # 2. 使用SFTDataset检查loss_mask（这是导致nan的关键）\n",
    "    print(\"\\n[2] 检查loss_mask（这是导致nan的关键）...\")\n",
    "    try:\n",
    "        dataset = SFTDataset(jsonl_path, tokenizer, max_length=max_length)\n",
    "        \n",
    "        for idx in range(len(dataset)):\n",
    "            try:\n",
    "                X, Y, loss_mask = dataset[idx]\n",
    "                \n",
    "                # 检查loss_mask是否全为0\n",
    "                loss_mask_sum = loss_mask.sum().item()\n",
    "                if loss_mask_sum == 0:\n",
    "                    stats['zero_loss_mask'].append({\n",
    "                        'index': idx,\n",
    "                        'line': valid_samples[idx][0] if idx < len(valid_samples) else 'unknown',\n",
    "                        'X_shape': X.shape,\n",
    "                        'Y_shape': Y.shape\n",
    "                    })\n",
    "                \n",
    "                # 检查长度\n",
    "                seq_len = X.shape[0]\n",
    "                if seq_len < 10:\n",
    "                    stats['too_short'].append({\n",
    "                        'index': idx,\n",
    "                        'line': valid_samples[idx][0] if idx < len(valid_samples) else 'unknown',\n",
    "                        'length': seq_len\n",
    "                    })\n",
    "                elif seq_len >= max_length - 1:\n",
    "                    stats['too_long'].append({\n",
    "                        'index': idx,\n",
    "                        'line': valid_samples[idx][0] if idx < len(valid_samples) else 'unknown',\n",
    "                        'length': seq_len\n",
    "                    })\n",
    "                    \n",
    "            except Exception as e:\n",
    "                stats['special_chars_issues'].append({\n",
    "                    'index': idx,\n",
    "                    'line': valid_samples[idx][0] if idx < len(valid_samples) else 'unknown',\n",
    "                    'error': str(e)\n",
    "                })\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"  错误: 无法创建dataset - {e}\")\n",
    "    \n",
    "    # 3. 打印详细报告\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"详细异常报告:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    if stats['json_errors']:\n",
    "        print(f\"\\n❌ JSON格式错误 ({len(stats['json_errors'])} 个):\")\n",
    "        for err in stats['json_errors'][:5]:\n",
    "            print(f\"  行 {err['line']}: {err['error']}\")\n",
    "    \n",
    "    if stats['zero_loss_mask']:\n",
    "        print(f\"\\n⚠️  关键问题: loss_mask全为0 ({len(stats['zero_loss_mask'])} 个)\")\n",
    "        print(\"   这会导致训练时除零错误，产生nan！\")\n",
    "        for err in stats['zero_loss_mask'][:10]:\n",
    "            print(f\"  样本索引 {err['index']} (行 {err['line']})\")\n",
    "            if err['index'] < len(valid_samples):\n",
    "                line_num, data = valid_samples[err['index']]\n",
    "                print(f\"    conversations数量: {len(data['conversations'])}\")\n",
    "                for i, conv in enumerate(data['conversations']):\n",
    "                    content_preview = conv['content'][:50] + \"...\" if len(conv['content']) > 50 else conv['content']\n",
    "                    print(f\"      [{i}] {conv['role']}: {content_preview}\")\n",
    "    \n",
    "    if stats['empty_conversations']:\n",
    "        print(f\"\\n❌ 空conversations ({len(stats['empty_conversations'])} 个):\")\n",
    "        for err in stats['empty_conversations'][:5]:\n",
    "            print(f\"  行 {err['line']}\")\n",
    "    \n",
    "    if stats['invalid_structure']:\n",
    "        print(f\"\\n❌ 结构错误 ({len(stats['invalid_structure'])} 个):\")\n",
    "        for err in stats['invalid_structure'][:5]:\n",
    "            print(f\"  行 {err['line']}: {err['error']}\")\n",
    "    \n",
    "    if stats['empty_content']:\n",
    "        print(f\"\\n⚠️  空内容 ({len(stats['empty_content'])} 个):\")\n",
    "        for err in stats['empty_content'][:5]:\n",
    "            print(f\"  行 {err['line']}, conversation {err['conversation_index']}, role: {err['role']}\")\n",
    "    \n",
    "    if stats['too_short']:\n",
    "        print(f\"\\n⚠️  序列过短 ({len(stats['too_short'])} 个):\")\n",
    "        for err in stats['too_short'][:5]:\n",
    "            print(f\"  样本索引 {err['index']} (行 {err['line']}): 长度 {err['length']}\")\n",
    "    \n",
    "    if stats['too_long']:\n",
    "        print(f\"\\n⚠️  序列过长 ({len(stats['too_long'])} 个):\")\n",
    "        for err in stats['too_long'][:5]:\n",
    "            print(f\"  样本索引 {err['index']} (行 {err['line']}): 长度 {err['length']}\")\n",
    "    \n",
    "    if stats['special_chars_issues']:\n",
    "        print(f\"\\n⚠️  特殊字符问题 ({len(stats['special_chars_issues'])} 个):\")\n",
    "        for err in stats['special_chars_issues'][:5]:\n",
    "            print(f\"  样本索引 {err['index']} (行 {err['line']}): {err['error']}\")\n",
    "    \n",
    "    # 4. 总结\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"检查总结:\")\n",
    "    print(\"=\" * 80)\n",
    "    total_issues = (len(stats['json_errors']) + \n",
    "                   len(stats['empty_conversations']) + \n",
    "                   len(stats['invalid_structure']) + \n",
    "                   len(stats['zero_loss_mask']) + \n",
    "                   len(stats['empty_content']))\n",
    "    \n",
    "    if total_issues == 0:\n",
    "        print(\"✅ 未发现明显异常！\")\n",
    "    else:\n",
    "        print(f\"⚠️  发现 {total_issues} 个潜在问题\")\n",
    "        if stats['zero_loss_mask']:\n",
    "            print(f\"\\n🔴 严重: 有 {len(stats['zero_loss_mask'])} 个样本的loss_mask全为0\")\n",
    "            print(\"   这些样本会导致训练时出现nan，建议修复或过滤！\")\n",
    "    \n",
    "    return stats\n",
    "\n",
    "full_stats = check_training_data_full(\n",
    "    jsonl_path='dataset/bbc_news_train.jsonl',\n",
    "    tokenizer=tokenizer,\n",
    "    max_length=512\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b7e35a3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniMind模型参数: 25.83 M(illion)\n"
     ]
    }
   ],
   "source": [
    "device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "args = argparse.Namespace(\n",
    "        load_from='model',\n",
    "        save_dir='out',\n",
    "        weight='full_sft',\n",
    "        lora_weight='None',\n",
    "        hidden_size=512,\n",
    "        num_hidden_layers=8,\n",
    "        use_moe=0,\n",
    "        inference_rope_scaling=False,\n",
    "        max_new_tokens=8192,\n",
    "        temperature=0.85,\n",
    "        top_p=0.85,\n",
    "        historys=0,\n",
    "        device='cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    )\n",
    "def init_model(args):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.load_from)\n",
    "    if 'model' in args.load_from:\n",
    "        model = MiniMindForCausalLM(MiniMindConfig(\n",
    "            hidden_size=args.hidden_size,\n",
    "            num_hidden_layers=args.num_hidden_layers,\n",
    "            use_moe=bool(args.use_moe),\n",
    "            inference_rope_scaling=args.inference_rope_scaling\n",
    "        ))\n",
    "        moe_suffix = '_moe' if args.use_moe else ''\n",
    "        ckp = f'./{args.save_dir}/{args.weight}_{args.hidden_size}{moe_suffix}.pth'\n",
    "        model.load_state_dict(torch.load(ckp, map_location=args.device), strict=True)\n",
    "        if args.lora_weight != 'None':\n",
    "            apply_lora(model)\n",
    "            load_lora(model, f'./{args.save_dir}/lora/{args.lora_weight}_{args.hidden_size}.pth')\n",
    "    else:\n",
    "        model = AutoModelForCausalLM.from_pretrained(args.load_from, trust_remote_code=True)\n",
    "    print(f'MiniMind模型参数: {sum(p.numel() for p in model.parameters()) / 1e6:.2f} M(illion)')\n",
    "    return model.eval().to(device), tokenizer\n",
    "model, tokenizer = init_model(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbc16676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43, 2344, 363, 1059, 3067]\n",
      "I| love| you| so| much|"
     ]
    }
   ],
   "source": [
    "sentence='''I love you so much'''\n",
    "encoded=tokenizer.encode(sentence,add_special_tokens=False)\n",
    "print(encoded)\n",
    "for i in encoded:\n",
    "    print(tokenizer.decode([i]),end='|')\n",
    "# decoded=tokenizer.decode(encoded)\n",
    "# print(decoded)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d311ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<|endoftext|>', '<|im_start|>', '<|im_end|>', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '¡', '¢', '£', '¤', '¥', '¦', '§', '¨', '©', 'ª', '«', '¬', '®', '¯', '°', '±', '²', '³', '´', 'µ', '¶', '·', '¸', '¹', 'º', '»', '¼', '½', '¾', '¿', 'À', 'Á', 'Â', 'Ã', 'Ä', 'Å', 'Æ', 'Ç', 'È', 'É', 'Ê', 'Ë', 'Ì', 'Í', 'Î', 'Ï', 'Ð', 'Ñ', 'Ò', 'Ó', 'Ô', 'Õ', 'Ö', '×', 'Ø', 'Ù', 'Ú', 'Û', 'Ü', 'Ý', 'Þ', 'ß', 'à', 'á', 'â', 'ã', 'ä', 'å', 'æ', 'ç', 'è', 'é', 'ê', 'ë', 'ì', 'í', 'î', 'ï', 'ð', 'ñ', 'ò', 'ó', 'ô', 'õ', 'ö', '÷', 'ø', 'ù', 'ú', 'û', 'ü', 'ý', 'þ', 'ÿ', 'Ā', 'ā', 'Ă', 'ă', 'Ą', 'ą', 'Ć', 'ć', 'Ĉ', 'ĉ', 'Ċ', 'ċ', 'Č', 'č', 'Ď', 'ď', 'Đ', 'đ', 'Ē', 'ē', 'Ĕ', 'ĕ', 'Ė', 'ė', 'Ę', 'ę', 'Ě', 'ě', 'Ĝ', 'ĝ', 'Ğ', 'ğ', 'Ġ', 'ġ', 'Ģ', 'ģ', 'Ĥ', 'ĥ', 'Ħ', 'ħ', 'Ĩ', 'ĩ', 'Ī', 'ī', 'Ĭ', 'ĭ', 'Į', 'į', 'İ', 'ı', 'Ĳ', 'ĳ', 'Ĵ', 'ĵ', 'Ķ', 'ķ', 'ĸ', 'Ĺ', 'ĺ', 'Ļ', 'ļ', 'Ľ', 'ľ', 'Ŀ', 'ŀ', 'Ł', 'ł', 'Ń', 'Ġt', 'Ġa', 'in', 'he', 're', 'ï¼', 'ä¸', 'on', 'at', 'çļ', 'çļĦ', 'ï¼Į', 'Ġs', 'Ġc', 'nd', 'ãĢ', 'er', 'Ġthe', 'es', 'en', 'or', 'an', 'Ġand', 'ing', 'Ġp', 'it', 'al', 'ãĢĤ', 'Ġo', 'Ġw', 'ä»', 'Ġto', 'is', 'ou', 'Ġm', 'äº', 'Ġin', 'Ġf', 'Ġb', 'ed', 'ion', 'åı', 'ic', 'Ġd', 'Ġof', 'le', 'ar', 'ro', 'ĠĠ', 'åħ', 'ent', 'æľ', 'Ġe', 'åĴ', 'è¿', 'ä½', 'åĴĮ', 'æĪ', 'å®', 'åĪ', 've', 'us', 'Ġre', 'Ġh', 'Ġth', 'as', 'ct', 'çĶ', 'om', 'åľ', 'å¤', 'æĺ', 'åĬ', 'åĲ', 'ä¸Ģ', 'im', 'è¯', 'æĸ', 'ation', 'lo', 'ç»', 'Ġbe', 'ãĢģ', 'id', 'Ġcan', 'il', 'æĺ¯', 'ä¹', 'è®', 'ĠA', 'Ġthat', 'ĠT', 'ä»¥', 'ch', 'Ġy', 'ce', 'ï¼ļ', 'ot', 'ers', 'Ġn', 'éĢ', 'ra', 'å°', 'Ġg', 'Ġyou', 'åŃ', 'Ġpro', 'et', 'åº', 'åľ¨', 'ly', 'Ġis', 'ä¸ª', 'Ġl', 'ur', 'Ġfor', 'åı¯', 'éĩ', 'st', 'çļĦæ', 'ut', 'Ġhe', 'if', 'ĥ½', 'ä¼', 'ĠI', 'è¡', 'ir', 'ith', 'å¹', 'Ġare', 'ig', 'Ġst', 'el', 'ol', 'å¸', 'ul', 'æĿ', 'æĪĳ', 'Ġon', 'è¦', 'æľī', 'æĹ', 'å¯', 'è§', 'è¦ģ', 'Ġus', 'ay', 'æķ', 'çī', 'ow', 'ment', 'çĶ¨', 'ess', 'ä¸Ń', 'ä»¬', 'äºº', 'åĩ', 'Ġex', 'ĠĠĠĠ', 'åĽ', 'åĮ', 'å¼', 'Ġcon', 'se', 'èĥ½', 'çİ', 'Ġan', 'Ġwith', 'ä¸º', 'ate', 'iv', 'am', 'Ġas', 'ure', 'è¿Ļ', 'åĨ', 'çŃ', 'Ġor', 'å·', 'Ġal', 'ies', 'ç§', 'Ġim', 'æĢ', 'ver', 'ab', 'äºĨ', 'Ġsu', 'Ġde', 'ge', 'th', 'åı¯ä»¥', 'èĢ', 'ä¸į', 'å¾', 'ĠAI', 'Ġen', 'éĹ', 'æī', 'ak', 'ive', 'Ġmo', 'å¥', 'éĿ', 'çĽ', 'ity', 'ä¿', 'un', 'è´', 'åį', 'Ġit', 'Ġimp', 'ect', 'æł', 'å½', 'èĩ', 'é¢', 'åĵ', 'æ³', 'ort', 'ad', 'æŀ', 'em', 'Ġcom', 'å¦', 'her', 'ere', 'ĠS', 'ial', 'ĠC', 'ĠThe', 'çĲ', 'çĶŁ', 'æĦ', 'pp', 'æŃ', 'æĸ¹', 'qu', 'Ġwh', 'å¦Ĥ', 'éľ', 'ant', 'Ġle', 'Ġv', 'æĭ', 'æĬ', 'ust', 'æĹ¶', 'çŃī', 'åĳ', 'å¯¹', 'ter', 'ld', 'è¡Į', 'Ġch', 'ud', 'éľĢ', 'æ°', 'æĪĲ', 'Ġ|', 'ac', 'ain', 'iz', 'æı', 'ions', 'Ġha', 'æĽ', '--', 'æĿ¥', 'ome', 'å¿', \"'s\", 'Ġne', 'est', 'ä¾', 'um', 'åĪ°', 'åľ°', 'ist', 'âĢ', 'çī©', 'ä¸Ģä¸ª', 'lp', 'æİ', 'èĩª', 'Ġhelp', 'Ġtheir', 'æĶ', 'ä½ľ', 'ä¼ļ', 'æĮ', 'æĪĳä»¬', 'nt', 'äºİ', 'åĪĨ', 'res', 'pe', 'åĩº', 'ide', 'æĥ', 'ĠH', 'è¾', 'ĠM', 'ff', 'æ¯', 'od', 'ical', 'Ġwor', 'ä¸Ĭ', 'are', 'æĽ´', 'Ġyour', 'ä¸ĭ', 'èµ', 'ations', 'æķ°', 'Ġte', 'åİ', 'çĲĨ', 'ĠTh', 'è¿ĩ', 'å¹¶', 'du', 'éĿ¢', 'Ġad', 'ill', 'æµ', 'å¥½', 'oc', 'act', 'éľĢè¦ģ', 'ä»ĸ', 'å±', 'Ġr', 'Ġmore', 'åŃ¦', 'ç®', 'igh', 'äºĽ', 'ĠB', 'åĬ¨', 'åĵģ', 'èī', 'ple', 'Ġinc', 'åĲĮ', 'Ġexp', 'ould', 'ä½ł', 'æį', 'æıĲ', 'å¤§', 'çİ°', 'pt', 'ĠP', 'all', 'åĬł', 'ç§į', 'Ġse', 'åĬĽ', 'out', 'Ġhave', 'çº', 'ä½ĵ', 'Ġprov', 'åĮĸ', 'å¤ļ', 'å®ļ', 'Ġused', 'éĢļ', 'cc', 'è¿Ľ', 'æ´', 'Ġsh', 'Ġab', 'os', 'Ġres', 'ĠThis', 'ç¨', 'æĢ§', 'age', 'ri', 'æ¸', 'able', 'åŃĲ', 'Ġby', 'åıĳ', 'éĩı', 'åºĶ', 'Ġlo', 'ä½¿', 'åħ¶', 'é«', 'éĻ', 'é«ĺ', 'åº¦', 'è§£', 'é£', 'å°Ĩ', 'æ³ķ', 'and', 'ä¿Ŀ', 'ans', 'for', 'rom', 'reat', 'Ġpl', 'çļĦç', 'å¸¸', 'è½', 'Ġwe', 'è¡¨', 'ake', 'æĪĸ', 'é¢ĺ', 'åŁ', 'Ġme', 'æĸĩ', 'ther', 'ke', 'å®¶', 'åĲĪ', 'æľĢ', 'ine', 'Ġsome', 'ç±', 'éĩį', 'æŀľ', 'ĠW', 'ĠE', 'éĺ', 'our', 'rou', 'çĤ', 'æ±', 'åħ³', 'Ġint', 'ance', 'ä¹Ł', 'éģ', 'ĠĠĠ', 'å®ĥ', 'ag', 'æ¬', '00', 'è°', 'ult', 'yst', 'éĹ´', 'ç³', 'Ġtr', 'pl', 'art', 'æĦŁ', 'æĤ', 'ata', 'ĠF', 'form', 'è®¡', 'Ġfrom', 'ĠD', 'éĹ®', 'ight', 'ces', 'æį®', 'lop', 'ä¹ĭ', 'Ġfe', 'åģ', 'velop', 'Ġ1', 'åĽł', 'ks', 'æ²', 'Ġu', 'å°ı', 'ystem', 'Ġdis', 'ĠR', 'gy', 'å·¥', 'ç¨ĭ', 'å¢', 'ence', 'èĤ', 'ç¡', 'Ġtra', 'å»', 'åħ¥', 'ign', 'alth', 'Ġsuch', 'ach', 'æĻ', 'arn', 'Ġdata', 'è¶', 'å®ŀ', 'so', 'Ġdevelop', 'ç¤', 'Ġacc', 'ast', 'èĢĮ', 'Ġ\"', 'Ġother', 'å»º', 'Ġeff', 'ç«', 'Ġman', 'åħ¬', 'åĢ', 'çĦ', 'ms', 'å¼ı', 'èī²', 'å¾Ĺ', 'ific', 'Ġj', 'Ġro', 'Ġhas', 'chn', 'olo', 'åĪ¶', 'èĬ', 'ä½¿çĶ¨', 'ous', 'ual', 'Ġat', 'Ġem', 'ell', 'Ġsystem', 'Ġhealth', 'ities', 'Ġexam', 'ib', 'éĶ', 'Ġabout', 'äº§', 'åĲİ', 'æĦı', 'ç±»', 'Ġpre', 'æĤ¨', 'Ġalso', 'ents', 'Ġind', 'ind', 'éĢĤ', 'Ġtechn', 'ress', 'æĥħ', 'éĹ®é¢ĺ', 'Ġuse', 'ï¼Ł', 'Ġincl', 'Ġspe', 'ich', 'ps', 'æľº', 'Ġthey', 'ie', 'Ġhow', 'Ġwork', 'ä¸ļ', 'ç´', 'Ġimpro', 'Ġlearn', 'æĸ°', 'çĤ¹', 'Ġcont', 'ard', 'çĦ¶', 'æľ¬', 'ç³»', 'ç¡®', 'è®¾', 'åħ·', 'éĢī', 'èĢħ', 'éħ', 'gh', '__', 'Ġnot', 'çľ', 'çĽ¸', 'Ġprovide', 'åī', 'ional', 'Ġens', 'ä¸İ', 'è´¨', 'ential', 'ç»ı', 'å¿ĥ', 'ang', 'æŃ¤', 'end', 'Ġpo', 'è¿Ľè¡Į', 'ice', 'Ġ-', 'Ġway', 'å·±', 'Ġ2', 'ime', 'ç½', 'èĩªå·±', 'Ġun', 'bot', 'Ġinclud', 'ated', 'æ°´', 'éķ', 'æĮģ', 'ä»£', 'é¡', 'æīĢ', 'çĿ', 'pport', 'ood', 'ike', 'ru', 'Ġcomm', 'ĠL', 'ä¿¡', 'ĠG', 'çŁ', 'çĶµ', 'Ġwas', 'low', 'erv', 'åĮħ', 'ĠĠĠĠĠĠĠĠ', 'Ġwhe', 'dit', 'Ġwhich', 'Ġcomp', 'éª', 'ore', 'ç¾', 'Ġ=', 'çī¹', 'iff', 'ert', 'æģ', 'rit', 'Ġrec', 'åĨħ', 'æĺİ', 'ors', 'Ġpat', '----', 'æŁ', 'Ġapp', 'ns', 'åĬ¡', 'aly', 'ace', 'æ´»', 'ä¾Ľ', 'av', 'ä¸»', 'Ġpers', 'çĥ', 'è¯¥', 'Ġmy', 'ç©', 'eri', 'è®©', 'æĬĢ', 'éķ¿', 'ack', 'ĠN', 'Ġdiff', 'Ġthis', 'åĿ', 'Ġensure', 'å½ĵ', 'Ġout', 'Ġcl', 'Ġk', 'é¦', 'ount', 'çİ¯', 'åĬ©', 'Ġtechnolo', 'Ġthese', 'ful', 'éļ', 'æ·', 'ä¸ĢäºĽ', 'Ġsoc', 'å¼Ģ', 'å¤©', 'Ġev', 'Ġredu', 'Ġthem', 'Ġ(', 'éĥ½', 'æĪ·', 'è·', 'åľº', 'æ°Ķ', 'ĠY', 'è¯Ń', 'éĢļè¿ĩ', 'å±ķ', 'Ġco', 'å½±', 'ç¬', 'Ġanaly', 'æ¯Ķ', 'åħ¨', 'Ġimprove', 'ç»ĵ', 'å¹´', 'çķ', 'çĿĢ', 'Ġhum', 'Ġqu', 'ç®Ĺ', 'ĠO', 'é£Ł', 'ility', 'Ġsystems', 'åıĺ', 'ail', 'ç¼', 'çł', 'è¿Ļä¸ª', 'æıĲä¾Ľ', 'ase', 'åŀ', 'ments', 'Ġpot', 'Ġany', 'ä½Ĩ', 'Ġcons', 'ĠIt', 'æł¼', 'Ġar', 'æľ¯', 'éĿŀ', 'Ġdo', 'Ġmay', 'æĭ©', 'ue', 'éĢīæĭ©', 'ry', 'éĥ', 'Ġlike', 'ong', 'èģ', '``', 'ile', 'æ±Ĥ', 'Ġnew', 'ient', 'Ġimpact', 'è¿ĺ', 'æ³¨', 'ä¹Ī', 'çĽ®', 'âĢľ', 'âĢĿ', 'ef', 'ä¾ĭ', 'Ġpotential', 'ok', 'åı¯èĥ½', 'Ġtrans', 'Ġact', 'ï¼ī', 'Ġspec', 'æ¶', 'Ġwill', 'äº¤', 'ize', 'ç¾İ', 'å¸Ĥ', 'Ġstud', 'pon', 'èº', 'ä¸įåĲĮ', 'one', 'å¾Ī', 'åıĬ', 'å¦Ĥæŀľ', 'çĲĥ', 'ange', 'Ġneed', 'å¤ĸ', 'ety', 'aking', 'è¯·', 'ater', 'Ġperson', 'ident', 'Ġso', 'Ġmake', 'å¹³', 'å¤Ł', 'èº«', 'ï¼Ī', 'Ġinform', 'æ¡', 'äºĭ', 'åıĹ', 'ased', 'ild', 'Ġoff', 'Ġthere', 'cis', 'è¢', 'éĥ¨', 'æ¯ı', 'ract', 'ass', 'Ġlearning', 'åĸ', 'å½¢', 'ire', 'ä»İ', 'bots', 'èĻ', 'å¸®', 'Ġdes', 'ĠIn', 'cess', 'Ġpe', 'ify', 'Ġwho', 'ä¹ł', 'æľŁ', 'Ġexperi', 'éĤ', 'Ġsc', 'ep', 'ä½ķ', 'Ġtime', 'éĿŀå¸¸', 'æĭ¬', 'åķ', 'ä»¥ä¸ĭ', 'éģĵ', 'Ġcommun', 'Ġcould', 'ap', 'èĲ', 'è°ĥ', 'lic', 'duct', 'Ġits', 'cy', 'è¯´', 'Ġmed', 'Ġcol', 'ular', 'éĩįè¦ģ', 'Ġsp', 'åĪ©', 'èµ·', 'Ġprovid', 'ices', 'åĻ', 'æĸĻ', 'Ġimport', 'ural', 'åŃĹ', 'Ġund', 'int', 'Ġover', 'åı¸', 'æł¹', 'é¥', 'ples', 'ä»ĸä»¬', 'gra', 'uring', 'now', 'åįķ', 'è¿ĻäºĽ', 'åīį', 'å®ī', 'Ġpr', 'åĮħæĭ¬', 'ç»Ļ', 'The', 'ä½į', 'å§', 'ç´ł', 'åĳĺ', 'Ġident', 'åŀĭ', 'Ġadd', 'å¼º', 'æĺ¯ä¸Ģ', 'ip', 'gor', 'Ġsupport', 'ne', 'Ġdiffere', 'åħĥ', 'Ġass', 'åĨ³', 'éĽ', 'åĲį', 'Ġgo', 'Ġtechnology', 'æĢ»', 'è®®', 'Ġinter', 'Ġinv', 'Ġour', 'æķĪ', 'ustom', 'Ġrel', 'ife', 'åĻ¨', 'ings', 'ä»·', 'Ġpart', 'è¢«', 'æīĭ', 'ary', 'Ġrespon', 'ĊĠĠĠ', 'å¥½çļĦ', 'ative', 'å¸®åĬ©', 'ç»Ł', 'æĶ¾', 'ĠHere', 'çģ', 'Ġbut', 'æģ¯', 'æŃ£', 'ark', 'åħ¬åı¸', 'ory', 'å¢ĥ', 'lect', 'éŁ', 'æĥ³', 'é£İ', 'ating', 'Ġam', 'its', 'æ»', 'gorith', 'åĵį', 'ures', 'Ġeffect', 'Ġshould', 'Ġper', 'è±', 'ç²', 'ict', 'Ġalgorith', 'uc', 'rough', 'ä»»', 'ä»¶', 'Ġbet', 'ia', 'Ġanalyz', 'æł¹æį®', 'ized', 'æµģ', 'è§Ĥ', 'è£', 'æłĩ', 'iron', 'Ġcustom', 'Ġreg', 'Ġpersonal', 'èĥ½å¤Ł', 'ics', 'ivid', 'çĪ', 'èµĦ', 'æŃ¥', 'å®¹', 'åĪĽ', 'èĪ', 'ä¹Ĳ', 'å¯¼', 'gan', 'èĬĤ', 'Ġall', 'ens', 'ame', 'ness', 'Ġup', 'ĠU', 'èĢĥ', 'elf', 'åĢ¼', 'å°ĳ', 'æľį', 'ari', 'thical', 'viron', 'èĥ', 'ord', 'Ġsign', 'éĩĮ', 'ound', 'ople', 'åŁº', 'Ġinformation', 'Ġidentify', 'åĽŀ', 'Ġcre', 'éŁ³', 'ible', 'ub', 'è¿Ĳ', 'Ġlead', 'æ¸¸', 'æ¬¡', 'åĨĻ', 'éĤ£', 'get', 'èį', 'Ġexample', 'ä¼ĺ', 'å½±åĵį', 'ish', 'xt', 'æº', 'éªĮ', 'ob', 'å®¢', 'å¤ĩ', 'åģ¥', 'è½¦', 'ç¤¾', 'ividual', 'ered', 'les', 'Ġenviron', 'Ġpeople', 'æĺŁ', 'çĸ', 'çĭ', 'Ġdet', 'æĹł', 'Ġif', 'ose', 'ite', 'å¢ŀ', 'éĴ', 'åĲĮæĹ¶', 'è¿°', 'æĸ¹å¼ı', 'åĽ½', 'é»', 'å¤Ħ', 'Ġexamples', 'æ®', 'Ġinto', 'æĮĩ', 'Ġhuman', 'åĲĳ', 'ç¤º', 'æķ°æį®', 'Ġ3', 'ĠJ', 'èı', 'çİ¯å¢ĥ', 'als', 'erst', 'Ġethical', 'ç»Ħ', 'ä¼ł', 'Ġdifferent', 'Ġknow', 'åºı', 'Ġindividual', 'æıĲé«ĺ', 'round', 'å°±', 'åıĸ', 'åŃĺ', 'ä¸¤', 'çŁ¥', 'ources', 'ck', 'å£', 'ines', 'è¾¾', 'Ġmany', 'æķ´', 'æł·', 'ditional', 'omm', 'çĶ±', 'éĢł', 'å®ĥä»¬', 'ues', 'Ġment', 'Ġimportant', 'Ġopt', 'Ġloc', 'ph', 'Ġprocess', 'Ġalgorithms', 'è®¾è®¡', 'Ġsocial', 'very', 'åĪĻ', 'ä¾ĭå¦Ĥ', 'è®¤', 'Ġaut', 'Ġserv', 'gg', 'äº§åĵģ', 'è§Ħ', 'çľĭ', 'vel', 'æĸ¹æ³ķ', 'Ġben', 'åĽłæŃ¤', 'care', 'per', 'åĬŁ', 'å»ºè®®', 'Ġpos', 'æ¤', 'we', 'åĮº', 'iqu', 'Ġreal', 'æĹ¥', 'Ġreduce', 'af', 'angu', 'Ġsk', 'Ġed', 'erstand', 'åĨµ', 'mot', 'åħĪ', 'ç¥', 'åºĶè¯¥', 'Ġthrough', 'Ġconc', 'åıĳå±ķ', 'è¯ķ', 'æ¡Ī', 'Ġenvironment', 'åı£', 'Ġadv', 'åĪ«', 'Ġbenef', 'æ¸ħ', 'åĳ³', 'åħī', 'Ġdevelopment', 'eng', 'å¦Ĥä½ķ', 'ç®¡', 'ivers', 'åĲĦ', 'Ġris', 'row', 'ergy', 'è®¡ç®Ĺ', 'ä¿¡æģ¯', 'Ġproduct', 'è¾ĥ', 'è®º', 'èĩªå·±çļĦ', 'æĬ¤', 'åıį', 'åħ¶ä»ĸ', 'åĪĹ', 'ç»Ĩ', 'ç©º', 'Ġgreat', 'ear', 'æºĲ', 'ject', 'çĶŁæ´»', 'ä¸ŃçļĦ', 'Ġunderstand', 'èĭ', 'hat', 'Ġprogra', 'çĬ', 'éĩĳ', 'Ġincluding', 'Ġaccess', 'ĠĠĠĠĠĠĠ', 'è¯Ĩ', 'ç¦', 'og', 'è£ħ', 'Ġart', 'Ġwrit', 'Ġincre', 'Ġph', 'æĸ¹éĿ¢', 'Ġpract', 'Ġusing', 'é¡¹', 'æİ¥', 'Ġways', 'Ġlangu', 'æĶ¯', 'Ġchall', 'åİ»', '____', 'imate', 'æĸŃ', 'è¨', 'Ġwell', 'll', 'Ġpol', 'æĢģ', 'Ġra', 'Can', 'åİŁ', 'ber', 'è¨Ģ', 'ç«ĭ', 'Ġgen', 'éħį', 'æ·±', 'te', 'ä¸ī', 'ç§ĳ', 'ĠFor', 'çº¿', 'çħ', 'æ¼', 'åķĨ', 'æĿĲ', 'Ġsignific', 'Ġgu', 'Ġdecis', 'Ġtrain', 'Ġag', 'Ġcreat', 'å®Į', 'æĹ¶éĹ´', 'Ġone', 'èĦ', 'Ġnat', 'åŃ¦ä¹ł', 'çļĦæķ', 'ced', 'Ġwhen', 'Ġbi', 'èİ', 'æĽ´åĬł', 'ives', 'port', 'å·¥ä½ľ', 'ving', 'Ġbeen', 'æĻº', 'Ġlife', 'å¼ķ', 'arm', 'çİĩ', 'çĶ¨æĪ·', 'ä¹ī', 'ä»½', 'è¯Ŀ', 'iness', 'com', 'åº·', 'åĩı', 'ä»Ģ', 'è¾ĵ', 'Ġvari', 'con', 'Ġmod', 'ä»Ģä¹Ī', 'Ġenergy', 'æĬĢæľ¯', 'ertain', 'mm', 'verall', 'åĪĴ', 'Ġrobots', 'Ġorgan', 'æİ¨', 'ants', 'åĩĨ', 'ds', 'æŀģ', 'çĻ', 'Ġrequ', 'Ġess', 'ç®Ģ', 'ustain', 'æ¨', 'Ġstr', 'cing', 'ability', 'ree', 'Ġeduc', 'åİĨ', 'Ġcreate', 'åģ¥åº·', 'Ġdesign', 'ips', 'åģļ', 'èĬ±', 'ink', 'èıľ', 'æī¾', 'æ®µ', 'æµĭ', 'ĠV', 'ĠBy', 'åĶ', 'é¦ĸ', 'è¯į', 'Ġwhere', 'Ġdisc', 'äºĨè§£', 'ric', 'ä¸Ķ', 'è¶³', 'æĺ¯ä¸Ģä¸ª', 'arch', 'ç§¯', 'å¸¦', 'Ġwhile', 'Ġsignificant', 'çłģ', 'æĪ¿', 'Ġbeing', 'Ġlanguage', 'itive', '20', 'Ġanalyze', 'æĻ¯', 'èĮ', 'rib', 'æ¨¡', 'ĠSt', 'è´¹', \"'t\", 'Ġhealthcare', 'Ġexperience', 'Ġ5', 'ä¸ªäºº', 'ays', 'è±¡', 'plo', 'Ġwould', 'èĻĳ', 'æĶ¶', 'é¢Ħ', 'é¢Ĩ', 'ä¿ĿæĮģ', 'ences', 'åıª', 'èĩ´', 'æĪı', 'Ġmental', 'Ġfew', 'ates', 'è¿ĩç¨ĭ', 'å®īåħ¨', 'Ġsustain', 'Ġwere', 'å¤ª', 'çĮ', 'Ġspecific', 'Ġworld', 'çŃĶ', '```', 'Ġtake', 'åħ»', 'éĢŁ', 'ever', 'SS', 'éĶĢ', 'Ġbo', 'hes', 'Ġmus', 'æľįåĬ¡', 'è§Ĵ', 'ten', 'æŀĲ', 'pow', 'dict', 'vent', '10', 'çļĦæĹ', 'ĸçķ', 'Ġprot', 'ç½®', 'Ġhigh', 'Ġbus', 'Ġindust', 'åĲ¦', 'cial', 'äººä»¬', 'ĠAs', 'åĳĬ', 'ade', 'æĶ¹', 'çĹ', 'Ġhad', 'Ġher', 'Ġjust', 'ï¼Ľ', 'è´Ń', 'ç¬¬', 'éĵ', 'Ġwater', 'Ġfood', 'éĺŁ', 'aus', 'Ġchalleng', 'åħį', 'æĸĩåĮĸ', 'Ġmost', 'é¸', 'ç½ĳ', 'çĽ´', 'Ġsm', 'Ġactiv', 'ploy', 'Overall', 'å¿«', 'ruct', 'Ġindividuals', 'å§ĭ', 'gies', 'æŁ¥', 'çĪ±', 'iety', 'In', 'åĪĨæŀĲ', 'è§Ĩ', 'æ¸©', 'ç»´', 'olut', 'åŁŁ', 'ommend', 'Ġcomple', 'æķĻ', 'Ġbu', 'Ġeducation', 'ather', 'Ġ4', 'ting', 'Ġfind', 'æ²¡', 'Ġhis', 'ä¹ĭéĹ´', 'Ġeffective', 'Ġatt', 'Ġrese', 'èĥ½åĬĽ', 'åŁİ', 'Ġallow', 'Ġav', 'Ġpromot', 'æĻºèĥ½', 'æ»¡', 'åħ±', 'iew', 'come', 'ç³»ç»Ł', 'Ġrespons', 'äºĴ', 'Ġcult', 'powered', 'Ġrecommend', 'èĲ¥', 'OSS', 'Ġchange', 'è¯ģ', 'ved', 'æİĴ', 'è§£åĨ³', 'ici', 'ĠHow', 'Ġfeel', 'æľĪ', 'Ġwhat', 'ä»¥åıĬ', 'Ġsee', 'åŃ©', 'bs', 'Ġsur', 'æ£', 'ality', 'Ġvis', 'ç¡®ä¿Ŀ', 'pect', 'å®ŀçİ°', 'Ġcare', 'å¹¿', 'ills', 'åºŃ', 'ases', 'å¤į', 'åºĶçĶ¨', 'çļĦæĥ', 'ards', 'Ġaddress', 'Ġcompan', 'Ġinvol', 'Ġcustomer', 'åĽłä¸º', 'Ġstudents', 'Ġins', 'æ³¨æĦı', 'æŀĦ', 'æ¬¢', 'æµ·', 'åıĤ', 'èĩªçĦ¶', 'é©', 'ĠThese', 'wn', 'æĺĵ', 'çĬ¶', 'ren', 'Ġtreat', 'Ġbenefits', 'ĊĠĠĠĠĠĠĠ', 'å¯¹äºİ', 'æĢĿ', 'ider', 'ĠYes', 'ĠK', 'åĸľ', 'Ġke', 'Ġeng', 'Ġpop', 'ost', 'pare', 'Ġmon', 'æ¬¾', 'ĠMOSS', 'Ġemot', 'Ġac', 'ç¼ĸ', 'fore', 'åı¥', 'Ġval', 'ily', 'Ġiss', 'èĤī', 'èĩ³', 'æ¸¸æĪı', 'ween', 'Ġinclude', 'Ġprotect', 'åħ³ç³»', 'éĻ©', 'Ġsever', 'Ġthan', 'éľĢæ±Ĥ', 'ç»ĥ', 'ĠThey', 'iss', 'ys', 'Ġjob', 'éĺ³', 'æĲ', 'Ġbetween', 'Ġmach', '--------', 'èĢĥèĻĳ', 'è´¨éĩı', 'Ġbusiness', 'wor', 'ick', 'eg', 'åħħ', 'ç¯', 'æĿ¡', 'ner', 'apt', 'Ġappro', 'Ġplay', 'æ²¡æľī', '¤Ĳ', 'æľª', 'æĪĺ', 'å®¶åºŃ', 'ãĢĭ', 'ency', 'ĠCh', 'ãĢĬ', 'Ġproviding', 'Ġresources', 'âĢĻ', 'Ġassist', 'Ġnatural', 'è¯Ħ', 'ä¾¿', 'Ġsaf', 'åħ·æľī', 'è°¢', 'çĥŃ', 'ss', 'eth', 'old', 'Ġperform', 'Ġseveral', 'é¤Ĳ', 'Ġeach', 'è½¬', 'ci', 'Ġty', 'Ġpub', 'æ´»åĬ¨', 'ocus', 'çīĮ', 'è¶Ĭ', 'åĽ¢', 'è½»', 'è¯Ńè¨Ģ', 'Ġareas', 'éĩĩ', 'ft', 'riend', 'å·²', 'å¸Ĥåľº', 'ition', 'ients', 'ç®¡çĲĨ', 'è®¸', 'äººç±»', 'èº«ä½ĵ', 'ique', 'Ġpartic', 'ç»Ń', 'agement', 'ves', 'ç¬¦', 'line', 'çº¢', 'åĲ¸', 'Ġpatter', '000', 'ç¤¾ä¼ļ', 'åĨħå®¹', 'Ġorganiz', 'ough', 'Ġve', 'åŃ©åŃĲ', 'æĸ½', 'æ¤į', 'åĩł', 'ä½Ĩæĺ¯', 'Ġaff', 'Ġnum', 'lement', 'èīº', 'èĳ', 'Ġcar', 'ages', 'abor', 'æĺ¯ä¸Ģç§į', 'Ġinst', 'èĽ', 'ä¹ĭä¸Ģ', 'è·¯', 'åį³', 'Ġmain', 'éļı', 'How', 'å¿ħ', 'ç¨ĭåºı', 'éŁ³ä¹Ĳ', 'red', 'æ²¹', 'Ġoffer', 'ets', 'ç¢', 'Ġduring', 'çļĦäºº', 'æĽ´å¤ļ', 'Ġdi', 'ä»£çłģ', 'èİ·', 'åħĭ', 'Ġguid', 'ä¸»è¦ģ', 'Ġfam', 'æİ§', 'éĢļå¸¸', 'ĠAd', 'å¤ĦçĲĨ', 'urn', 'ower', 'åĳ½', 'æıı', 'Ġskills', 'Ġtool', 'ware', 'æĸĩæľ¬', 'Ġpatterns', 'çĽ®æłĩ', 'acy', 'æīĵ', 'åŁİå¸Ĥ', 'Ġevery', 'ries', 'è¯»', 'éģ¿', 'çĻ½', 'éĢĤåĲĪ', 'Ġpatient', 'çľŁ', 'oth', 'å¥¹', 'åĶ®', 'ä¸Ģç§į', 'Ġmade', 'ä½İ', 'ise', 'Ġrem', 'æ¶Ī', 'åĲ«', 'air', 'Ġgener', 'oy', 'ç²¾', 'æĥħåĨµ', 'ights', 'Ġexpl', 'è§ģ', 'Ġpredict', 'ç±³', 'æĽ´å¥½', 'ä¿®', 'Ġclimate', 'Ġfocus', 'Ġgrow', 'å®¢æĪ·', 'ä¸įæĸŃ', 'itor', 'ĠEn', 'çº¦', 'æĺ¯åĲ¦', 'ä»ħ', 'æĪĳä»¬çļĦ', 'æľĽ', 'op', 'Ġmaking', 'yth', 'ccess', 'Ġown', 'ggest', 'Ġtas', 'uture', 'Ġmodel', 'put', 'Ġresearch', 'erest', 'éļ¾', 'Ġ[', 'iel', 'ational', 'Ġcommunic', 'ç¥ŀ', 'ç©¶', 'Ġrest', 'æĪĲä¸º', 'king', 'pr', 'åĮ»', 'cur', 'èĤ²', \"Ġ'\", 'è¿Ļç§į', 'ç¯ĩ', 'Ġche', 'own', 'éĻħ', 'Ġfin', 'åĪ¶ä½ľ', 'Ġsuggest', 'å¢ŀåĬł', 'Ġmedia', 'ribut', 'çļĦæĥħ', 'åĬłåħ¥', 'Ġcle', 'åĳ¨', 'ç«ł', 'Ġthink', 'Ġlocal', 'pportun', 'ĠYou', 'Ġplan', 'Ġeven', 'éĽĨ', 'å·§', 'ax', 'Ġchallenges', 'Ġprof', 'ĠCan', 'Ġconcer', 'Ġfuture', 'åĬ¿', 'Ġref', 'èģĶ', 'Ġself', 'æĪĸèĢħ', 'ble', 'åĽ´', 'è¿ĲåĬ¨', 'Ġinf', 'éĩĬ', 'Ġsustainable', 'Ġtext', 'Ġgra', 'äºĮ', 'åĵģçīĮ', 'ä¸įåĲĮçļĦ', 'led', 'çĭ¬', 'Ġopportun', 'Ġcontin', 'ym', 'Ġget', 'å¯Ĩ', 'éĻ¤', 'æħ', 'éģ¿åħį', 'Ġ+', 'è§ī', 'Ġret', 'å¸ĥ', 'Ġinterest', 'Ġsociety', 'ç»ĵæŀľ', 'åĲ¬', 'é¦ĸåħĪ', 'Ġbre', 'Ġ20', 'ĠHowever', 'è®°', 'ons', 'è¿ĳ', 'å¼Ģå§ĭ', 'Ġbuild', 'Ġbeh', \"'m\", 'vers', 'Ġgood', 'çĲĨè§£', 'resent', 'ç¦»', 'åĬŁèĥ½', 'Ġeffort', 'labor', 'é»ĳ', 'Ġbetter', 'Ġread', 'å¾ĭ', 'èĽĭ', 'hed', 'ä¹°', 'å¯¼èĩ´', 'Ġimplement', 'ç¿', 'äº«', 'å¤´', 'ense', 'Ġlong', 'other', 'é¥®', 'åŃĺåľ¨', 'çļĦæĦ', 'ä¸Ģä»½', 'ython', 'ning', 'åĩıå°ĳ', 'åĢĻ', 'ä¸ĵ', 'åĲĦç§į', 'èħ', 'å°½', 'åįĩ', 'æĬ¥', 'Ġpublic', 'Ġlar', 'ä½łçļĦ', 'aut', 'é¢ĨåŁŁ', 'æļ', 'ollow', 'èģĮ', 'Ġchang', 'Ġbest', 'hip', 'åĨį', 'akes', 'Ġchat', 'ited', 'Ġpower', 'ä¿ĿæĬ¤', 'ä¹¦', 'è®¡åĪĴ', 'éĩįè¦ģçļĦ', 'åıĺåĮĸ', 'ilities', 'Ġconsider', 'æĪĳä»¬åı¯ä»¥', 'éĤ£ä¹Ī', 'Ġide', 'æ¼Ķ', 'aging', 'Ġbased', 'å®Ŀ', 'Ġrange', 'Ġresult', 'Ġmem', 'çħ§', 'Ġlevel', 'cou', 'Ġbr', 'Th', 'ä¼ģ', 'å»ºç«ĭ', 'Ġunique', 'è®Ń', 'Ġmark', 'è®¸å¤ļ', 'è¡Įä¸º', 'Ķç©¶', 'çļĦæĬ', 'Ġset', 'éª¤', 'ts', 'Ġhist', 'Ġaround', 'Ġrev', 'åħ¶ä¸Ń', 'ï¼ģ', 'æııè¿°', 'æľĢåĲİ', 'Ġsim', 'nect', 'åĽŀçŃĶ', 'éĺ²', 'èī¯', 'åĪ°äºĨ', 'ä¸ĸçķ', 'æĸ¹æ¡Ī', 'æĿĲæĸĻ', 'ä¸ĸçķĮ', 'æĽ´å¥½åľ°', 'ä¸¤ä¸ª', 'Ġemploy', 'Ġtry', 'æĵ', 'Ġback', 'åĪĩ', 'Ġsuccess', 'Ġdecisions', 'Ġthose', 'å¯Į', 'Ġfact', 'æİ¢', 'è¶£', 'Ġpractices', 'åĲĹ', 'æīį', 'çİ©', 'ption', 'æĸĩç«ł', 'Ġfeat', 'Ġprevent', 'Ġwriting', 'çļĦæĢ', 'Ġno', 'ä»ĭ', 'éĹ¨', 'Ġdel', 'æĴ', 'Ġoptim', 'ination', 'ĠĊ', 'usion', 'Ġaccount', 'ling', 'Ġdivers', '.\"', 'ath', 'èĭ±', 'ä¼ģä¸ļ', 'Ġgrou', 'åľ°çĲĥ', 'å¤±', 'Ġpersonalized', 'ĠHe', 'è¡¨è¾¾', 'curity', 'Ġfollow', 'äº§çĶŁ', 'Ġear', 'åİĭ', 'vern', 'Ġissues', 'åĿĩ', 'é²', 'Ġdr', 'iving', 'Ġtraining', 'Ġrisk', 'åĩ½', 'åı²', 'æĳ', 'çļĦæĹ¶', 'ogn', 'Ġrequire', 'Ġenvironmental', 'back', 'éĶ®', 'çĸĹ', 'Ġinteract', 'åĽ¢éĺŁ', 'æ¯ıä¸ª', 'çĦ¶åĲİ', 'Ġdist', 'çĶ¨äºİ', 'è®¤ä¸º', 'åĩ½æķ°', 'Ġsent', 'ĊĠĠĠĠĠĠĠĠ', 'Ġreducing', 'å¹²', 'Ġrep', 'Ġcaus', 'Ġmusic', 'çª', 'Ġmonitor', 'Ġform', 'é¢ľ', 'çĹħ', 'é¦Ļ', 'Ġoften', 'åı¯èĥ½ä¼ļ', 'åĳĺå·¥', 'Ġhand', 'æĬķ', 'Ġneeds', 'æŃ¤å¤ĸ', 'åıĭ', 'ivity', 'Ġactivities', 'åĸľæ¬¢', 'Ġpur', 'ian', 'self', 'åĬ¨çī©', 'comes', 'å©', 'Ġpriv', 'az', 'Ġrelations', 'Ġmachine', 'çļĦæ°', 'ä»·æł¼', 'ä»·åĢ¼', 'ç´¢', 'Ġfeed', 'ä¸Ģä¸ĭ', 'Ġteam', 'Ġindustry', 'è´¢', 'ĠPro', 'Ġwant', 'ç§°', 'Ġclass', 'Ġlove', 'åħ³äºİ', 'è¾ĵåħ¥', 'Ġtransport', 'Ġcomplex', 'Ġyear', 'éĶĢåĶ®', 'å¯»', 'ience', 'ists', 'æĶ¯æĮģ', 'Ġmind', 'Ġfun', 'Ġchar', 'æĮī', 'Ġconcerns', 'conom', 'ç®Ģåįķ', 'ä»¥ä¸ĭæĺ¯', 'Ġstart', 'å¹¶ä¸Ķ', 'avi', 'ä¸ŃåĽ½', 'åħĥç´ł', 'Ġconf', 'Ġpositive', 'Ġcur', 'Ġcount', 'ery', 'å¡', 'å®¤', 'Ġcost', 'Ġequ', 'Ġpolic', 'aste', 'aw', 'éħĴ', 'coura', 'iven', 'place', 'chie', 'çļĦæķ°', 'åĽłç´ł', 'Ġfl', 'ism', 'Ġmedical', 'Ġhumans', 'Ġautom', 'ertainly', 'Ġ0', 'Ġoffers', 'Ġdetect', 'Ġ6', 'é£İæł¼', 'Ġshow', 'çģ«', 'Ġanim', 'é¢ľèī²', 'lease', 'ave', 'åĵª', 'ĠThere', 'ä»¥ä¸Ĭ', 'æľªæĿ¥', 'XX', 'çīĩ', 'uch', 'Ġtasks', 'åħ·ä½ĵ', 'æ¤įçī©', 'Ġmin', 'èīºæľ¯', 'icult', 'Ġexperiences', 'æİ§åĪ¶', 'be', 'Ġpatients', 'å²', 'ĠWe', 'Ġrecogn', 'çĥ¤', 'Ġsmall', 'åĿĹ', 'åĦ', 'å¤ªéĺ³', 'ction', 'Ġent', 'æį¢', 'Ġbefore', 'Ġbecome', 'å·²ç»ı', 'è¡¨çİ°', 'Ġexplo', 'Ġachie', 'ä»»åĬ¡', 'å¤§çļĦ', 'Ġday', 'Ġfound', 'å±±', 'ond', 'Ġtreatment', 'pend', 'hen', 'Ġcondit', 'ç¡®å®ļ', 'Ġbusinesses', 'ĠWh', 'æīĢæľī', 'Ġdeveloped', 'ç»Ī', 'æŃ¥éª¤', 'Ġdifficult', 'åı·', 'ĠRe', 'éĶĻ', 'Ġcho', 'Ġquest', 'Ġtranspare', 'Ġproject', 'Ġcommunity', 'ov', 'å¸Ī', 'å¼ł', 'åĪĨç±»', 'äººçļĦ', 'sis', 'çĽĬ', 'oid', 'ĠAn', 'ways', 'Ġeas', 'Ġaffect', 'Ġothers', 'Ġregul', 'æĢ§åĴĮ', 'åĸĦ', 'agn', 'ä½ľä¸º', 'åı¯ä»¥å¸®åĬ©', 'åĦ¿', 'Ġorganizations', 'é¸¡', 'åħ´', 'Ġfriend', 'Ġ$', 'Ġdetail', 'Ġtraditional', 'Ġdesigned', 'è´Ńä¹°', 'ä½ĵéªĮ', 'ç»į', 'erm', 'Ġconnect', 'è¿Ļæł·', 'Ġrecommendations', 'Ġboth', 'ŁéĢļ', 'æ¯į', 'Ġsit', 'ä½ľçĶ¨', 'ä»ĭç»į', 'Ġste', 'ĠSure', 'åı°', 'æĤ¨çļĦ', 'Ġshe', 'Ġmanagement', 'joy', 'è´Ł', 'Ġpromote', 'Ġvarious', '(\"', 'por', 'Ġsens', 'Ġessential', 'gether', 'ularly', 'äºī', 'irst', 'Ġop', 'Ġspecies', 'çİ°åľ¨', 'cho', 'Ġbehavi', 'çŃĳ', 'å¥³', 'Ġquality', 'Ġext', 'è¥', 'å®ĮæĪĲ', 'æĢ»ä¹ĭ', 'éĥ¨åĪĨ', 'ä»İèĢĮ', 'åĽ¾', 'Ġtyp', 'Ġstrate', 'è¥¿', 'Ġhere', 'ars', 'å¸Į', 'çļĦæĿ', 'å°Ŀ', 'ee', 'ier', 'Ġec', 'ically', 'ering', 'å¿µ', 'ĠDe', 'Ġneg', 'å»ºçŃĳ', 'Ġservices', 'Ġable', 'imes', 'Ġoptions', 'çĽ¸åħ³', 'Ġsub', 'Ġdecision', 'ĠCertainly', 'Ġåľ¨', 'æ¢', 'Ġservice', '):', 'å¸¦æĿ¥', 'Ġchild', 'è§£éĩĬ', 'irt', 'çĨ', 'ä¸įä»ħ', 'æĿ¾', 'ç§¯æŀģ', 'ron', 'åı¤', 'çłĶç©¶', 'ç²ī', 'hor', 'Ġprofess', 'çļĦéĹ®é¢ĺ', 'Ġopportunities', 'åİĨåı²', 'Ġdef', 'ĠAm', 'Ġgr', 'aur', 'å±Ĥ', 'çŃĸ', 'Ġpopular', 'æ´ģ', 'åıĳçİ°', 'Ġpoem', 'èµĽ', 'Ġob', 'Ġdon', 'Ġsound', 'Ġtransportation', 'ious', 'åı¦', 'Ġrole', 'Ġfiel', 'ç§ĳåŃ¦', 'èĢģ', 'reen', 'æľīæķĪ', 'Ġcor', 'Ġfeedback', 'Ġtechnologies', 'äº¤éĢļ', 'Ġadapt', \"'re\", 'ervation', 'Ġcommunities', 'çİ°ä»£', 'Ġlook', 'Ġfac', 'çĶµå½±', 'Ġcollect', 'å¾ĹåĪ°', 'hips', 'Ġavail', 'eren', 'ä¸Ģèµ·', 'çīĽ', 'Ġposs', 'Ġweather', 'Ġefforts', '¿Ģ', 'æĹħ', 'oh', 'Ġcollabor', 'æĭ¥', 'æĪĲåĬŁ', 'èİ·å¾Ĺ', 'å±ħ', 'Ġtre', 'Ġsources', 'Ġstudy', 'Ġprograms', 'éĻĲ', 'Ġtips', 'Ġmarket', 'ally', 'å®³', 'wards', 'æ£Ģ', 'ä¸Ģç¯ĩ', 'rior', 'Ġtop', 'Ġend', 'åĭ', 'Ġlarge', 'iciency', 'Ġdec', 'å®ļçļĦ', 'icient', 'è¿ĩç¨ĭä¸Ń', 'lications', 'ç¼º', 'Ġtour', 'Ġtogether', 'äººå·¥', 'Ġtools', 'æĸ¯', 'æ°ĳ', 'æĬĬ', 'ä¹ĭéĹ´çļĦ', 'çī¹çĤ¹', 'Ġbel', 'ditionally', 'åĪ©çĶ¨', 'è¾¹', 'éĻį', 'ĠIf', 'é¢Ŀ', 'åįı', 'å¾Ģ', 'lish', 'è¯ī', 'ins', 'å¥¶', 'Ġeconom', 'Ġinvest', 'ĠDo', 'tain', 'åĩºçİ°', 'çļĦå½±åĵį', 'aterial', 'Ġsure', 'Ġpass', 'çĶ»', 'è´£', 'ç»ĵæŀĦ', 'æķħ', 'æĥħæĦŁ', 'æ¿Ģ', 'ellig', 'ä¼Ĺ', 'æ¯Ķè¾ĥ', 'tern', 'Ġoutcomes', 'up', 'Ġbeaut', 'read', 'çĶŁæĪĲ', 'æķ°åŃĹ', 'Ġdem', 'ires', 'åı¯ä»¥éĢļè¿ĩ', 'æĸ°çļĦ', 'Ġdeep', 'å¨', 'çĭĹ', 'åħ³æ³¨', 'çĶŁåĳ½', 'ä¼łç»Ł', 'Ġstay', 'æŃĮ', 'åħ³éĶ®', 'Ġplace', 'ä¸»é¢ĺ', 'å¾Īå¤ļ', 'èĪĴ', 'Ġprofessional', 'yle', 'æĽ²', '19', 'Ġessay', 'Ġgive', 'ç³ĸ', 'Ġonly', 'æŁĲ', 'Ġphys', 'å¯¹è¯Ŀ', 'Ġcontro', 'Ġamount', 'cept', 'ization', 'ç¼ĸåĨĻ', 'åıĹåĪ°', 'Ġalways', 'æ¯Ķå¦Ĥ', 'Ġprivacy', 'au', '________', 'Ġresponsible', '()', 'çŃīçŃī', 'Ġmaterial', 'Ġonline', 'é¼', 'æĶ¿', 'åĽĽ', 'Ġenjoy', 'åľŁ', 'Ġsafety', 'Ġtw', 'Ġcommunication', 'ä¸½', 'æĺ¾', 'olution', 'erg', 'įä½ľ', 'Ġuser', 'Ġemotional', 'time', 'é¾', 'Ġsecurity', 'Ġsense', 'elines', 'åĬ±', 'çī©è´¨', 'ura', 'Ġshare', 'Ġanalyzing', 'ital', 'é±', 'irtual', 'Ġvisit', 'bers', 'Ġcour', 'Ġproble', 'è®¾å¤ĩ', 'atch', 'land', 'é±¼', 'æĪĳä»¬éľĢè¦ģ', 'ç¨³', 'ibility', 'Ġefficiency', 'å£°', 'èĴ', 'æľºåĻ¨', 'Ġclear', 'åĪ¶å®ļ', 'izing', 'Ġconditions', 'lusion', 'Ġlow', 'Ġlim', 'hers', 'Ġrisks', 'ç¿»', 'Ġlet', 'åĴĸ', 'å¿ĥçĲĨ', 'è¿ľ', 'print', 'Ġchanges', 'Ġmeas', 'Ġimproving', 'Ġcrit', '50', 'å¸ĮæľĽ', 'Ġaud', 'åįĹ', 'æĹłæ³ķ', 'Ġnegative', 'é¡¹çĽ®', 'und', 'ats', 'Ġcompanies', 'æī¾åĪ°', 'Ġcontribut', 'æŃ£ç¡®', 'é»Ħ', 'å±ŀ', 'Ġunderstanding', 'Ġmult', 'Ġclo', 'å¾ģ', 'Ġprior', 'rim', 'äººå·¥æĻºèĥ½', 'Ġvariety', 'Ġtaking', 'åĤ', 'aster', 'ody', 'Ġ{', 'çļĦéĩįè¦ģ', 'Ġfore', 'èµĦæºĲ', 'è¦ģæ±Ĥ', 'Ġfeatures', 'èįī', 'me', 'èĮĥ', 'Ġoper', 'çº§', 'é²ľ', 'æĬĢå·§', 'ĳæĪĺ', 'ç±»åŀĭ', 'æĿ¿', 'è½¯', 'ew', 'Ġrestaur', 'Ġwithout', 'ructure', 'çļĦæĺ¯', 'çı', 'Ġlist', 'urate', 'Ġbook', 'äº²', 'åºĹ', 'ä¹Łæĺ¯', 'ä»»ä½ķ', 'Ġcam', 'ĠBe', 'Ġgovern', 'Ġbehavior', 'è®Ńç»ĥ', 'Ġfamily', 'æĿĤ', 'Ġcity', 'Ġapproach', 'Ġaccurate', 'Ġsom', 'Ġel', 'èĪŀ', 'èŀ', 'åŁºæľ¬', 'Ġdise', 'Ġencoura', 'ĠWhat', 'åĥ', 'è¯¦', '¦Ĥ', 'å·¥åħ·', 'åķ¡', 'Ġstill', 'chool', 'æĦŁåĪ°', 'çĶŁçī©', 'åĴĸåķ¡', 'åĩĨå¤ĩ', 'Ġwaste', 'Ġevents', 'æķĻèĤ²', 'Ġ8', 'Ġmust', 'ied', 'asing', 'å½¢æĪĲ', 'Ġproducts', 'åħ¸', 'è®²', 'fter', 'å·®', 'less', 'Ġcro', 'Ġfinan', 'åıįåºĶ', 'åĪĽéĢł', 'Ġguidelines', 'åĪ¤', 'ä½ľåĵģ', 'è¡¨ç¤º', 'å¼Ĥ', 'Ġknown', 'Ġtest', 'è¯¯', 'ope', 'Ġusers', 'AI', 'å¾·', 'new', 'è¿½', 'iques', 'æ¨¡åŀĭ', 'åĬĽåĴĮ', 'Ġhistory', 'ĠAl', 'æĬķèµĦ', 'å°Ŀè¯ķ', 'ank', 'Ġhome', 'éĴŁ', 'ä¸°', 'èĪĴéĢĤ', 'Ġincrease', 'Ġhab', 'åĪ»', 'è¾ĵåĩº', 'Ġleading', 'Ġ7', 'é£İéĻ©', 'Ġperformance', 'Ġhapp', 'åŃ£', 'Ġstand', 'ty', 'ç¦ı', 'Ġcustomers', 'åįİ', 'Ġbelie', 'Ġcompany', 'å½ķ', 'é£Łçī©', 'ĠUn', 'Ġsumm', 'rent', 'ĠCon', 'éĢĤéĩı', 'anced', 'Ġi', 'Ġlight', 'Ġanalysis', 'å°Ĭ', 'ĠUse', 'ouse', 'ted', 'Ġcharact', 'Ġ#', 'to', 'ç»ľ', 'ä¸įæĺ¯', 'Ġdeveloping', 'åŁ¹', 'Ġstrategies', 'Ġmight', 'çŁŃ', 'çļĦæİ', 'Ġfirst', 'èĥĮ', 'çĮ«', 'Ġincludes', 'åĽŃ', 'Ġdiagn', 'Ġgrowth', 'ä¸ĵä¸ļ', 'Ġdoes', '12', 'ç»¿', 'Ġkeep', 'è¯¦ç»Ĩ', 'åĥı', 'åıĳçĶŁ', 'fact', 'åı¯ä»¥åľ¨', 'ç«Ļ', 'æĭī', 'æµİ', 'Ġchatbots', 'Ġbreak', 'è¡¡', 'çŁ³', 'æĮģç»Ń', 'life', 'Ġ10', 'æ´Ĺ', 'ĠAdditionally', 'å£«', 'ember', 'Ġgoals', 'å¾®', 'Ġview', 'Â·', 'ove', 'åŁºç¡', 'Ġoptimize', 'Ġtem', 'Ġdown', 'åŁºç¡Ģ', 'è¶ħ', 'ercis', 'Ġless', 'ees', 'æĿĥ', 'Ġkey', 'Ġworks', 'è®¨', 'åı¥åŃĲ', 'Ġrobot', 'uss', 'åħ¨çĲĥ', 'ç»ıæµİ', 'æīįèĥ½', 'egr', 'ä»ĸä»¬çļĦ', 'äºĶ', 'èµ·æĿ¥', 'çĵ', 'Ġfactors', 'Ġcultural', 'æľ¨', 'Ġworking', 'ä¼¼', 'èĲ½', 'éĢŁåº¦', 'ä½ı', 'Ġeffects', 'å©ļ', 'br', 'åİħ', 'rain', '\")', 'åŃ¦çĶŁ', '\",', 'Ġpar', 'atform', 'Ġensuring', 'çĶ±äºİ', 'Ġmuch', 'Ġwords', 'Ġmar', 'ç»ıéªĮ', 'ä¸ºäºĨ', 'åĲĪä½ľ', 'ven', 'Ġ/', 'Ġfinancial', 'work', 'ories', 'æ²»', 'Ġtechniques', 'æĭ¥æľī', 'rap', 'å°Ķ', 'Ġest', 'Ġavailable', 'Ġlit', 'æ¹', 'Ġefficient', 'els', 'over', 'Ġland', 'Ġarea', 'Ġintellig', 'Ġpref', 'ature', 'çŁ¥è¯Ĩ', 'æĵįä½ľ', 'å¾ħ', 'igate', 'çļĦæĶ', 'Ġmean', 'bo', 'Ġcontrol', 'éĩĩçĶ¨', 'ricult', 'Ġprogramm', 'Ġtowards', 'thing', 'ä¸įè¦ģ', 'Ġthough', 'å½©', 'Ġcertain', 'Ġwild', 'ä»Ĭ', 'Ġconservation', 'çŁ¥éģĵ', 'Ġreally', 'çļĦåľ°', 'io', 'é¥°', 'Ġful', 'çİ¯ä¿Ŀ', 'Ġexplore', 'çļĦæ¸', 'Ġdiverse', 'åĬłå¼º', 'çļ®', 'Ġemotions', 'Ġavoid', \"'ll\", 'çļĦæī', 'åį¡', 'Ġplatform', 'ances', 'Ġsitu', 'ä»ĺ', 'ä½įç½®', 'oring', 'çĽĲ', 'ä¸ĩ', 'Ġdev', 'nov', 'ash', 'Ġtwo', 'å®ł', 'bon', 'èµ°', 'åĪĹè¡¨', 'Ġcy', 'èįĲ', 'ĠSome', 'Ġexplain', 'Ġaware', 'ç¤¾äº¤', 'day', 'åıĮ', 'æ²ŁéĢļ', 'æ°§', 'å¼Ģåıĳ', 'åħ¬åı¸çļĦ', 'Ġair', 'åĩ»', 'aring', 'éĥ½æĺ¯', 'Ġlevels', 'ods', 'Ġsteps', 'Ġcap', 'æ´ŀ', 'é©¬', 'Ġreturn', 'Ġmet', 'çĶŁæĢģ', 'ä¸°å¯Į', 'æŁĵ', 'æīĢä»¥', 'é¡»', 'Ġer', 'Ġfra', '30', 'èĵ', 'âĢĶ', 'Ġå½ĵ', 'ah', 'ä¿ĥ', 'Ġlikely', 'ĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'åĪĿ', 'Ġcreating', 'Ġfarm', 'Ġbal', 'Ġlives', 'å®ĥçļĦ', 'Ġability', 'ä¸ĬçļĦ', 'Ġsentence', 'åĤ¨', 'Ġrout', 'Ġprovides', 'Ġagain', 'å®łçī©', 'éĢĲ', 'Ġyears', 'èŀį', 'Ġphysical', 'Python', 'ĠEx', 'iting', 'è°ĥæķ´', 'ç½ĳç»ľ', 'æħ¢', 'ç©ºéĹ´', 'åĽ°', 'è±Ĩ', 'æĽ´å¤ļçļĦ', 'ĠAr', 'Ġmaintain', 'å®ŀéĻħ', 'Ġtravel', 'Ġsat', 'pro', 'çĶµåŃĲ', 'æ±½', 'ex', 'åģĩ', 'æĲŃ', 'éļıçĿĢ', 'è¿ĺæľī', 'ç¤¼', 'ale', 'Ġconsum', 'ĊĠ', 'ncy', 'Ġquestions', 'fort', 'making', 'Ġdesc', '15', 'Ġinvolves', 'Ġstress', 'åŃĹç¬¦', 'here', 'Ġimpacts', 'Ġexercis', 'åĿļ', 'ledge', 'ç§ĳæĬĢ', 'oci', 'Ġeffectively', 'æ¶Īè´¹', 'Ġconclusion', 'éĺħ', 'Ġstre', 'issions', 'æ·»', 'It', 'éĿĻ', 'Ġvirtual', 'è¡£', 'Ġachieve', 'ource', 'è¿ŀ', 'acks', 'è¡¨æł¼', 'Ġimportance', 'èĩªæĪĳ', 'These', 'num', 'çļĦæł', 'Ġrelationships', 'Ġworkers', 'gical', 'orpor', 'erson', 'åĳ¢', 'nds', 'æİ¨èįĲ', 'ohn', 'å¿ħé¡»', 'å®¹æĺĵ', 'ĠGo', 'Ġtell', 'ĠRes', 'onom', 'Ġbec', 'æ³Ľ', 'pos', 'Ġmove', 'Ġstory', 'æŃ¢', 'Ġpriorit', 'Ġindustries', 'èľ', 'Ġpossible', 'ĠMan', 'Ġexpress', 'abilities', 'Ġintegr', 'ä»£è¡¨', 'Ġrespond', 'åĪĨéĴŁ', 'æľºä¼ļ', 'Ġthings', 'äº¤æµģ', 'Ġmeth', 'urther', 'Ġwide', 'èĳĹ', 'æĪĳçļĦ', 'ĸçķ¥', 'ides', 'ething', 'ĠWhile', 'pan', 'çŃĸçķ¥', 'Ġcent', 'Ġplease', 'ology', 'uracy', 'å¾ª', 'ward', 'nce', 'Ġthen', 'çªģ', 'å¥ĩ', 'Ġblo', 'ai', 'æŀĹ', 'ç®Ĺæ³ķ', 'ç»¼', 'Ġprint', 'aces', 'lu', 'ªæĸ½', 'pre', 'çļĦæĦı', 'Ġsol', 'Ġoverall', 'hold', 'Ġes', 'çļĦä¸Ģ', 'éģĩ', 'Ġpopul', 'å°ıè¯´', 'æ³¢', 'åįģ', 'ä¹Łåı¯ä»¥', 'é£Łåĵģ', 'Ġcontent', 'å°Ħ', 'Ġrequires', 'æ£ĢæŁ¥', 'ĊĠĠĠĠĠĠĠĠĠĠĠ', 'Ġgroups', 'Ġfair', 'Ġbl', 'å®ŀéªĮ', 'æĮīçħ§', 'osp', 'str', 'ä¸įèĥ½', 'Ġharm', 'Ġprodu', 'çļĦæĬĢ', 'çĩ', 'tle', 'Ġanimals', 'è§Ĵèī²', 'lev', 'æ¸Ĳ', 'å¤įæĿĤ', 'Ġdepend', 'æĮĳæĪĺ', 'åĮħåĲ«', 'Ġhelps', 'Ġopen', 'Ġnet', 'ĠĠĠĠĠ', 'Ġstrong', 'Ġjour', 'å¹¿æ³Ľ', 'æķ´ä¸ª', 'Ġelect', 'Ġresponse', 'åįķè¯į', 'æľĭ', 'Ġ<', 'åĮĸåŃ¦', 'éĴĪ', 'Ġquick', 'ually', 'Ġsomething', 'Ġtrack', 'åº¦åĴĮ', 'erences', 'æłĳ', 'Ġaccuracy', 'Ġexc', 'é£ŀ', 'Ġfield', 'å¯»æī¾', 'éħ¸', 'Ġhope', 'çĳ', 'Ġinnov', 'ç»ª', 'alk', 'Ġtypes', 'Ġdid', 'åĬª', 'Ġcall', 'è¯Ĺ', 'Ġearly', 'ĠOne', 'app', 'Ġcommon', 'æľĢç»Ī', 'Ġcheck', 'Ġsym', 'çĤĴ', 'æĬĢèĥ½', 'Ġenh', 'Ġagricult', 'Ġimm', 'ç»ĩ', 'æ»¡è¶³', 'Ġschool', 'bal', 'Ġfollowing', 'based', 'Ġwebs', 'Ġculture', 'ĠCom', 'way', 'ä¸Ģå®ļ', 'åķĨåĵģ', 'ude', 'çļĦåıĳå±ķ', 'çĶŁäº§', 'osystem', 'Ġplant', 'åı¶', 'åĲĥ', 'ä»ĸçļĦ', 'der', 'è¯¢', 'å®¶åħ·', 'Ġfree', 'ç§»', 'æİĮ', 'Ġbody', 'Ġpresent', 'Ġparticularly', 'Ġchildren', 'Ġstudent', ').', 'çī¹å¾ģ', 'èĶ', 'éĺħè¯»', 'æķĪçİĩ', 'Ġprogram', 'éħ±', 'åıĺå¾Ĺ', 'ix', 'Ġcome', 'çļĦæ²', 'ĠTe', 'ĠTo', 'åħ±åĲĮ', 'Ġemployees', 'è¯´æĺİ', 'Ġheart', 'Ġmot', 'æľĭåıĭ', 'eric', 'è¯ĳ', 'Ġcurrent', 'æĪĲæľ¬', 'Ġtoo', 'çİ©å®¶', 'åĪĽæĸ°', 'Ġecosystem', 'å¸¸è§ģ', 'ä¸ĢæŃ¥', 'Ġpres', 'Ġmulti', 'åĳĬè¯ī', 'ä¸¥', 'Ġmit', 'Ġaction', 'çĨŁ', 'Ġhabit', 'åı£æĦŁ', 'ç®±', 'Ġuses', 'å¢ŀå¼º', 'ç»Ļåĩº', 'Ġ9', 'Ġdep', 'Ġeconomic', 'æĢ§çļĦ', '18', 'åĨ°', 'Ġhelped', 'åĲ¸å¼ķ', 'çİĭ', 'Ġdiagnos', 'åł', 'èģĶç³»', 'ç¾¤', 'ç»ĥä¹ł', 'æĪĲéķ¿', 'Ġpoint', 'å®ļæľŁ', 'åĳ¼', 'èį¯', 'æĿ¯', 'æ¤Ĵ', 'æķĪæŀľ', 'Ġspecial', 'æ··', 'åĩłä¸ª', 'ause', 'éĨ', 'æ¯ĶèµĽ', 'è·Ŀ', 'What', 'Ġtimes', 'icles', 'Ġ*', 'ç´§', 'å¦Ĥæŀľä½ł', 'çĭ¬çī¹', 'çģµ', 'ç¨İ', 'Ġcarbon', 'Ġbias', 'åĬ©äºİ', 'Ġconst', 'èĩªçĶ±', 'æĿ¥è¯´', 'å°±æĺ¯', 'åį°', 'Ġmeet', 'è§ĦåĪĴ', 'çļĦç¾', 'èĲ¥åħ»', 'ators', 'ç¨³å®ļ', 'ode', 'çħ®', 'Ġassoci', 'å¿Ĺ', 'è¡ĮæĺŁ', 'æĿİ', 'Ġreview', 'åĩĢ', 'ĠRo', 'Ġknowledge', 'ä»¥ä¾¿', 'æµĭè¯ķ', 'åĲĪéĢĤ', 'sc', 'å½¢å¼ı', 'Ġfriends', 'Ġnature', 'Ġcritical', 'æ´ĭ', 'Ġafter', 'erve', 'Ġrece', 'çļĦæŃ', 'æ±½è½¦', 'çķĮ', 'Ġloss', 'Ġapplications', 'å¤ļç§į', 'éĶħ', 'ä¸²', 'Ġinsp', '---', 'ĠSh', 'Ġvol', 'lut', 'oks', 'sequ', 'Ġbir', 'åĲĪçĲĨ', 'Ġnecess', 'æĪĳæĥ³', 'çŃīæĸ¹éĿ¢', 'é¼ĵ', 'Ġsoft', 'Ġlive', 'å°ıæĺİ', 'ĠInd', 'Ġbring', 'æĺ¯æĮĩ', 'Ġsoil', 'ilar', 'ä¸ľ', 'æĿ¡ä»¶', 'Ġtri', 'äº®', 'Ġmom', 'æı¡', 'ä¼°', 'ŀäºī', 'çĽĳ', 'èĤ¤', 'è´¢åĬ¡', 'æ·»åĬł', 'é¥®é£Ł', 'Ġallowing', 'åºķ', 'Ġright', 'Ġexpert', 'Ġsupp', 'Ġinit', 'çļĦæµ', 'arget', 'Ġexpect', 'Ġ19', 'Ġmeasures', 'olutions', 'just', 'arc', 'å°ļ', 'Ġpractice', 'æľīåĬ©äºİ', 'å¤§éĩı', \"',\", 'iment', 'Ġcontinue', 'Ġdiscuss', '100', 'éļľ', 'çļĦæĦŁ', 'Ġreflect', 'itation', 'åį«', 'äºĨä¸Ģ', 'ney', 'ĠLe', 'ised', 'è¶ĭ', 'äºĨä¸Ģä¸ª', 'Ġincreasing', 'çļĦæĮ', 'Ġstru', 'æĢ»ç»ĵ', 'ely', 'å®ĩ', 'Ġauthor', 'è¡¨éĿ¢', 'Ġx', 'æķħäºĭ', 'emic', 'Ġrepresent', 'ger', 'Ġincreased', 'ones', 'ains', 'Ġtrained', 'Ġfish', 'Ġstate', 'åĨ·', 'çĶŁéķ¿', 'Ġrenew', 'ording', 'åĮĹ', 'æİªæĸ½', 'å¹³è¡¡', 'Ġsuccessful', 'ä¸ĭéĿ¢', 'Ġactivity', 'èĮ¶', 'éĢĤåºĶ', 'èĦĳ', 'æİ¢ç´¢', 'ffic', 'ç»ĦæĪĲ', 'atives', 'äºļ', 'Ġscen', 'æ²Ļ', 'gress', 'ä½¿å¾Ĺ', 'æī¿', 'Ġdiscrim', 'Ġassistants', 'Ġexist', 'çķĻ', 'Ġspace', 'æľĢè¿ĳ', 'Ġideas', 'éĩĩåıĸ', 'light', 'æ³¨éĩį', 'çļĦæĹ¶éĹ´', 'è¿İ', 'Ġcomb', 'éĢĤå½ĵ', 'Ġyourself', 'rite', 'ason', 'åĮĢ', 'åı¯ä»¥ä½¿çĶ¨', 'åħħæ»¡', 'Ġvalues', 'æ½', 'Ġbiases', 'ä¿ĥè¿Ľ', 'åľºæĻ¯', 'ross', 'åį³åı¯', 'Ġcru', 'Ġnumber', 'Ġtype', 'rast', 'åĩĨç¡®', 'This', 'Ġpast', 'çģ¯', 'å®ļä¹ī', 'Ġsolutions', 'Ġter', 'ä¿Ŀè¯ģ', 'èĶ¬', 'å¹¸', 'åī§', 'åħ´è¶£', 'åª', 'ention', 'avor', 'Ġscient', 'åĬªåĬĽ', 'Ġproviders', 'Ġpolicies', 'alu', 'ĠIm', 'Ġallows', 'Ġintelligence', 'çļĦæĸ¹æ³ķ', 'è¿Ļæĺ¯', 'Ġ`', 'Ġemissions', 'Ġå°Ĩ', 'Ġmeaning', 'Ġstyle', 'åİŁåĽł', 'Ġstrugg', 'çļĦç¾İ', 'iful', 'dition', 'éĥ½æľī', 'ç©ºæ°Ķ', 'å®ĥä»¬çļĦ', 'ä¼ĺåĮĸ', 'Ġinflu', 'åŁºäºİ', 'Ġdetails', 'Ġtransparency', 'Ġmess', 'ĠCl', 'Ġgame', 'pri', 'è¶ĭåĬ¿', 'å½Ĵ', 'ç¿»è¯ĳ', 'æķ£', 'By', 'éŃ', 'ĠAmeric', 'Ġproduction', 'Ġincorpor', 'æĻļ', 'Ġinvolve', 'Ġhot', 'æĻ®', 'by', 'Ġflow', 'Ġemerg', 'åº§', 'Ġidea', 'åİĭåĬĽ', 'éĿĴ', 'oms', 'èģĮä¸ļ', 'Ġreport', 'Ġpap', 'Ġtherap', 'Ġsal', 'åıĤä¸İ', 'æĸĩåŃ¦', 'æĲŃéħį', 'oot', '),', 'Ġcr', 'Ġprocesses', 'gin', 'å¹³åı°', 'å¯Ł', 'Ġpromoting', 'æļĸ', 'akehold', 'ç»§', 'iver', 'æ¦Ĥ', 'Ġmodels', 'Ġdra', 'èĸ', 'Ġgroup', 'è¶³å¤Ł', 'Ġgreen', 'Ġhealthy', 'Ġcomfort', 'Ġadditional', 'ä¸Ģæ¬¡', 'é¤Ĳåİħ', 'Ġmaterials', 'Ġmanage', 'çļĦæ¯', 'ä¼¤', 'åıĬæĹ¶', 'Ġglo', 'Ġstat', 'å¿«éĢŁ', 'Ġmonitoring', 'aily', 'rand', 'oice', 'resh', 'ç»Ħç»ĩ', 'Ġunder', 'Ġnecessary', 'Ġhelpful', 'ĠCol', 'é»ĳæ´ŀ', 'åģļåĩº', 'Ġcourse', 'Ġmat', 'Ġleg', 'Ġface', 'ä»¤', 'èī¯å¥½çļĦ', 'ock', 'åĮ»çĸĹ', 'çĽĸ', 'idence', 'Ġassociated', 'Ġprogress', 'åľĨ', 'Ġeveryone', 'ç¼ĵ', 'ĠEng', 'word', 'èĵĿ', 'å¤©æ°Ķ', 'Ġactions', 'ems', 'ĠPl', 'å®Ļ', 'ush', 'é¡¾', 'Ġcosts', 'ator', 'ç©¿', 'Ġamounts', 'èĶ¬èıľ', '..', 'Ġmanner', 'Ġconsequ', 'æ°ĶåĢĻ', 'Ġinsights', 'being', 'atory', 'ener', 'lex', 'Ġmeans', 'Ġcollaboration', 'Ġperspect', 'orm', 'priate', 'å°Ĭéĩį', 'Ġtarget', 'è®°å½ķ', 'åĢĴ', 'Ġrenewable', 'æĦ¿', 'èĥ½æºĲ', 'Ġinput', 'å®ĩå®Ļ', 'ape', 'Ġadjust', 'eries', 'Ġdire', 'ä¾Ŀ', 'ustr', 'fect', 'Ġbeautiful', 'Ġdue', 'reci', 'çĮ®', 'èĥĮæĻ¯', 'èĤ¡', 'Ġdam', 'ik', 'Ġadvanced', 'çĽ¸å¯¹', 'åĲįç§°', 'Ġshort', 'Ġobject', 'è¿ĻéĩĮ', 'éĢłæĪĲ', 'èĲ¥éĶĢ', 'çļĦæĥħæĦŁ', 'ç¥¨', 'Ġcountries', 'ining', 'istic', 'Ġplans', 'è´£ä»»', 'Ġstakehold', 'the', 'Ġassess', 'æĢĿèĢĥ', 'ech', 'æĪĲåĳĺ', '21', 'Ġdaily', 'Ġcomput', 'çļĦæĥħåĨµ', 'æıĲåĩº', 'ĠâĢľ', 'åªĴ', 'ä¸Ńå¿ĥ', 'ished', 'ĠSe', 'onomous', 'ern', 'ç»´æĬ¤', 'ames', 'Ġprioritize', 'çº¸', 'èĤ¥', 'Ġtemper', 'æ¸ħæ´ģ', 'use', 'æ±¡', 'Ġminim', 'æĺ¯åľ¨', 'å¤§å°ı', 'åĵªäºĽ', 'Ġappreci', 'reng', 'Ġregulations', 'ĠZ', 'éĶĻè¯¯', 'rans', 'èĢĮä¸Ķ', 'èĪ¬', 'èĳ±', 'èĨ', 'æ°´å¹³', 'è´Ńçī©', 'åŃĹç¬¦ä¸²', 'å¯¹æĸ¹', 'Ġhim', 'Ġconsequences', 'å·´', 'é¼ĵåĬ±', 'Ġfil', 'äººåĳĺ', 'è·Ŀç¦»', 'ĠWhen', 'çļĦæ°´', 'çī©çĲĨ', 'åĲĮæĹ¶ä¹Ł', 'åľ¨è¿Ļä¸ª', 'åħ¶æ¬¡', ',\"', 'æ¶²', 'çĶ·', 'ival', 'åı¯ä»¥è®©', 'æĥ¯', 'Ġadvance', 'Ġveh', 'å¦ĤæŀľæĤ¨', 'Ġestab', 'ript', 'ç«¯', 'ä¸įä¼ļ', 'Ġtransparent', 'æķ°éĩı', 'çĽĺ', 'Ġspeak', 'Ġpark', 'Ġstakeholders', 'éº', 'Ġevent', 'çļĦæķ°æį®', 'èĩªåĬ¨', 'ç»ĨèĬĤ', 'è¯Ħä¼°', 'æ¶¦', 'Ġpreferences', 'Ġveget', 'æįŁ', 'equ', 'Ġgl', 'Ġpain', 'ogra', 'Ġtraffic', 'Ġoce', 'ä¹ĺ', 'ext', 'âĢĿï¼Į', 'Ġanother', 'å¤ļå°ĳ', 'Ġagainst', 'ç»ıåİĨ', 'è®¡ç®Ĺæľº', 'èĢĲ', 'è½¯ä»¶', 'ĠPre', 'Ġplants', 'çĽ¸äºĴ', 'é¢ĳ', '\\\\_', 'Ġsame', 'rug', 'Ġvalu', 'Ġocc', 'çļĦç¤', 'Ġsustainability', 'ĠShe', 'de', 'ote', 'Ġdig', 'NA', 'Ġcrucial', 'æī§', 'å±Ģ', 'æĭŁ', 'æĭĮ', 'Ġnon', 'Ġengaging', 'Ġintern', 'LP', 'æ¸©åº¦', 'æł¸', 'æĬ¥åĳĬ', 'æĿ¥è¶Ĭ', 'hood', 'ä¸īä¸ª', 'å¦Ĥä¸ĭ', 'çī©ä½ĵ', 'force', 'Ġneeded', 'Ġimages', 'Ġbuilding', 'icious', 'ĠæĪĳ', 'è¶ĬæĿ¥è¶Ĭ', 'æĶ¾åħ¥', 'go', 'éĻįä½İ', 'å½ĵåľ°', 'æ¶Īè´¹èĢħ', 'ç£', 'iversity', 'é¢Ħç®Ĺ', 'icle', 'æ··åĲĪ', 'Ġparticip', 'Ġdishes', 'Ġthroughout', 'Ġwithin', 'åı³', 'é«ĺçļĦ', 'Ġphot', 'Ġtrust', 'æĦıè¯Ĩ', 'ä»¥ç¡®ä¿Ŀ', 'çĬ¶æĢģ', 'Ġautomation', '11', 'Ġpost', 'æīĭæľº', 'works', 'éĢı', 'åºĵ', 'Ġwind', 'Ġ==', 'Ġprocessing', 'èĮĥåĽ´', 'æĦıä¹ī', 'è¿½æ±Ĥ', 'Ã©', 'å¾Ħ', 'éĿł', 'ä¸ĸ', 'èĻ½', 'ç«ŀäºī', 'Ġappropriate', 'æĽ´å¥½çļĦ', 'Ġcharacter', 'cl', 'ç§ĺ', 'itude', 'Ġteac', 'leep', 'ĠDevelop', 'ince', 'å·¦', 'ground', 'è¡Įä¸ļ', 'éĴĪå¯¹', 'å¿ħè¦ģ', 'Ġdeterm', '----------------', 'Ġstreng', 'do', 'Ġchallenging', 'ork', 'Ġanx', 'èī²çļĦ', 'Ġhard', 'æĺİç¡®', 'åĪĨäº«', 'æĶ¹åıĺ', 'ä½³', 'åıªæľī', 'å±ķç¤º', 'Ġcamp', 'çº³', 'aj', 'etic', 'ument', 'ä½łåı¯ä»¥', 'Ġpollut', 'Ġhig', 'pping', 'ead', 'çĦ¶èĢĮ', 'ç¬¬äºĮ', 'é¸Ł', 'çī©åĵģ', 'ä¸¾', 'Ġencourage', 'pecial', 'Ġacross', 'elves', 'äºĭä»¶', 'cle', 'æ©', 'åªĴä½ĵ', 'ners', 'Ġcal', 'èĻ½çĦ¶', 'åĽº', 'ä¹łæĥ¯', 'Ġsafe', 'èĥ½éĩı', 'istics', 'ä¹ĭåīį', 'Ġissue', 'å¤ļä¸ª', 'åĨ³çŃĸ', 'è¾¾åĪ°', 'æĹ©', 'ä¸įåı¯', 'ä¸ĢçĽ´', 'å·¨', 'æĦŁè°¢', 'ĠNew', 'ä¸Ģæ®µ', 'Ġmachines', 'å°Ĩåħ¶', 'ç»§ç»Ń', 'Ġword', 'çī¹åĪ«', 'Ġagriculture', 'æĢİ', 'éĢĲæ¸Ĳ', 'éĵ¾', 'è¯¾', 'Ġkind', 'å¢Ļ', 'è°¢è°¢', 'Ġalgorithm', 'è£ħé¥°', 'Ġalong', 'Ġeasy', 'äºĳ', 'è§£åĨ³æĸ¹æ¡Ī', 'Ġawareness', \"'ve\", 'æĸ¹åĲĳ', 'Ġnever', 'Ġquickly', 'Ġrespect', 'çļĦæĻ', 'Ġamong', 'Ġaccountability', 'Ġlaw', 'ening', 'Ġdefin', 'Ġsurround', 'éĵģ', 'Ġpowerful', 'An', 'Ġcause', 'æ¥', 'æİĮæı¡', 'è¿ĺæĺ¯', 'Ġcreative', 'è¡Ģ', 'Ġlocated', 'unning', 'åľ°åĮº', 'éĿ¢ç§¯', 'éĽ¨', 'Ġnear', 'Ġiniti', 'ression', 'ä¸ĭæĿ¥', '25', 'é©¶', '¾çĹħ', 'ables', 'æľīè¶£', 'å¾ªçİ¯', 'çŃĶæ¡Ī', 'çł´', 'ication', 'éĻ¢', 'æ²»çĸĹ', 'Ġaddition', 'äºĭæĥħ', 'Ġbecause', 'åıĪ', 'èĤĮ', 'çºª', 'side', 'æĭħ', 'æ¹¿', 'åįĬ', 'é¡º', 'ĠAnd', 'Ġrestaurant', 'Ġvide', 'Ġproblem', 'azing', 'Ġmembers', 'Ġnut', 'Ġcou', 'æµª', 'Ġè¿Ļ', 'Ġhelping', 'ĠIs', 'æıĲåįĩ', 'ĠĠĠĠĠĠ', 'Ġsho', 'Ġrelev', 'Ġarg', 'Ġbalance', 'illed', 'æĺ¯ä»Ģä¹Ī', 'åĬĽéĩı', 'ired', 'å¤ľ', 'åı¯æĮģç»Ń', 'Ġperfect', '**', 'ification', 'æ¶ī', 'Ġwildlife', 'ane', 'Ġrelated', 'å®¤åĨħ', 'åºľ', 'äº«åıĹ', 'ours', 'è·ĳ', 'åķĨä¸ļ', 'aching', 'Ġsun', 'Ġrecognition', 'elt', 'Ġorder', 'å¹³åĿĩ', 'ging', 'ä¸´', 'çĤ¼', 'Ġgoing', 'åĳ¼åĲ¸', 'Ġsoftware', 'Ġremot', 'èĳĹåĲį', 'å¹¸ç¦ı', 'Ġenhance', 'èĻļ', 'Ġnow', 'Ġthreat', 'Ġdest', 'åĿĩåĮĢ', 'Ġacad', 'åºĶå¯¹', 'çľĭåĪ°', 'cast', 'è¾Ĩ', 'ificial', 'Ġvery', 'ook', 'åĮºåŁŁ', '¹ģ', 'æĪ¿éĹ´', 'æıĲä¾ĽäºĨ', 'Ġmotiv', 'Ġaccessible', 'åĨ³å®ļ', 'Ġhy', 'å®Ī', 'Ġflo', 'ug', 'Ġinformed', 'åĵģè´¨', 'çļĦçŁ', 'aves', 'arr', 'ĠWith', 'let', 'è§ĤçĤ¹', 'enge', 'è¡ĮåĬ¨', 'friend', 'ç³ķ', 'Ġfurther', 'ĠEns', 'ç§ģ', 'Ġado', 'Ġclean', 'çĽ¸åºĶ', 'Ġfre', 'pecially', 'èĹ', 'Ġcapt', 'çļĦçľ', 'Ġsomeone', 'Ġcell', 'æĶ¾åľ¨', 'æ¬¢è¿İ', 'ĠâĢ', 'Ġdevices', 'çļĦæĸ¹å¼ı', 'Ġjobs', 'augh', 'not', 'æľīäºĽ', 'åħ¬åħ±', 'gest', 'çļĦçĶŁæ´»', 'çľ¼', 'çļĦä¿¡æģ¯', 'ĠCons', 'æİĴåºı', 'Ġbenefit', 'rect', 'å¤ı', 'unte', 'ç¬¦åĲĪ', 'ä¸Ģä½į', 'åĨħéĥ¨', 'Ġlooking', 'ding', 'æĬĺ', 'è¾ĳ', 'è¿Ļä¸ªéĹ®é¢ĺ', 'Ġespecially', 'çľł', 'âĢĿãĢĤ', 'å¥ı', 'ray', 'è¿ĺåı¯ä»¥', 'åĪĽä½ľ', 'coming', 'Ġmultiple', 'éļĲ', 'æ³¡', 'æłĩåĩĨ', 'Ġmil', 'éľĢè¦ģæ³¨æĦı', 'Ġanxiety', 'æĶ¹è¿Ľ', 'å±ĭ', 'æ±¡æŁĵ', 'ç¼ĸç¨ĭ', 'è´¹çĶ¨', 'Ġevalu', 'imately', 'Ġliter', 'ograph', 'Ġsearch', '16', 'enced', 'Ġmethods', 'çĥĪ', 'æ¨¡å¼ı', 'çĬ¶åĨµ', 'æĶ¹åĸĦ', 'å¤ļæł·', 'cer', 'å¥ĸ', 'Ġsatis', 'Ġwebsite', 'åĬŀ', 'åģ¥èº«', 'Ġglobal', 'Ġask', 'Ġplatforms', 'Ġdiseases', 'çİ°è±¡', 'tics', 'æ±ģ', 'åĪ¤æĸŃ', 'Ġconvers', 'Ġrelationship', 'è®¾ç½®', 'æ³ķå¾ĭ', 'Ġmindful', 'é¢Ħæµĭ', 'overy', 'åģľ', 'çĶµè§Ĩ', 'è§ĦåĪĻ', 'aken', 'Ġimplementing', 'ising', 'åıĤåĬł', 'æĥħç»ª', 'Ġprovided', 'æ·±åħ¥', 'Ġprogrammed', 'Ġrelevant', 'çļĦçĥ', 'çĸ¾çĹħ', 'åĮ»çĶŁ', 'åĪĽå»º', 'Ġgenerate', 'æĶ¶åħ¥', 'ä¼ĳ', 'izes', 'Ġtransform', 'éģµ', 'astic', 'åĳĪ', 'æ¯ıä¸ªäºº', 'è¿Ķ', 'iet', 'Ġvoice', 'éĢĶ', 'æĶ¾æĿ¾', 'åį´', 'èĥľ', 'Ġstructure', 'æĹ¶å°ļ', 'ĠQ', 'Ġelse', 'duc', 'Ġemp', 'èģļ', 'è´§', 'aches', 'ç§Ģ', 'anks', 'Ġnight', 'Ġprofessionals', 'Ġbas', 'è´µ', 'ec', 'Ġdiversity', 'ites', 'dr', 'åĽ°éļ¾', 'ĥåľ', 'åŀĥåľ', 'åŀĥåľ¾', 'Ġdrug', 'ç¢³', 'Ġname', 'åĮĸçļĦ', 'aid', 'æľĢå¤§', 'æĳĦ', 'ç®ĢåįķçļĦ', 'Ġwarm', 'Ġdone', 'Ġfunction', 'asc', 'å¼ºè°ĥ', 'Ġdemand', 'Ġvisual', 'Ġupd', 'æŃ£åľ¨', 'Ġsimilar', 'éĢĴ', 'æ¯Ľ', 'éĶ»', 'ently', 'Ġvaluable', 'Ġdisaster', 'ä¸ĢèĪ¬', 'æ´²', 'ĠReg', 'Ġdiscrimination', 'åĨĻä¸Ģç¯ĩ', 'Ġgovernment', 'Ġå¥½çļĦ', '500', 'lying', 'Ġprev', 'Ġprepare', 'Ġproblems', 'è·³', 'Ġprom', 'åĨ²', 'å®īè£ħ', 'éĶ»çĤ¼', 'æµĵ', 'è¹', 'åºĶçĶ¨ç¨ĭåºı', 'ng', 'Ġcompet', 'åĪĨåĪ«', 'ological', 'å®¡', 'Ġtransl', 'Ġdirect', 'åīĤ', 'Ġsuggestions', 'Ġpaper', 'Ġrecognize', 'ton', 'Ġmitigate', 'è®¨è®º', 'äºĴåĬ¨', 'ĠEar', 'Ġamazing', 'cre', 'é¦Ī', 'Ġinvolved', 'face', 'æľīåħ³', '))', 'Ġexce', 'Ġproductivity', 'èŃ', 'é¦Ĩ', 'Ġsounds', 'Ġidentifying', '],', 'é¾Ļ', 'Ġfit', 'Ġcontribute', 'ths', 'friendly', 'ele', 'ified', 'iveness', 'itely', 'ĠX', 'Ġled', 'åĿı', 'Ġhistor', 'Ġdat', 'Ġjourney', 'Ġ}', 'Ġselect', 'æ¼«', 'Ġconduct', 'è¿Ľä¸ĢæŃ¥', 'ç»ĻæĪĳ', 'Ġlif', 'è£ħä¿®', 'ä¸ºä»Ģä¹Ī', 'äº¬', 'Ġnav', 'Ġwhole', 'ç¹ģ', 'åĨľ', 'æĶ»', 'Ġbreat', 'Ġmiss', 'é¾Ħ', 'tt', 'sw', 'Ġbar', 'è¯·éĹ®', 'èģĶç½ĳ', 'Ġattract', 'æĤ¨åı¯ä»¥', 'One', 'åħħåĪĨ', 'ring', 'Ġå½ĵçĦ¶', 'ream', 'Ġevol', 'Ġsn', 'ĠEm', 'mosp', 'Ġchoose', 'view', 'Ġarr', 'Ġsleep', 'ended', 'æŀ¶', 'Ġvehicles', 'Ġfresh', 'Ġorganization', 'è¿Ļæ®µ', 'æ±¤', 'ĠInt', 'Ġcontext', 'åı¦å¤ĸ', 'Ġocean', 'æĦŁåıĹ', 'Ġpollution', 'urb', 'æī§è¡Į', 'ersonal', 'ĠHealth', 'ä¼ĺçĤ¹', 'Ġattention', 'æľīçĿĢ', 'é£ŁæĿĲ', 'Ġerr', 'çļĦæĿ¥', 'çļĦçĪ', 'èŃ¦', 'è·Ł', 'æĹħè¡Į', 'èĴľ', 'çļĦæĢĿ', 'Ġchatbot', 'çļĦéľĢæ±Ĥ', 'çķ¥', 'Ġfeeling', 'Ġimplemented', 'ç¤¾åĮº', 'çļĦå»ºè®®', 'æĲħ', 'éĹ»', 'åıįé¦Ī', 'çĽ´æİ¥', 'æĺ¥', 'itable', 'æĪĳä¼ļ', 'åį±', 'èī¯å¥½', 'Ġliving', 'åıĺéĩı', 'ĠBut', 'Ġcomplete', 'Ġtrends', 'Ġmakes', 'ä»Ĭå¤©', 'Ġdistribut', 'Ġcommit', 'Ġatmosp', 'ä¼´', 'Ġsensors', 'Ġsw', 'æĹłè®º', 'omen', 'æĶ¿åºľ', 'Ġchallenge', 'Ġturn', 'çĲĨè®º', 'par', 'Ġwrite', 'ç»ıåħ¸', 'emember', 'é¥Ń', 'æĸ¹ä¾¿', 'Ġcu', 'Ġvalue', 'Ġfund', 'pose', 'è°ĥæŁ¥', 'çĿ¡', 'Ġcommunicate', 'Ġdisease', 'Ġresearc', 'Ġlack', 'arning', 'ĠPark', 'çĦ¦', 'é«ĺåº¦', 'Ġrather', 'å®£', 'çĪ¶', 'éĺ¶', 'è®¢', 'çĥ§', 'Ġhigher', 'Ġsummary', 'ĠAut', 'çļĦæ³', 'Ġele', 'isms', 'Ġreli', 'ä¹Łä¼ļ', 'fra', 'åĳĬè¯īæĪĳ', 'æĬ½', 'Ġsituations', 'Ġmarine', 'æĥ³è¦ģ', 'inci', 'inal', 'Ġgain', 'Ġdifference', 'æľºåĻ¨äºº', 'æµģç¨ĭ', 'ĠChat', 'ç½ĳç«Ļ', 'æľ«', 'Ġcolor', 'Ġaspect', 'ç½Ĺ', 'ĠEduc', 'Ġdeploy', 'Ġbeauty', 'æĤ£', 'ruction', 'itut', 'æĿŁ', 'è®©æĪĳä»¬', 'éķ¿åº¦', 'ules', 'æ¶īåıĬ', 'Ġdigital', 'Ġexisting', 'ĠOr', '\\\\_\\\\_', 'Ġbackground', 'çĹĩ', 'æ¯ıå¤©', 'python', 'Ġfarmers', 'Ġcontinu', '\":', 'Ġgiven', 'å°ıæĹ¶', 'Ġmoment', '200', 'John', 'éĿ¢å¯¹', 'Ġintro', 'Ġtherapy', 'è¿ĶåĽŀ', 'å¹¶åľ¨', 'Ġz', 'Ġafford', 'ä¸Ŀ', 'å®½', 'ĠÃ', 'ĠNational', 'èĥ¡', 'Ġexercise', 'æĲħæĭĮ', 'æĶ¯ä»ĺ', 'éĺ³åħī', 'è¯ļ', 'Ġsect', 'ĠSu', 'å¢ŀéķ¿', 'ç¾İä¸½', 'Ġwa', 'ä»¥ä¸ĭæĺ¯ä¸ĢäºĽ', 'èĽĭç³ķ', 'Ġill', 'æ¸ħæĻ', 'etry', 'æ¢¦', 'ç¾İåĽ½', 'ä»į', 'oney', 'Ġecosystems', 'æĮĩå¯¼', 'def', '99', 'æŁĶ', 'pped', 'Ġlimit', 'çİī', 'Ġacademic', 'Ġrestaurants', 'Ġhead', 'ä¿¡ä»»', 'asters', 'å²ģ', 'akers', '14', 'As', 'æł¡', 'é«ĺæķĪ', 'phas', 'yn', 'ç¨ĭåº¦', 'è¾£', 'ä¸ĬéĿ¢', 'å®¶å±ħ', 'term', 'ç¾İé£Ł', 'Ġovers', 'å®ĺ', 'Ġindic', 'ĠYour', 'St', 'å½¢è±¡', 'è´¡', 'åºĬ', 'ĠSc', 'agra', 'çľŁæŃ£', 'oint', 'ids', 'arent', 'éĵ¶', 'èģĬ', 'Ġregular', 'ä¼ĺç§Ģ', 'Ġcolle', 'çĸĳ', 'Ġsubject', 'Ġgreater', 'Ġstore', 'åŁ¹è®Ń', 'Ġimag', 'Ġansw', 'ä½Ļ', 'Ġspot', 'åĪĨåŃĲ', 'Ġaudience', 'pet', 'Ġvers', 'Ġtrail', 'åĭĩ', 'erous', 'Ġguidance', 'Ġspeech', 'åĵ²', 'æĺ¯çĶ±', 'è´¡çĮ®', 'åĲĪéĢĤçļĦ', 'è®¾æĸ½', 'ä»ĸäºº', 'ensive', 'åĢ¾', 'aling', 'Ġprojects', 'å³', 'Ġtakes', 'ç»©', 'That', 'Ġbro', 'ived', 'Ġ&', 'åĿĲ', 'placement', 'è¿ŀæİ¥', 'çļĦç¤¾', 'ĠTra', 'Ġrelax', 'ufact', 'éģį', 'Ġsurv', 'åı£åĳ³', 'Ġcreativity', 'of', 'å¨ģ', 'çļĦçł', 'Ġbreath', 'Ġplaces', 'Ġdescrib', 'èĭ±è¯Ń', 'Ġdamage', 'oration', 'ä¸ºæĤ¨', 'ift', 'Ġcase', 'å¹´é¾Ħ', 'Ġpress', 'çĶľ', 'éĩİ', 'æĹħæ¸¸', 'Ġtaken', 'ined', 'Ġconcept', 'æĴŃ', 'Ġinteresting', 'è·µ', 'Ġsea', '60', 'Ġfoot', 'ĠName', 'Ġresearchers', 'éĢģ', 'Ġwee', ');', 'çļĦåħ³éĶ®', 'ä¼½', 'elebr', 'å¡ĳ', 'We', 'ç»ıå¸¸', 'Ġpopulations', 'åħ¬å¼ı', 'orn', 'çĩĥ', 'äººçĶŁ', '17', 'æİ¥åıĹ', 'Ġlocation', 'Ġinequ', 'Ġintervent', 'Ġinterested', 'Ġdefinitely', 'Ġassistance', 'è¿Ļä¸Ģ', 'åĲĪåĲĮ', 'ä¼ĺåĬ¿', 'çļĦå·¥ä½ľ', 'Ġ12', 'Ġmov', 'åģı', 'åŃĺåĤ¨', 'usive', 'æĹı', 'ï¼īï¼Į', 'Ġgas', 'Ġinterests', 'æ¸ħæĻ°', 'Ġgard', 'çĸ«', 'Ġsay', 'å¤«', 'ges', 'èĲ¨', 'ä¸ļåĬ¡', 'ä¸ªæĢ§', 'åĲ¯', 'Ġengagement', 'Ġbig', 'éľĢè¦ģèĢĥèĻĳ', 'Ġprinci', 'åĳ¨åĽ´', 'Ġopportunity', 'çģ¾', 'èĹı', 'rel', 'ç¼ºçĤ¹', 'Ġhappy', 'åĴĮåħ¶ä»ĸ', 'ava', 'Ġestablish', 'é¸¡èĽĭ', 'iking', 'ĠTrans', 'rastructure', 'forest', 'èİ·åıĸ', 'èĦļ', 'inally', 'èµı', 'Ġdelicious', 'Ġresults', 'è§Ĥå¯Ł', 'å®ŀè·µ', 'Ġlast', 'Ġpolit', 'æĢ§èĥ½', 'For', 'bi', 'çĽ¸ä¿¡', 'ffee', 'Ġphr', 'Ġforest', 'elling', 'æµģè¡Į', 'atic', 'å¤§å®¶', 'ĠInst', 'æķ°åŃ¦', 'æī©', 'å®Įåħ¨', 'å¼ķèµ·', 'ese', 'è½¬æį¢', 'Ġaffected', 'Ġrobotics', 'ç»¼ä¸Ĭ', 'Ġprop', 'è®©äºº', 'æ²³', 'ä¸ŃæľĢ', 'Ġautonomous', 'Ġhaving', 'Ġtrip', 'ury', 'Ġbiased', 'Ġconsiderations', 'Ġparticular', 'åįł', 'æİ¨å¹¿', 'Ġinitiatives', 'ials', 'åĳ³éģĵ', 'Ġtreatments', 'Ġemphas', 'çĭ¬çī¹çļĦ', 'Ġlay', 'æĶ¿çŃĸ', 'æĢİä¹Ī', 'ronic', 'play', 'Ġcook', 'è¿Ľåħ¥', 'è½®', 'Ġvolunte', 'Ġrain', 'ĠMon', 'Ġconsumption', 'èĽĭçĻ½', 'ĠSoc', 'å£¤', 'Ġroutine', 'Ġimproved', 'To', 'äººçī©', 'è¯»èĢħ', 'Ġgoal', 'å¹¿åĳĬ', 'éķ¿æľŁ', 'Ġey', 'He', 'Ġoutdo', 'Ġcuis', 'Ġaway', 'Ġbooks', 'Ġtopic', 'å¤§åĪ©', 'house', 'Ġones', 'ç§Ł', \"':\", 'æĪ¿å±ĭ', 'ç§»åĬ¨', 'Ġdisasters', 'ests', 'illing', 'ç»¿èī²', 'åĵ²åŃ¦', 'æĪĲåĪĨ', 'Ġoccur', 'ľä¼½', 'åľŁå£¤', 'çļĦä¸»è¦ģ', 'çİ°å®ŀ', 'Ġanimal', 'é¢Ĩå¯¼', 'Ġviews', 'éĤ®', 'æ°§åĮĸ', 'athy', 'éģĵå¾·', 'ç¤¾äº¤åªĴä½ĵ', 'ĠPersonal', 'ĽåĽ´', 'Ġpurch', 'Ġcountry', 'Ġremind', 'å¯¸', 'Ġrights', 'çļĦçİ¯å¢ĥ', 'ĠPr', 'Ġline', 'ibr', 'é©¾', 'Ġmaj', 'Ġovercome', 'Ġnext', 'æīĢè¿°', 'è§Ħå®ļ', 'Ġinteractions', 'Ġconflic', 'Ġwhy', 'ç³»åĪĹ', 'å°¼', 'ibly', 'çīĽå¥¶', 'Ġresponses', 'ses', 'åŃ¦ä¼ļ', 'bol', 'Ġstandards', 'ulner', 'å¯¹è¯ĿåĨħå®¹', 'lished', 'çļĦæĢ§', 'çĶŁæĢģç³»ç»Ł', 'ann', 'æĥħåĨµä¸ĭ', 'å¯»æ±Ĥ', 'Ġhold', 'den', 'åįĥ', 'Ġmention', 'ĠMany', 'çĽ´åĪ°', 'éģĹ', 'hel', 'Ġbelieve', 'aries', 'æľīä¸Ģä¸ª', '13', 'Ġatmosphere', 'Ġmor', 'æĹ¥æľŁ', 'ä¹ħ', 'ä½łå¥½', 'Ġaddressing', 'ĠâĢĵ', 'çļĦåľ°æĸ¹', 'ming', 'Ġcannot', 'Ġmanufact', 'Ġpie', 'icing', 'Ġstudies', 'ç¾İåĳ³', 'ĠAmerican', 'ĠNLP', 'Ġaccording', 'mselves', 'èĦĤ', 'èĩªä¿¡', 'æīĢéľĢ', 'Ġthemselves', 'Ġremote', 'åŁ¹åħ»', 'å®īæİĴ', 'ä½łéľĢè¦ģ', 'Ġregard', 'iring', 'è¯ĨåĪ«', 'Ġarticle', 'æģĴ', 'æĢ»çļĦæĿ¥', 'Ġalign', 'æ±ł', 'tenance', 'faction', 'åĬ¨ä½ľ', 'çļĦç©', 'ç¼©', 'æĢ¥', 'Ġ100', 'Ġtesting', 'åŃĹæ¯į', 'å¹´è½»', 'åĪ¶éĢł', 'Ġswe', 'å°º', 'hens', 'æ°´æŀľ', 'Ġinfrastructure', 'èī²å½©', 'æĢ»çļĦæĿ¥è¯´', 'æľīä»Ģä¹Ī', 'text', 'è½¦è¾Ĩ', 'Ġpay', 'rop', 'ĊĠĠ', 'Ġcaused', 'Ġcorrect', 'Ġì', 'èĥŀ', 'ĠMed', 'ç²¾ç¥ŀ', 'æ°ĶåĢĻåıĺåĮĸ', 'ĠRed', 'äºĴèģĶç½ĳ', 'Ġengage', 'åĪĨä¸º', 'ĠData', 'Ġfull', 'enc', 'éĩįæĸ°', 'æŃ£ç¡®çļĦ', 'çļĦæ°Ķ', 'åıĮæĸ¹', 'Ġcomes', 'åı¤ä»£', 'æŁĲäºĽ', 'åĳĪçİ°', 'Ġtoday', 'aged', 'æĪĳåı¯ä»¥', 'æĹ¥å¸¸', 'æ»ĳ', 'Ġclin', 'Ġ\\\\', 'Ġobs', 'Ġartificial', 'Ġexcell', 'çļĦç¬', 'alls', 'Ġproduce', 'ĠDes', 'oss', 'è¹Ī', 'Ġdraw', 'Ġletter', 'Ġadvice', 'Ġhighly', 'çĬ¯', 'ç»¼ä¸ĬæīĢè¿°', 'æ»¡æĦı', 'Ġprinciples', 'èĮĦ', 'Ġfeelings', 'çļĦæ´', 'Ġhom', 'Ġfail', 'Ġcrop', 'å§ľ', 'Ġquestion', 'Ġdisabilities', 'èĪŀè¹Ī', 'Ġimplications', 'ral', 'Ġsing', '40', 'Ġfamil', 'Ġgovernments', 'Ġrecord', 'å½¢çĬ¶', 'Ġbegin', 'ises', 'çļĦæĥ³', 'achine', 'è°±', 'Ġvulner', 'Ġproper', 'Ġoversight', 'è´ŁéĿ¢', 'Ġemail', 'Ġnews', 'Ġexploring', 'Ġfavor', 'æ¥¼', 'å®ľ', 'Ġunivers', 'å·®å¼Ĥ', 'ï¼īãĢĤ', 'è§£åĨ³éĹ®é¢ĺ', 'Ġfamous', 'gn', 'Ġmessage', 'atitude', 'Ġcra', 'Ġcover', 'æ·±åĪ»', 'åı¯ä»¥éĢīæĭ©', 'çĶŁæ´»ä¸Ń', 'ç§įç±»', 'Ġsmart', 'onstr', 'vey', 'çĶ²', 'Ġregularly', 'ĠSm', 'æĦŁè§ī', 'Ġthought', 'Ġexh', 'cure', 'ç»ĺ', 'è®¤è¯Ĩ', 'Ġold', 'æĦī', 'ç§°ä¸º', 'Ġfields', 'Ġconsist', 'ãģ', 'ç»Ĩèĥŀ', 'Ġhours', '80', 'alking', 'è§īå¾Ĺ', 'ç»Ŀ', 'ä½łä»¬', 'ĠEnglish', 'Ġsignificantly', 'Ġsource', 'Ġant', 'Ġeducational', 'Ġtask', 'Ġhandle', 'æĲľ', 'ĠSp', 'Ġcalled', 'Ġterms', 'æ²ī', 'Ġwin', 'duction', 'Ġmodern', 'Ġcuisine', 'å¥Ĺ', 'è§¦', 'olutely', 'ç«¥', 'pite', 'Ġfelt', 'Ġcompre', 'Ġwond', 'è¿Ĳè¡Į', 'Ġresil', 'çĽ¸ä¼¼', 'éĩĳèŀį', 'çĪ±æĥħ', 'ç¬Ķ', 'èĪª', 'è°Ī', 'åĬĽçļĦ', 'æľīæīĢ', 'æ½ľ', 'ulate', 'Ġdetection', 'å®£ä¼ł', 'Ġmatter', 'éĩıåŃĲ', 'Write', 'ç»ĵåĲĪ', 'ç»ıè¿ĩ', 'Ġdevelopers', 'èª', 'Ġ---', 'äººéĻħ', 'çŃ¾', 'ï¼ļâĢľ', 'Ġinnovative', 'ãĢĤâĢĿ', 'å½¼', 'é¥¼', 'è¿ĩåº¦', 'Ġplanet', 'åħ°', 'å¸ģ', 'æķ¬', 'Ġlegal', 'Ġlot', 'æĪĲä¸ºäºĨ', 'iate', 'Ġmis', 'åģĩè®¾', 'çļĦæĸĩç«ł', 'ĠCompan', 'Ġdoc', 'Ġcareful', 'Ġever', 'æĪĳä»¬å°Ĩ', 'ä¾ĭåŃĲ', 'ä¹³', 'ä½ľèĢħ', 'åĲ§', 'æļ´', 'Ġremember', 'çĽ®çļĦ', 'Ġput', 'å¸¸è§ģçļĦ', 'Ġfest', 'å»ºè®¾', 'å®ŀçĶ¨', 'Ġactive', 'çªĹ', 'outh', 'åİŁçĲĨ', 'Ġtrying', 'è¿·', 'çĽ¸åĲĮ', 'éħĴåºĹ', 'Another', 'æľĢä½³', 'Ġanalytics', 'Ġperpet', 'ipment', 'Ġå¦Ĥæŀľ', 'è§Ĥä¼Ĺ', 'Ġcelebr', 'Ġheav', 'Ġmeditation', 'å¤§æ°Ķ', 'And', 'ä¸įéĶĻ', 'Ġwhether', 'set', 'Ġdemonstr', 'ä¸Ģæ¬¾', 'æĶ¶éĽĨ', 'éĻĲåĪ¶', 'Ġing', 'Ġrevolution', 'çľģ', 'Ġscience', 'çĽ®åīį', 'Ġthinking', '±ä¹Ĳ', 'è¯¾ç¨ĭ', 'Ġpack', 'Ġimage', 'loc', 'Ġstories', 'uck', 'Ġsatisfaction', 'Ġcollection', 'ho', 'èµŀ', 'éĿ¢ä¸´', 'Ġla', 'Ġsymbol', 'Ġemb', 'Ġhabitats', 'Ġlower', 'Ġcontinues', 'éľĩ', 'åĵĪ', 'ĠTake', 'Ġenvironments', 'Ġthree', 'Ġenc', 'ĠAcc', 'æĦıåĳ³', 'åİ¨', 'chan', 'ĠHum', 'Ġtrue', 'åĪĩæĪĲ', 'sing', 'âĢĶâĢĶ', 'åĩºæĿ¥', 'Ġregion', 'Ġinterpre', 'Ġdiagnosis', 'éŀ', 'Ġdoing', 'Ġrun', 'Ġcoffee', 'Ġmajor', 'Ġmindfulness', 'Ġaffordable', 'çĻ¾', 'Ġdetailed', 'éĿŀå¸¸éĩįè¦ģçļĦ', 'çļĦæ²ŁéĢļ', 'çļĦæķħ', 'åĢĴåħ¥', 'Ġthemes', 'Ġnetwork', 'ï¼īï¼ļ', 'ĠUnited', 'çļĦæĮĩ', 'orts', 'åį«çĶŁ', 'Ġplanning', 'æĥł', 'åīª', 'ĠProv', 'çļĦåºĶçĶ¨', 'Ġperi', 'Ġaccountable', 'çīĻ', 'çļĦçģ', 'Ġchoice', 'ĠComm', 'idents', 'çļĦå®īåħ¨', 'å¹¶ä¸į', 'å¤ªéĺ³ç³»', 'Ġreceive', 'Ġclose', 'çļĦæĹ¶åĢĻ', 'Ġchanging', 'ä»·åĢ¼è§Ĥ', 'Ġperpetu', 'Ġseason', 'Ġmen', 'Ġlearned', 'Ġsituation', 'Ġreplace', 'head', 'è®©æĪĳ', 'åľ¨ä¸Ģèµ·', 'çļĦç©º', 'éľ²', 'Ġenough', 'å±ķçİ°', 'Ġleaders', 'ancing', 'Ġtemperature', 'åı«', 'Ġ30', 'æĦıåĳ³çĿĢ', 'æ±ĩ', 'ĠGovern', 'Ġfocused', 'uro', 'Ġsimple', 'Ġhiking', 'æ¯Ĵ', 'Ġcomprehens', 'äºĪ', 'Ġcreated', 'cond', 'é¡µ', 'ĠWor', 'è¯ģæį®', 'Ġworkplace', 'Ġcharacters', 'çļĦè®¾è®¡', 'Ġmechan', 'ĠDis', 'ç¥ŀç§ĺ', 'å·ŀ', 'ĠOn', '</', 'ç§įæ¤į', 'Ġpath', 'Ġlimited', 'Ġsolar', 'çļĦæı', '22', 'Ġappreciate', 'å¿«ä¹Ĳ', 'æĦŁåıĹåĪ°', 'èĢĹ', 'med', 'icine', 'Ġnote', 'å½ĵåīį', 'æĪĳä»¬åºĶè¯¥', 'Ġseen', 'ä¸ĢåĲį', 'å°½åı¯èĥ½', 'è¿Ĳç®Ĺ', 'è§Ĵåº¦', 'Ġequipment', 'Ġspread', 'è¸', 'è®¿', 'åı¥è¯Ŀ', 'æĮ¥', 'Ġpurpose', 'è¯·ä½ł', 'Your', 'arian', 'ä»ª', 'Ġperspectives', 'åĩºäºĨ', 'å©ļç¤¼', 'Ġexcellent', 'ĠEnsuring', 'Ġreach', 'éĺ¶æ®µ', 'ä¿Ŀéļľ', 'Ġempathy', 'ĠMy', 'çĳľä¼½', 'Ġver', 'abel', 'ĠPredict', 'Ġmaintenance', 'è¯Ħä»·', 'Ġult', 'åĴ¨', 'ox', 'åĴ¨è¯¢', 'Ġshared', 'ina', 'list', 'Ġoutdoor', 'Ġthoughts', 'inating', 'éĴ±', 'Ġframe', 'éĺ¿', 'åĪ©æ¶¦', 'çļĦæİ¨', 'åįļ', 'Ġrecent', 'Ġaltern', 'ared', '==', 'Ġroad', 'äºĭé¡¹', 'ged', 'ynt', 'Ġspend', 'ç½ª', 'åıĸå¾Ĺ', 'é¹', 'li', 'æĹ¶æľŁ', 'ä¸¥éĩį', 'å¿Ĩ', 'å©´', 'æİ¥ä¸ĭæĿ¥', 'ĠEarth', 'ĠChatbots', 'Ġsetting', 'ç¥Ŀ', 'éĶĢåĶ®é¢Ŀ', 'ä¼¦', 'Ġreading', 'æİ¢è®¨', 'aign', 'éŀĭ', 'Ġyoung', 'Ġcareer', 'Ġteachers', 'çļĦè´¨éĩı', 'å±ŀäºİ', 'Ġeasier', 'Ġscientific', 'ç¾İåħĥ', 'Ġspir', 'åĬ³', 'çļĦæĶ¯', 'rist', 'èµĦäº§', 'çĶŁåŃĺ', 'èĩ³å°ĳ', 'å§¿', 'Ġvideo', 'Ġaim', 'å®Ŀå®Ŀ', 'çĪ¶æ¯į', '________________', 'alities', 'Ġbud', 'Ġstreet', 'Ġæĺ¯', 'æĸ¹ç¨ĭ', 'ä¸ĸçºª', 'ches', 'earch', 'æĴ°', 'Ġengine', 'Ġdisplacement', 'ĠRobots', 'ervised', 'é¡¶', 'oud', 'Ġwalk', 'Ġemergency', 'èģĺ', 'nal', 'Ġdatas', 'åĢº', 'åĲİçļĦ', 'å¾Īå¥½', 'Ġmyself', 'çļĦæīĭ', 'Ġusage', 'Ġshown', 'æ®Ĭ', 'Ġtypically', 'uly', 'æĸ°éĹ»', 'æĽ¿', 'Ġorig', 'è½»æĿ¾', 'æĺ¾ç¤º', 'Ġadopt', 'èĤ¡ç¥¨', 'Ġparent', 'aps', 'æĢĿæĥ³', 'Ġmarketing', 'èĻ«', 'éĥ¨éĹ¨', 'çļĦæķĪ', 'Ġcomfortable', 'åŃ¦ä¹łåĴĮ', 'Ġforecast', 'iction', 'Ġgetting', 'Ġtrees', 'aving', 'çļĦåŁºç¡Ģ', 'ready', 'æĸ°é²ľ', 'going', '¹é¥', 'Ġevidence', '¹é¥ª', 'ç§ĭ', 'æľīå¾Īå¤ļ', 'éĿ¢è¯ķ', 'éģĩåĪ°', 'ç»Ļå®ļ', 'irc', 'åı¯ä»¥æł¹æį®', 'é©¾é©¶', 'å·§åħĭ', 'Ġstunning', 'çļĦæ¦Ĥ', 'æ¡Į', 'ĠJohn', 'ulation', 'åıĤèĢĥ', 'Ġflex', 'çĦ¦èĻĳ', 'ymakers', 'Ġforms', 'sh', 'val', 'ĠSo', 'co', 'æİ¨åĬ¨', 'èħ¿', 'çī¹æ®Ĭ', 'Ġenab', 'å°Ĩä¼ļ', 'æĶ¯åĩº', 'åĿļæĮģ', 'çº¢èī²', 'Ġoption', 'Ġstarted', 'ration', 'Ġpoetry', 'Ġport', 'gen', 'èªī', 'Ġdeliv', 'çĶļ', 'éĢ»', 'éĢīé¡¹', 'Ġground', 'å½¼æŃ¤', 'ana', 'çļĦæĹ¥', 'åľ¨çº¿', 'Ġsecure', 'Ġæł¹æį®', 'é¥®æĸĻ', 'Ġgratitude', 'ç¬¬ä¸ī', 'Ġsong', 'Ġpoints', 'Ġalready', 'çļĦçĪ±', 'ĠTechn', 'Ġreality', 'çıŃ', 'Ġsince', 'Ġpopulation', 'yond', 'bor', 'ĠSocial', 'æıĲåıĸ', 'å·¥ç¨ĭ', 'aff', 'äº¤æĺĵ', 'Ġworth', 'å¡«', 'å¨±ä¹Ĳ', 'Ġdog', 'ĠArt', 'ç¡¬', 'æµ·æ´ĭ', 'åĨĴ', 'çīĪ', 'Ġprogramming', 'ĠAss', 'ĠMachine', 'åĢ¼å¾Ĺ', 'è¯·è¾ĵåħ¥', 'å£°éŁ³', 'Ġexercises', 'åħīçº¿', 'æ³ķåĴĮ', 'Ġfeature', 'eff', 'è¿ĽæŃ¥', 'å¥³æĢ§', 'Ġefficiently', 'çļĦæĬĢæľ¯', 'Ġgenetic', 'ä»¤äºº', 'è´¦', 'çļĦäº§åĵģ', 'åİļ', 'åĴĮæĸĩåĮĸ', 'éĻĦ', 'Ġmob', 'ç»¼åĲĪ', 'ters', 'æľīä¸Ģ', 'å¦Ĩ', 'åįĪ', 'Ġoutside', 'Ġpropert', 'éĤ®ä»¶', 'ä¸»ä¹ī', 'Ġpolicy', 'èĩªèº«', 'Ġnavigate', 'Ġsty', 'çĶµèĦĳ', 'Ġabilities', 'Ġfaced', 'çļĦç¼', 'çļĦå°ı', 'èķ', 'Ġtone', 'igation', 'åıĤæķ°', 'èĽĭçĻ½è´¨', 'ä½Ľ', 'çĶļèĩ³', 'Ġskin', 'èĴ¸', 'æĭĽ', 'éŃĶ', 'ashion', 'Ġingred', 'æĹĭ', 'Ġcampaign', 'Ġmount', 'Ġconsid', 'Ġmuse', 'nter', 'water', 'ä¼ļè®®', 'Ġprotection', 'ä¿ĿéĻ©', 'Ġcrops', 'ogle', 'éļıæĹ¶', 'æļĹ', 'ium', 'ä¹ı', 'Ġdiet', 'lies', 'çĶ¨æĿ¥', 'ĠEncoura', 'æĬĹ', 'apan', 'éĺ²æŃ¢', 'Wow', 'çļĦåŁºæľ¬', 'å¹³æĸ¹', 'Ġstep', 'åı¯éĿł', 'è¡¨æĺİ', 'Ġpredictions', 'Ġsympt', 'Ġdiagnoses', 'åħ¬åĽŃ', 'Ġsupply', 'Ġprevious', 'ç»ĦåĲĪ', '.,', 'çļĦè¿ĩç¨ĭ', 'æķı', 'su', 'aris', 'çķħ', 'ocol', 'æĲľç´¢', 'itle', 'éĨĴ', 'é¡¾å®¢', 'éĢ»è¾ĳ', 'éĿŀå¸¸éĩįè¦ģ', 'ĠBi', 'å·¦åı³', 'amm', 'Ġeverything', 'æĺł', 'Ġincred', 'Ġpeace', 'èľľ', 'Ġmuseum', 'çĭ¬ç«ĭ', 'Ġcomprehensive', 'Ġrates', '//', 'Ġrad', 'åĦ¿ç«¥', 'çī¹èī²', 'ĠPredictive', 'å¼ķåĬĽ', 'ler', 'å°¤', 'icro', 'è¡¥', 'Ġdetermine', 'çļĦåĨħå®¹', 'Ġcompl', 'Ġgreenhouse', 'èħĲ', 'Ġhighlight', 'Ġpartners', 'Ġdoct', 'çļĦä½¿çĶ¨', 'æŃĮæĽ²', 'æĮĩåįĹ', 'ĠAf', 'æľºæŀĦ', 'éĢĢ', 'Ġpoems', 'å¿ĥåĴĮ', 'Ġattend', 'çļĦæ¸¸', 'Ġside', 'ales', 'Ġmentioned', 'ĠAbs', 'Ġhistorical', 'Ġleft', 'ä»¥ä¸ĭåĩłä¸ª', 'åıĹæ¬¢è¿İ', 'èıľåĵģ', 'Ġremain', 'æĩ', 'Ġtours', 'łéģĵ', 'Ġerrors', 'æľºåĪ¶', 'æ¦', 'æĤ£èĢħ', 'more', 'Ġexperts', 'çļĦçłĶç©¶', 'ç»ĵæĿŁ', 'Ġwritten', 'çłĶ', 'Ġet', 'input', 'æ°Ķä½ĵ', 'èļ', 'æĥĬ', 'Ġage', 'éĩįå¤į', 'å¼¹', 'åŃ¤', 'Ġsymptoms', 'Ġbelief', \"'d\", 'iol', 'Ġ18', 'åħħè¶³', 'çıį', 'forcement', 'æĸĹ', 'ªèĮĦ', 'Ġ15', 'ä¸Ģä¸ªäºº', 'Ġapplic', 'è´¥', 'ä½įäºİ', 'éĻ¤äºĨ', '=\"', 'ä¸īè§Ĵ', 'æĢĿç»´', 'åį·', 'Ġfru', 'ĠCollabor', 'Ġprim', 'Ġrequired', 'Ġwatch', 'è°ĥåĳ³', 'ç»ĵè®º', 'ony', 'Ġguide', 'Ġmax', 'ĠCould', 'Ġadvent', 'ĠOverall', 'çļĦæĬķ', 'Ġexper', 'åĺ', 'icial', 'oster', 'çļĦé¢ľèī²', 'Ġoperations', 'éĥģ', 'Ġmoney', 'ley', 'cling', 'Ġoil', 'çļ®èĤ¤', 'Ġge', 'Ġbat', 'ĠPh', 'Ġsche', 'Ġelectric', 'vest', 'Ġchain', 'Ġcapabilities', 'ird', 'è¯ģæĺİ', 'æľĢå¥½', 'ivil', 'Ġdepending', 'Ġsave', 'Ġpractical', 'Ġcultures', 'çĽ¸åºĶçļĦ', 'sy', 'çļĦç²', 'Ġbehind', 'æĹ¶éĹ´åĴĮ', 'å¹ħ', 'ĠAg', 'Ġeffectiveness', 'Ad', 'ĠOf', 'Ġanything', 'å·§åħĭåĬĽ', 'Ġmist', 'Ġlanguages', 'ĠMake', 'å«', 'æ£®', 'ĠCont', 'ĠAbsolutely', 'Ġinvestment', 'mat', 'çļĦæķħäºĭ', 'æ¬§', 'Ġspeed', 'çļĦæ¸©', 'Ġcities', 'åĨĻä½ľ', 'Thanks', 'Ġded', 'åĪĨéħį', 'Ġdark', 'Ġsupporting', 'å¹ķ', 'ĠKe', 'éĽ¶', 'Ġsharing', 'Ġhouse', 'è®¤çŁ¥', 'Ġsurrounding', 'Ġreduced', 'Ġfu', 'Ġstor', 'Ġabs', 'Tom', 'cent', 'ĠEducation', 'Ġthr', 'ott', 'ĠThat', 'Ġhear', 'ung', 'Ġbeyond', 'ĠCo', 'room', 'è¯ĹæŃĮ', 'reme', 'Ġlittle', 'Ġgames', 'ä¹ĭåĲİ', 'éĥ½ä¼ļ', 'è¯ŃéŁ³', 'ç¬ĳ', 'çī¹å®ļ', 'ç¬¬ä¸Ģ', 'Ġdepression', 'Ġinnovation', 'ĠFr', 'Ġcomputer', 'can', 'å³°', 'ç¼ĸåĨĻä¸Ģä¸ª', 'Ġinternational', 'Ġcancer', 'åŃ¦èĢħ', 'Ġdiscover', 'het', 'Ġcompos', 'Ġrecy', 'Ġ200', 'åĲ«æľī', 'çĹĽ', 'ç¼ĵè§£', 'Ġfrequ', 'çĶ³', 'ĠMar', 'çļĦéĢīæĭ©', 'Ġunt', 'Ġregions', 'Ġopin', 'ĠGovernments', 'æ¶Ĥ', 'åĨħå¿ĥ', 'ä¸ĬæľĢ', 'ä»įçĦ¶', 'lier', 'æ³³', 'äºĴçĽ¸', 'ĠStud', 'azon', 'Ġarch', 'Ġchem', 'çļĦèĥ½åĬĽ', 'çļĦä¸Ģä¸ª', 'Ġap', 'Ġred', 'Ġwomen', 'Ġprote', 'Ġfinding', 'å§»', 'éĢĤå½ĵçļĦ', 'Ġforward', 'å¯¹è±¡', 'Ġwait', 'Ġconsidered', 'dule', 'backs', 'Ġclinical', 'åħ·å¤ĩ', 'éº¦', 'Ġongoing', 'åĨĽ', 'Ġfar', 'åĴĮè°', 'XXX', 'Ġpolitical', 'Ġcamer', 'çļĦè¡Įä¸º', 'æĦıå¤§åĪ©', 'Ġapps', 'åĩıè½»', 'Ġreaders', 'å©ļå§»', 'æ°¸', 'ores', 'åħ¨éĿ¢', 'ĠAfric', 'Ġfavorite', 'Ġmill', 'Ġdang', 'ĠStates', 'åĢŁ', 'å¯¿', 'Ġlat', 'è¿ĩåİ»', 'Ġtruly', 'åĽŀçŃĶéĹ®é¢ĺ', 'Ġcogn', 'ä»°', 'ĠJapan', 'izz', 'çļĦæĿĲ', 'xx', 'é¢ĺçĽ®', 'ription', 'éĤ£äºĽ', 'Ġbudget', 'Ġvast', 'éļĲç§ģ', 'Ġpolicymakers', 'è¿ĺéľĢè¦ģ', 'å¹¶æıĲä¾Ľ', 'Ġsweet', 'Ġgeneral', 'æ»¤', 'Ġbirds', 'Ġplastic', 'Ċĉ', 'åĪº', 'mental', 'Ġinclusive', 'Ġtopics', 'Ġslow', 'ä½łèĥ½', 'è¶³å¤ŁçļĦ', 'è§Ĩè§ī', 'ww', 'Ġä½¿çĶ¨', 'æī¹', 'æ¦Ĥå¿µ', 'é£ŁçĶ¨', 'èĢ³', 'cks', 'Ġfraud', 'Ġingredients', 'Ġfasc', 'åĮĹäº¬', 'Ġfr', 'Ġmanufacturing', 'Ġä½ľä¸º', 'Ġbeach', 'é¡¿', 'erious', 'å¤ĸè§Ĥ', 'é¢Ħéĺ²', 'æĿ¥èĩª', 'èĤĮèĤī', 'Ġdays', 'Ġassign', 'Ġadvant', 'Ġteams', 'é¢Ĺ', 'nown', 'ĠPo', '}{', 'Ġminut', 'itions', 'Ġeasily', 'ĠBl', 'name', 'åŃ¦æł¡', 'Ġresponsibility', 'åıĳæĮ¥', 'Ġsensitive', 'çŃīäºİ', 'cious', 'Ġsou', 'å±ı', 'Ġrich', 'å½ĵçĦ¶', 'man', 'Ġinterpret', '24', 'Ġshows', 'èģĮåľº', 'Ġfall', 'è½½', 'ä¸°å¯ĮçļĦ', \"('\", 'ä¿®æĶ¹', 'æĽ´æį¢', 'Al', 'åı¯èĥ½æĺ¯', 'Ġrate', 'Ġprotecting', 'fit', 'Ġ50', 'Ġmovement', 'è§Ī', 'Ġemployee', 'Ġdisord', 'åĪĽæĦı', 'äº§åĵģçļĦ', 'æľĿ', 'ĊĠĠĠĠĠĠĠĠĠĠĠĠĠĠĠ', 'Ġpred', 'Ġoffering', 'åįģåĪĨ', 'èĢĮä¸įæĺ¯', 'Thank', 'æĽ¾', 'Ġelements', 'ç²Ĵ', 'Ġcourses', 'Ġintegrated', 'ĠCar', 'agraph', 'åŁºåĽł', 'Ġinstead', 'èĦ±', 'åı¦ä¸Ģä¸ª', 'å¯Ĩçłģ', 'Ġallowed', 'éĿ¢åĮħ', 'çķªèĮĦ', 'åĴĮåıĳå±ķ', 'å°ģ', 'Ġconnection', 'åľ¨ä¸Ģä¸ª', 'Ġuseful', 'è¯Ńåı¥', 'åĪĨå¸ĥ', 'è¡¨æ¼Ķ', 'æľīæĹ¶', 'çļĦæĹħ', 'çļĦæĢ»', 'Ġfashion', 'èĭ¦', 'è¦ģæ³¨æĦı', 'çĶŁç´ł', 'Ġnutri', 'èĩªè¡Į', 'çļĦçĭ', 'çĲĨè§£åĴĮ', 'Ġcat', 'æľºåĻ¨åŃ¦ä¹ł', 'Ġexhib', 'åĴĮæľįåĬ¡', 'frac', 'epend', 'Ġimpacted', 'Ġut', 'æķ°ç»Ħ', 'ĠWorld', 'Ġanswer', 'erse', 'éª¨', 'Ġartists', 'åŃ©åŃĲçļĦ', 'ä»Ķ', 'çĻ»', 'ĠAre', 'Ġcool', 'Ġcognitive', 'åĲĦä¸ª', 'like', 'å©´åĦ¿', 'åĪĹåĩº', 'å¹»', 'ront', 'å®¶éķ¿', 'ç¼ºä¹ı', 'Ġcyber', 'ilt', 'Ġcapture', 'åĹ', 'åľ¨äºİ', 'Ġthreats', 'åĴĮç¤¾ä¼ļ', 'Ġcells', 'æ¸ħåįķ', 'ĠVis', 'æİī', 'Ġhol', 'åŃĲçļĦ', 'Ch', 'èĿ', 'Ġsaid', 'Ġdream', 'unch', 'une', 'ĠDon', 'å®¶äºº', 'ç±į', 'æĦŁåĴĮ', 'Ġexperienced', 'çļĦéĩįè¦ģæĢ§', 'å¼ĥ', 'ump', 'éĺĲ', 'Ġhabitat', 'è¢ĭ', 'Ġjo', 'ç®Ģæ´ģ', 'Ġbur', 'Ġvisitors', 'éĽħ', 'çļĦçŁ¥', 'Ġentire', 'è®²è¿°', 'äºĨä¸ĢäºĽ', 'åįıä½ľ', 'ĠBus', 'å°¾', 'çļĦæķĻ', 'olog', 'Ġsigns', 'Ġspeaker', 'çļĦéŁ³ä¹Ĳ', 'Ġnovel', 'å±ħæ°ĳ', 'çļĦåıĺåĮĸ', 'å°½éĩı', 'Ġspirit', 'å®Įç¾İ', 'è´·', 'å¿ħè¦ģçļĦ', 'ief', 'ç¤ºä¾ĭ', 'Ġdiv', 'æķ´æķ°', 'Ġeconomy', 'Ġethically', 'éĻĪ', 'Ġschools', 'Ġnetworks']\n"
     ]
    }
   ],
   "source": [
    "vocab=tokenizer.get_vocab()\n",
    "print(sorted(vocab,key=lambda x:vocab[x]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0977649b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MiniMind模型参数: 25.83 M(illion)\n",
      "trainable params: 10,240 || all params: 25,840,128 || trainable%: 0.0396\n"
     ]
    }
   ],
   "source": [
    "from peft import PromptEmbedding, PromptTuningConfig,get_peft_model\n",
    "import torch.nn as nn\n",
    "model, tokenizer = init_model(args)\n",
    "peft_config = PromptTuningConfig(\n",
    "    task_type=\"CAUSAL_LM\",\n",
    "    num_virtual_tokens=20,\n",
    "    prompt_tuning_init=\"TEXT\",\n",
    "    num_transformer_submodules=1,\n",
    "    token_dim=512,\n",
    "    prompt_tuning_init_text=\"Classify the passage:\",\n",
    "    tokenizer_name_or_path='./model'\n",
    ")\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()  # 应显示 ~15K 可训练参数\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7cf7e4d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.7488, -0.0540,  0.1875,  ..., -4.4258,  0.8613, -0.9082],\n",
      "         [-0.4753,  0.7772,  0.0536,  ..., -0.0755,  1.1915, -0.8722],\n",
      "         [ 0.3381,  2.9204,  1.4160,  ..., -1.0023,  1.0548, -0.8825],\n",
      "         ...,\n",
      "         [ 0.8371,  1.6740, -1.3006,  ..., -0.7171, -3.4437, -0.4352],\n",
      "         [-0.3711,  1.4281,  0.0099,  ..., -0.6652, -2.6028, -0.7404],\n",
      "         [-0.6661,  0.9061, -0.4236,  ..., -2.7374, -0.9953, -0.8959]]],\n",
      "       device='cuda:0'), [None, None, None, None, None, None, None, None], 0)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[25]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      2\u001b[39m inputs = tokenizer(input_prompt, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(device)\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(model.model(inputs[\u001b[33m'\u001b[39m\u001b[33minput_ids\u001b[39m\u001b[33m'\u001b[39m]))\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43minput_ids\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mattention_mask\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpad_token_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43meos_token_id\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m prediction = tokenizer.decode(outputs[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28mprint\u001b[39m(prediction)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\peft\\peft_model.py:2050\u001b[39m, in \u001b[36mPeftModelForCausalLM.generate\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   2048\u001b[39m             outputs = \u001b[38;5;28mself\u001b[39m.base_model.generate(*args, **kwargs)\n\u001b[32m   2049\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2050\u001b[39m         outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m:\n\u001b[32m   2052\u001b[39m     \u001b[38;5;28mself\u001b[39m.base_model.prepare_inputs_for_generation = \u001b[38;5;28mself\u001b[39m.base_model_prepare_inputs_for_generation\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\transformers\\generation\\utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\transformers\\generation\\utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Git\\minimind\\model\\model_minimind.py:450\u001b[39m, in \u001b[36mMiniMindForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, past_key_values, use_cache, logits_to_keep, **args)\u001b[39m\n\u001b[32m    443\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,\n\u001b[32m    444\u001b[39m             input_ids: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    445\u001b[39m             attention_mask: Optional[torch.Tensor] = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    448\u001b[39m             logits_to_keep: Union[\u001b[38;5;28mint\u001b[39m, torch.Tensor] = \u001b[32m0\u001b[39m,\n\u001b[32m    449\u001b[39m             **args):\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[43minput_ids\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m)\n\u001b[32m    451\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m'\u001b[39m\u001b[33m--------------------------------\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m    452\u001b[39m     hidden_states, past_key_values, aux_loss = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    453\u001b[39m         input_ids=input_ids,\n\u001b[32m    454\u001b[39m         attention_mask=attention_mask,\n\u001b[32m   (...)\u001b[39m\u001b[32m    457\u001b[39m         **args\n\u001b[32m    458\u001b[39m     )\n",
      "\u001b[31mAttributeError\u001b[39m: 'NoneType' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "input_prompt = [\"Classify the sentiment of the following review as positive or negative.\\nReview: I love this film!\\nSentiment:\"]\n",
    "inputs = tokenizer(input_prompt, return_tensors=\"pt\").to(device)\n",
    "print(model.model(inputs['input_ids']))\n",
    "outputs = model.generate(\n",
    "    input_ids=inputs['input_ids'],\n",
    "    attention_mask=inputs['attention_mask'],\n",
    "    max_new_tokens=5,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")\n",
    "prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "b86dc140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token '<|im_start|>' 的 ID: 1\n",
      "当前embedding形状: torch.Size([512])\n",
      "当前embedding前5个值: tensor([ 0.0036, -0.0128, -0.0255,  0.0096, -0.0007], device='cuda:0',\n",
      "       grad_fn=<SliceBackward0>)\n",
      "修改后embedding前5个值: tensor([0., 0., 0., 0., 0.], device='cuda:0', grad_fn=<SliceBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# 1. 获取要修改的token的id\n",
    "token_str = \"<|im_start|>\"  # 或者任何你想修改的token\n",
    "token_id = tokenizer.convert_tokens_to_ids(token_str)\n",
    "print(f\"Token '{token_str}' 的 ID: {token_id}\")\n",
    "\n",
    "# 2. 访问embedding层\n",
    "# 注意：MiniMind中embed_tokens和lm_head共享权重\n",
    "embedding_layer = model.model.embed_tokens  # 或者 model.lm_head (它们共享权重)\n",
    "\n",
    "# 3. 查看当前embedding\n",
    "current_embedding = embedding_layer.weight[token_id]\n",
    "print(f\"当前embedding形状: {current_embedding.shape}\")\n",
    "print(f\"当前embedding前5个值: {current_embedding[:5]}\")\n",
    "\n",
    "# # 4. 修改embedding（需要先设置为训练模式或使用requires_grad_）\n",
    "model.model.embed_tokens.weight.requires_grad_(True)\n",
    "\n",
    "# # 方法A: 直接赋值新的embedding向量\n",
    "# new_embedding = torch.randn(model.config.hidden_size)  # 随机初始化\n",
    "# # 或者从其他token复制\n",
    "# # new_embedding = embedding_layer.weight[other_token_id].clone()\n",
    "\n",
    "# embedding_layer.weight.data[token_id] = new_embedding\n",
    "\n",
    "# # 方法B: 在现有embedding基础上调整\n",
    "# # embedding_layer.weight.data[token_id] += 0.1 * torch.randn(model.config.hidden_size)\n",
    "\n",
    "# # 方法C: 设置为特定值\n",
    "embedding_layer.weight.data[token_id] = torch.zeros(model.config.hidden_size)\n",
    "\n",
    "# # 5. 验证修改\n",
    "modified_embedding = embedding_layer.weight[token_id]\n",
    "print(f\"修改后embedding前5个值: {modified_embedding[:5]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6a610db0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer.bos_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68e94ab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ '<cls>' 不在vocab中，可以添加\n",
      "\n",
      "成功添加 1 个新词到tokenizer\n",
      "\n",
      "新添加的词及其ID:\n",
      "  '<cls>' -> ID: 6400\n",
      "\n",
      "⚠️  注意：模型嵌入层需要调整大小以支持新词\n",
      "当前vocab大小: 6401\n",
      "模型embedding层大小: 6400\n",
      "是否相同： True\n",
      "调整后模型embedding层大小: 6401\n",
      "✅ 已初始化 1 个新token的embedding\n",
      "\n",
      "验证新词的使用:\n",
      "  原文: 这是一个测试: <cls> 应该被正确识别\n",
      "  编码后: [434, 1589, 3560, 28, 223, 6400, 223, 1406, 1183, 2826, 5159]\n",
      "  解码后: 这是一个测试: <cls> 应该被正确识别\n"
     ]
    }
   ],
   "source": [
    "# 给tokenizer的vocab添加新词\n",
    "# 方法：使用 tokenizer.add_tokens() 添加新词\n",
    "\n",
    "# 1. 定义要添加的新词\n",
    "new_tokens = [\"<cls>\"]\n",
    "\n",
    "# 2. 检查这些词是否已经存在于vocab中\n",
    "existing_tokens = []\n",
    "for token in new_tokens:\n",
    "    if token in tokenizer.get_vocab():\n",
    "        existing_tokens.append(token)\n",
    "        print(f\"⚠️  '{token}' 已经存在于vocab中，ID: {tokenizer.convert_tokens_to_ids(token)}\")\n",
    "    else:\n",
    "        print(f\"✅ '{token}' 不在vocab中，可以添加\")\n",
    "\n",
    "# 3. 过滤掉已存在的词\n",
    "tokens_to_add = [token for token in new_tokens if token not in existing_tokens]\n",
    "\n",
    "if tokens_to_add:\n",
    "    # 4. 添加新词到tokenizer\n",
    "    num_added = tokenizer.add_tokens(tokens_to_add)\n",
    "    print(f\"\\n成功添加 {num_added} 个新词到tokenizer\")\n",
    "    \n",
    "    # 5. 查看新添加的词及其ID\n",
    "    print(\"\\n新添加的词及其ID:\")\n",
    "    for token in tokens_to_add:\n",
    "        token_id = tokenizer.convert_tokens_to_ids(token)\n",
    "        print(f\"  '{token}' -> ID: {token_id}\")\n",
    "    \n",
    "    # 6. 如果模型需要支持这些新词，需要调整模型嵌入层大小\n",
    "    # 注意：这需要重新初始化新token的embedding权重\n",
    "    if num_added > 0:\n",
    "        print(f\"\\n⚠️  注意：模型嵌入层需要调整大小以支持新词\")\n",
    "        print(f\"当前vocab大小: {len(tokenizer)}\")\n",
    "        print(f\"模型embedding层大小: {model.get_input_embeddings().weight.shape[0]}\")\n",
    "        # 在 resize 之前保存原有 embedding\n",
    "        old_embedding = model.get_input_embeddings().weight.data[:6400].clone()\n",
    "\n",
    "        # 执行 resize\n",
    "        model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "        # 比较 resize 后的前 6400 个 embedding\n",
    "        new_embedding = model.get_input_embeddings().weight.data[:6400]\n",
    "\n",
    "        # 检查是否相同\n",
    "        print('是否相同：',torch.allclose(old_embedding, new_embedding))  # 应该返回 True\n",
    "        # 调整模型嵌入层大小\n",
    "        # model.resize_token_embeddings(len(tokenizer))\n",
    "        print(f\"调整后模型embedding层大小: {model.get_input_embeddings().weight.shape[0]}\")\n",
    "        \n",
    "        # 7. 可选：初始化新token的embedding（可以随机初始化或从其他token复制）\n",
    "        # 获取新添加的token IDs\n",
    "        new_token_ids = [tokenizer.convert_tokens_to_ids(token) for token in tokens_to_add]\n",
    "        \n",
    "        # 方法A: 随机初始化（使用正态分布）\n",
    "        embedding_dim = model.get_input_embeddings().weight.shape[1]\n",
    "        for token_id in new_token_ids:\n",
    "            # 从标准正态分布随机初始化\n",
    "            new_embedding = torch.randn(embedding_dim) * 0.02  # 小方差初始化\n",
    "            model.get_input_embeddings().weight.data[token_id] = new_embedding\n",
    "            # 如果lm_head和embed_tokens共享权重，只需要修改一处\n",
    "            if hasattr(model, 'lm_head') and model.lm_head.weight is model.get_input_embeddings().weight:\n",
    "                pass  # 已经共享，无需额外操作\n",
    "        \n",
    "        print(f\"✅ 已初始化 {len(new_token_ids)} 个新token的embedding\")\n",
    "        \n",
    "        # 8. 验证新词可以正常使用\n",
    "        print(\"\\n验证新词的使用:\")\n",
    "        test_text = f\"这是一个测试: {tokens_to_add[0]} 应该被正确识别\"\n",
    "        encoded = tokenizer.encode(test_text, add_special_tokens=False)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        print(f\"  原文: {test_text}\")\n",
    "        print(f\"  编码后: {encoded}\")\n",
    "        print(f\"  解码后: {decoded}\")\n",
    "        \n",
    "        # 9. 可选：保存更新后的tokenizer\n",
    "        # tokenizer.save_pretrained('./model_updated/')  # 取消注释以保存\n",
    "        # print(\"\\n✅ Tokenizer已保存到 './model_updated/'\")\n",
    "else:\n",
    "    print(\"\\n没有需要添加的新词\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "83221d11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "保存tokenizer...\n",
      "================================================================================\n",
      "✅ Tokenizer已保存到: ./test/\n",
      "  ✓ tokenizer_config.json\n",
      "  ✗ vocab.json (可能不存在，取决于tokenizer类型)\n",
      "  ✗ merges.txt (可能不存在，取决于tokenizer类型)\n",
      "  ✓ special_tokens_map.json\n",
      "\n",
      "================================================================================\n",
      "保存模型权重...\n",
      "================================================================================\n",
      "✅ 模型权重已保存到: ./test\\full_sft_cls_512.pth\n",
      "   权重名称: full_sft_cls\n",
      "   隐藏层大小: 512\n",
      "   MoE后缀: \n",
      "   参数量: 29.11M\n",
      "✅ 配置信息已保存到: ./test\\model_config_info.json\n",
      "\n",
      "================================================================================\n",
      "保存验证:\n",
      "================================================================================\n",
      "✅ Tokenizer可以正常加载 (vocab大小: 6401)\n",
      "✅ 新添加的token仍然存在于vocab中\n",
      "✅ 模型权重文件可以正常加载 (包含 75 个参数键)\n",
      "\n",
      "================================================================================\n",
      "保存模型配置...\n",
      "================================================================================\n",
      "✅ 模型配置已保存到: ./test\\config.json\n",
      "✅ 模型定义文件已创建: ./test\\modeling_minimind.py\n",
      "✅ config.json验证通过\n",
      "   模型类型: minimind\n",
      "   vocab大小: 6401\n",
      "   隐藏层大小: 512\n",
      "\n",
      "================================================================================\n",
      "保存完成！\n",
      "================================================================================\n",
      "\n",
      "📌 重新加载时使用以下参数:\n",
      "    args.load_from = './test'\n",
      "    args.save_dir = './test'\n",
      "    args.weight = 'full_sft_cls'\n",
      "    args.hidden_size = 512\n",
      "    args.use_moe = 0\n",
      "\n",
      "✅ 现在可以使用 AutoModelForCausalLM.from_pretrained('./test', trust_remote_code=True) 加载模型\n"
     ]
    }
   ],
   "source": [
    "# 在Cell 14的最后，添加保存代码（在tokenizer和model都已更新之后）\n",
    "\n",
    "import os\n",
    "import torch\n",
    "\n",
    "# ========== 保存更新后的tokenizer和模型 ==========\n",
    "\n",
    "# 1. 设置保存路径\n",
    "save_base_dir = \"./test\"  # tokenizer保存目录\n",
    "save_model_dir = \"./test\"  # 模型权重保存目录（与init_model的save_dir保持一致）\n",
    "os.makedirs(save_base_dir, exist_ok=True)\n",
    "os.makedirs(save_model_dir, exist_ok=True)\n",
    "\n",
    "# 2. 保存tokenizer到目录\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"保存tokenizer...\")\n",
    "print(\"=\" * 80)\n",
    "tokenizer.save_pretrained(save_base_dir)\n",
    "print(f\"✅ Tokenizer已保存到: {save_base_dir}/\")\n",
    "\n",
    "# 验证tokenizer文件\n",
    "tokenizer_files = ['tokenizer_config.json', 'vocab.json', 'merges.txt', 'special_tokens_map.json']\n",
    "for file in tokenizer_files:\n",
    "    file_path = os.path.join(save_base_dir, file)\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"  ✓ {file}\")\n",
    "    else:\n",
    "        print(f\"  ✗ {file} (可能不存在，取决于tokenizer类型)\")\n",
    "\n",
    "# 3. 保存模型权重到.pth文件\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"保存模型权重...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 获取模型配置参数（用于构建文件名）\n",
    "# 这些参数需要与init_model时使用的参数一致\n",
    "model_weight_name = \"full_sft_cls\"  # 权重文件名（如 'pretrain', 'full_sft' 等）\n",
    "hidden_size = model.config.hidden_size if hasattr(model, 'config') else 512  # 从模型配置获取\n",
    "use_moe = getattr(model.config, 'use_moe', False) if hasattr(model, 'config') else False\n",
    "\n",
    "# 构建权重文件名（格式与init_model期望的一致）\n",
    "moe_suffix = '_moe' if use_moe else ''\n",
    "weight_filename = f'{model_weight_name}_{hidden_size}{moe_suffix}.pth'\n",
    "weight_path = os.path.join(save_model_dir, weight_filename)\n",
    "\n",
    "# 获取模型状态字典\n",
    "if hasattr(model, 'module'):  # 如果模型被包装在DataParallel中\n",
    "    state_dict = model.module.state_dict()\n",
    "else:\n",
    "    state_dict = model.state_dict()\n",
    "\n",
    "# 转换为半精度以节省空间（与训练脚本保持一致）\n",
    "state_dict_to_save = {k: v.half() if v.dtype == torch.float32 else v \n",
    "                      for k, v in state_dict.items()}\n",
    "\n",
    "# 保存权重\n",
    "torch.save(state_dict_to_save, weight_path)\n",
    "print(f\"✅ 模型权重已保存到: {weight_path}\")\n",
    "print(f\"   权重名称: {model_weight_name}\")\n",
    "print(f\"   隐藏层大小: {hidden_size}\")\n",
    "print(f\"   MoE后缀: {moe_suffix}\")\n",
    "print(f\"   参数量: {sum(p.numel() for p in state_dict.values()) / 1e6:.2f}M\")\n",
    "\n",
    "# 4. 保存模型配置信息（可选，用于记录）\n",
    "config_info = {\n",
    "    'vocab_size': len(tokenizer),\n",
    "    'hidden_size': hidden_size,\n",
    "    'use_moe': use_moe,\n",
    "    'new_tokens': tokens_to_add if 'tokens_to_add' in locals() else [],\n",
    "    'tokenizer_path': save_base_dir,\n",
    "    'model_weight_path': weight_path,\n",
    "    'model_weight_name': model_weight_name\n",
    "}\n",
    "\n",
    "import json\n",
    "config_path = os.path.join(save_base_dir, 'model_config_info.json')\n",
    "with open(config_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(config_info, f, indent=2, ensure_ascii=False)\n",
    "print(f\"✅ 配置信息已保存到: {config_path}\")\n",
    "\n",
    "# 5. 验证保存结果\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"保存验证:\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 验证tokenizer可以重新加载\n",
    "try:\n",
    "    test_tokenizer = AutoTokenizer.from_pretrained(save_base_dir)\n",
    "    print(f\"✅ Tokenizer可以正常加载 (vocab大小: {len(test_tokenizer)})\")\n",
    "    # 验证新添加的token是否还在\n",
    "    if tokens_to_add and all(token in test_tokenizer.get_vocab() for token in tokens_to_add):\n",
    "        print(f\"✅ 新添加的token仍然存在于vocab中\")\n",
    "    del test_tokenizer\n",
    "except Exception as e:\n",
    "    print(f\"❌ Tokenizer加载失败: {e}\")\n",
    "\n",
    "# 验证模型权重可以加载\n",
    "try:\n",
    "    test_weights = torch.load(weight_path, map_location='cpu')\n",
    "    print(f\"✅ 模型权重文件可以正常加载 (包含 {len(test_weights)} 个参数键)\")\n",
    "    del test_weights\n",
    "except Exception as e:\n",
    "    print(f\"❌ 模型权重加载失败: {e}\")\n",
    "\n",
    "# 6. 保存模型配置到config.json（使得AutoModelForCausalLM.from_pretrained()可以加载）\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"保存模型配置...\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# 注册MiniMindConfig和MiniMindForCausalLM到AutoClass\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "MiniMindConfig.register_for_auto_class()\n",
    "MiniMindForCausalLM.register_for_auto_class(\"AutoModelForCausalLM\")\n",
    "\n",
    "# 获取模型配置\n",
    "if hasattr(model, 'config'):\n",
    "    model_config = model.config\n",
    "else:\n",
    "    # 如果没有config，创建一个新的MiniMindConfig\n",
    "    model_config = MiniMindConfig(\n",
    "        vocab_size=len(tokenizer),\n",
    "        hidden_size=hidden_size,\n",
    "        num_hidden_layers=getattr(model, 'num_hidden_layers', 12),\n",
    "        use_moe=use_moe\n",
    "    )\n",
    "\n",
    "# 更新vocab_size以匹配实际的tokenizer大小\n",
    "model_config.vocab_size = len(tokenizer)\n",
    "\n",
    "# 保存配置到config.json\n",
    "config_path = os.path.join(save_base_dir, 'config.json')\n",
    "model_config.save_pretrained(save_base_dir)\n",
    "print(f\"✅ 模型配置已保存到: {config_path}\")\n",
    "\n",
    "# 创建modeling_minimind.py文件，用于AutoModelForCausalLM.from_pretrained()加载\n",
    "modeling_file_path = os.path.join(save_base_dir, 'modeling_minimind.py')\n",
    "modeling_code = '''# This file is auto-generated for loading MiniMind model with AutoModelForCausalLM\n",
    "# The actual model implementation is in the project's model/model_minimind.py\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root to path so we can import the actual model\n",
    "project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "# Import and re-export the model classes\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "\n",
    "# Re-export for transformers\n",
    "__all__ = ['MiniMindConfig', 'MiniMindForCausalLM']\n",
    "'''\n",
    "\n",
    "with open(modeling_file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(modeling_code)\n",
    "print(f\"✅ 模型定义文件已创建: {modeling_file_path}\")\n",
    "\n",
    "# 验证config.json是否存在且格式正确\n",
    "try:\n",
    "    import json\n",
    "    with open(config_path, 'r', encoding='utf-8') as f:\n",
    "        config_dict = json.load(f)\n",
    "    print(f\"✅ config.json验证通过\")\n",
    "    print(f\"   模型类型: {config_dict.get('model_type', 'N/A')}\")\n",
    "    print(f\"   vocab大小: {config_dict.get('vocab_size', 'N/A')}\")\n",
    "    print(f\"   隐藏层大小: {config_dict.get('hidden_size', 'N/A')}\")\n",
    "    \n",
    "    # 确保model_type字段存在\n",
    "    if 'model_type' not in config_dict:\n",
    "        print(f\"⚠️  config.json缺少model_type字段，正在修复...\")\n",
    "        config_dict['model_type'] = 'minimind'\n",
    "        with open(config_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(config_dict, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"✅ 已修复config.json\")\n",
    "except Exception as e:\n",
    "    print(f\"⚠️  config.json验证失败: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"保存完成！\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\n📌 重新加载时使用以下参数:\")\n",
    "print(f\"    args.load_from = '{save_base_dir}'\")\n",
    "print(f\"    args.save_dir = '{save_model_dir}'\")\n",
    "print(f\"    args.weight = '{model_weight_name}'\")\n",
    "print(f\"    args.hidden_size = {hidden_size}\")\n",
    "print(f\"    args.use_moe = {1 if use_moe else 0}\")\n",
    "print(f\"\\n✅ 现在可以使用 AutoModelForCausalLM.from_pretrained('{save_base_dir}', trust_remote_code=True) 加载模型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a6debd8d",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers_modules.test.model_minimind.MiniMindConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     21\u001b[39m args = Args()\n\u001b[32m     23\u001b[39m \u001b[38;5;66;03m# 重新加载tokenizer和模型\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m model_reloaded, tokenizer_reloaded = \u001b[43minit_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m + \u001b[33m\"\u001b[39m\u001b[33m=\u001b[39m\u001b[33m\"\u001b[39m * \u001b[32m80\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m重新加载完成！\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 33\u001b[39m, in \u001b[36minit_model\u001b[39m\u001b[34m(args)\u001b[39m\n\u001b[32m     31\u001b[39m         load_lora(model, \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33m./\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.save_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/lora/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.lora_weight\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs.hidden_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.pth\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m     model = \u001b[43mAutoModelForCausalLM\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mload_from\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mMiniMind模型参数: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28msum\u001b[39m(p.numel()\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mp\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mmodel.parameters())\u001b[38;5;250m \u001b[39m/\u001b[38;5;250m \u001b[39m\u001b[32m1e6\u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m M(illion)\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m model.eval().to(device), tokenizer\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Anaconda\\envs\\py312pt291cu128\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py:607\u001b[39m, in \u001b[36m_BaseAutoModelClass.from_pretrained\u001b[39m\u001b[34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[39m\n\u001b[32m    603\u001b[39m         config = config.get_text_config()\n\u001b[32m    604\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class.from_pretrained(\n\u001b[32m    605\u001b[39m         pretrained_model_name_or_path, *model_args, config=config, **hub_kwargs, **kwargs\n\u001b[32m    606\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m607\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    608\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig.\u001b[34m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    609\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(c.\u001b[34m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m._model_mapping)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    610\u001b[39m )\n",
      "\u001b[31mValueError\u001b[39m: Unrecognized configuration class <class 'transformers_modules.test.model_minimind.MiniMindConfig'> for this kind of AutoModel: AutoModelForCausalLM.\nModel type should be one of ApertusConfig, ArceeConfig, AriaTextConfig, BambaConfig, BartConfig, BertConfig, BertGenerationConfig, BigBirdConfig, BigBirdPegasusConfig, BioGptConfig, BitNetConfig, BlenderbotConfig, BlenderbotSmallConfig, BloomConfig, BltConfig, CamembertConfig, LlamaConfig, CodeGenConfig, CohereConfig, Cohere2Config, CpmAntConfig, CTRLConfig, Data2VecTextConfig, DbrxConfig, DeepseekV2Config, DeepseekV3Config, DiffLlamaConfig, DogeConfig, Dots1Config, ElectraConfig, Emu3Config, ErnieConfig, Ernie4_5Config, Ernie4_5_MoeConfig, Exaone4Config, FalconConfig, FalconH1Config, FalconMambaConfig, FlexOlmoConfig, FuyuConfig, GemmaConfig, Gemma2Config, Gemma3Config, Gemma3TextConfig, Gemma3nConfig, Gemma3nTextConfig, GitConfig, GlmConfig, Glm4Config, Glm4MoeConfig, GotOcr2Config, GPT2Config, GPT2Config, GPTBigCodeConfig, GPTNeoConfig, GPTNeoXConfig, GPTNeoXJapaneseConfig, GptOssConfig, GPTJConfig, GraniteConfig, GraniteMoeConfig, GraniteMoeHybridConfig, GraniteMoeSharedConfig, HeliumConfig, HunYuanDenseV1Config, HunYuanMoEV1Config, JambaConfig, JetMoeConfig, Lfm2Config, LlamaConfig, Llama4Config, Llama4TextConfig, LongcatFlashConfig, MambaConfig, Mamba2Config, MarianConfig, MBartConfig, MegaConfig, MegatronBertConfig, MiniMaxConfig, MinistralConfig, MistralConfig, MixtralConfig, MllamaConfig, ModernBertDecoderConfig, MoshiConfig, MptConfig, MusicgenConfig, MusicgenMelodyConfig, MvpConfig, NemotronConfig, OlmoConfig, Olmo2Config, Olmo3Config, OlmoeConfig, OpenLlamaConfig, OpenAIGPTConfig, OPTConfig, PegasusConfig, PersimmonConfig, PhiConfig, Phi3Config, Phi4MultimodalConfig, PhimoeConfig, PLBartConfig, ProphetNetConfig, QDQBertConfig, Qwen2Config, Qwen2MoeConfig, Qwen3Config, Qwen3MoeConfig, Qwen3NextConfig, RecurrentGemmaConfig, ReformerConfig, RemBertConfig, RobertaConfig, RobertaPreLayerNormConfig, RoCBertConfig, RoFormerConfig, RwkvConfig, SeedOssConfig, SmolLM3Config, Speech2Text2Config, StableLmConfig, Starcoder2Config, TransfoXLConfig, TrOCRConfig, VaultGemmaConfig, WhisperConfig, XGLMConfig, XLMConfig, XLMProphetNetConfig, XLMRobertaConfig, XLMRobertaXLConfig, XLNetConfig, xLSTMConfig, XmodConfig, ZambaConfig, Zamba2Config."
     ]
    }
   ],
   "source": [
    "# ========== 重新加载保存的tokenizer和模型 ==========\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "from model.model_minimind import MiniMindConfig, MiniMindForCausalLM\n",
    "import argparse\n",
    "\n",
    "# 设置加载参数（与保存时保持一致）\n",
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.load_from = \"./test\"  # tokenizer目录\n",
    "        self.save_dir = \"./test\"  # 模型权重目录\n",
    "        self.weight = \"full_sft_cls\"  # 权重文件名\n",
    "        self.hidden_size = 512  # 根据实际模型调整\n",
    "        self.num_hidden_layers = 12  # 根据实际模型调整\n",
    "        self.use_moe = 0  # 根据实际模型调整\n",
    "        self.lora_weight = \"None\"\n",
    "        self.inference_rope_scaling = None\n",
    "        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# 使用notebook中的init_model函数重新加载\n",
    "args = Args()\n",
    "\n",
    "# 重新加载tokenizer和模型\n",
    "model_reloaded, tokenizer_reloaded = init_model(args)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"重新加载完成！\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"Tokenizer vocab大小: {len(tokenizer_reloaded)}\")\n",
    "print(f\"模型参数量: {sum(p.numel() for p in model_reloaded.parameters()) / 1e6:.2f}M\")\n",
    "\n",
    "# 验证新添加的token是否还在\n",
    "if 'tokens_to_add' in locals() and tokens_to_add:\n",
    "    print(f\"\\n验证新添加的token:\")\n",
    "    for token in tokens_to_add:\n",
    "        if token in tokenizer_reloaded.get_vocab():\n",
    "            token_id = tokenizer_reloaded.convert_tokens_to_ids(token)\n",
    "            print(f\"  ✅ '{token}' -> ID: {token_id}\")\n",
    "        else:\n",
    "            print(f\"  ❌ '{token}' 不在vocab中\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51821c32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在处理文件: dataset/bbc_news_data.jsonl\n",
      "================================================================================\n",
      "已处理 100 条数据，当前最长长度: 1266\n",
      "已处理 200 条数据，当前最长长度: 1525\n",
      "已处理 300 条数据，当前最长长度: 1702\n",
      "已处理 400 条数据，当前最长长度: 1708\n",
      "已处理 500 条数据，当前最长长度: 1708\n",
      "已处理 600 条数据，当前最长长度: 1708\n",
      "已处理 700 条数据，当前最长长度: 2217\n",
      "已处理 800 条数据，当前最长长度: 6270\n",
      "已处理 900 条数据，当前最长长度: 6270\n",
      "已处理 1000 条数据，当前最长长度: 6270\n",
      "已处理 1100 条数据，当前最长长度: 6270\n",
      "已处理 1200 条数据，当前最长长度: 7710\n",
      "已处理 1300 条数据，当前最长长度: 7710\n",
      "已处理 1400 条数据，当前最长长度: 7710\n",
      "已处理 1500 条数据，当前最长长度: 7710\n",
      "已处理 1600 条数据，当前最长长度: 7710\n",
      "已处理 1700 条数据，当前最长长度: 7710\n",
      "已处理 1800 条数据，当前最长长度: 7710\n",
      "已处理 1900 条数据，当前最长长度: 7710\n",
      "已处理 2000 条数据，当前最长长度: 7710\n",
      "已处理 2100 条数据，当前最长长度: 7710\n",
      "已处理 2200 条数据，当前最长长度: 7710\n",
      "\n",
      "================================================================================\n",
      "结果统计:\n",
      "================================================================================\n",
      "总样本数: 2225\n",
      "最长训练文本长度: 7710 tokens\n",
      "最长文本的样本索引: 1185 (行号: 1186)\n",
      "\n",
      "长度统计:\n",
      "  平均长度: 800.13 tokens\n",
      "  最短长度: 259 tokens\n",
      "  最长长度: 7710 tokens\n",
      "  中位数长度: 708 tokens\n",
      "\n",
      "最长样本详情:\n",
      "  索引: 1185\n",
      "  行号: 1186\n",
      "  长度: 7710 tokens\n",
      "  User内容预览: 标题：Terror powers expose 'tyranny'\n",
      "内容： The Lord Chancellor has defended government plans to introduce...\n",
      "  Assistant内容: politics\n",
      "  训练模板预览: <|im_start|>system\n",
      "You are a helpful assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "标题：Terror powers expose 'tyranny'\n",
      "内容： The Lord Chancellor has defended government plans to introduce control orders to keep fo...\n",
      "\n",
      "长度分布:\n",
      "  0-1024    : 1797 条 (80.76%)\n",
      "  0-100     :    0 条 ( 0.00%)\n",
      "  100-200   :    0 条 ( 0.00%)\n",
      "  200-300   :    1 条 ( 0.04%)\n",
      "  300-400   :  100 条 ( 4.49%)\n",
      "  400-500   :  243 条 (10.92%)\n",
      "  500-600   :  382 条 (17.17%)\n",
      "  600-700   :  363 条 (16.31%)\n",
      "  700-800   :  253 条 (11.37%)\n",
      "  800-900   :  210 条 ( 9.44%)\n",
      "  900-1000  :  199 条 ( 8.94%)\n",
      "  1000-1100 :  136 条 ( 6.11%)\n",
      "  1100-1200 :  102 条 ( 4.58%)\n",
      "  1200-1300 :   71 条 ( 3.19%)\n",
      "  1300-1400 :   51 条 ( 2.29%)\n",
      "  1400-1500 :   42 条 ( 1.89%)\n",
      "  1500-1600 :   22 条 ( 0.99%)\n",
      "  1600-1700 :   17 条 ( 0.76%)\n",
      "  1700-1800 :    5 条 ( 0.22%)\n",
      "  1800-1900 :    6 条 ( 0.27%)\n",
      "  1900-2000 :    4 条 ( 0.18%)\n",
      "  2000+     :   18 条 ( 0.81%)\n"
     ]
    }
   ],
   "source": [
    "# 计算bbc_news_train.jsonl中数据转换为训练模板后的最长文本长度\n",
    "import json\n",
    "\n",
    "def get_max_training_length(jsonl_path, tokenizer):\n",
    "    \"\"\"\n",
    "    计算训练数据转换为训练模板后的最长文本长度\n",
    "    \n",
    "    Args:\n",
    "        jsonl_path: JSONL文件路径\n",
    "        tokenizer: tokenizer对象\n",
    "    \n",
    "    Returns:\n",
    "        max_length: 最长文本的token长度\n",
    "        max_index: 最长文本的样本索引\n",
    "        max_sample: 最长文本的样本内容\n",
    "    \"\"\"\n",
    "    max_length = 0\n",
    "    max_index = -1\n",
    "    max_sample = None\n",
    "    all_lengths = []\n",
    "    \n",
    "    print(f\"正在处理文件: {jsonl_path}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    with open(jsonl_path, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            try:\n",
    "                data = json.loads(line.strip())\n",
    "                \n",
    "                # 检查数据格式\n",
    "                if 'conversations' not in data:\n",
    "                    print(f\"⚠️  行 {line_num}: 缺少 'conversations' 字段，跳过\")\n",
    "                    continue\n",
    "                \n",
    "                conversations = data['conversations']\n",
    "                if not conversations or len(conversations) == 0:\n",
    "                    print(f\"⚠️  行 {line_num}: conversations 为空，跳过\")\n",
    "                    continue\n",
    "                \n",
    "                # 使用与SFTDataset相同的方式创建训练模板\n",
    "                # 参考 SFTDataset._create_chat_prompt 方法\n",
    "                messages = conversations.copy()\n",
    "                tools = conversations[0][\"functions\"] if (conversations and conversations[0][\"role\"] == \"system\" and conversations[0].get(\"functions\")) else None\n",
    "                \n",
    "                # 应用chat_template（与训练时保持一致）\n",
    "                prompt = tokenizer.apply_chat_template(\n",
    "                    messages,\n",
    "                    tokenize=False,\n",
    "                    add_generation_prompt=False,\n",
    "                    tools=tools\n",
    "                )\n",
    "                \n",
    "                # 对转换后的文本进行tokenize\n",
    "                input_ids = tokenizer(prompt, add_special_tokens=False).input_ids\n",
    "                length = len(input_ids)\n",
    "                all_lengths.append(length)\n",
    "                \n",
    "                # 更新最大值\n",
    "                if length > max_length:\n",
    "                    max_length = length\n",
    "                    max_index = line_num - 1  # 转换为0-based索引\n",
    "                    max_sample = {\n",
    "                        'index': max_index,\n",
    "                        'line_num': line_num,\n",
    "                        'length': length,\n",
    "                        'conversations': conversations,\n",
    "                        'prompt_preview': prompt[:200] + '...' if len(prompt) > 200 else prompt\n",
    "                    }\n",
    "                \n",
    "                # 每处理100条数据打印一次进度\n",
    "                if line_num % 100 == 0:\n",
    "                    print(f\"已处理 {line_num} 条数据，当前最长长度: {max_length}\")\n",
    "                    \n",
    "            except json.JSONDecodeError as e:\n",
    "                print(f\"⚠️  行 {line_num}: JSON解析错误 - {e}\")\n",
    "                continue\n",
    "            except Exception as e:\n",
    "                print(f\"⚠️  行 {line_num}: 处理错误 - {e}\")\n",
    "                continue\n",
    "    \n",
    "    return max_length, max_index, max_sample, all_lengths\n",
    "\n",
    "# 执行计算（需要先运行Cell 10初始化tokenizer）\n",
    "if 'tokenizer' in globals():\n",
    "    max_length, max_index, max_sample, all_lengths = get_max_training_length('dataset/bbc_news_data.jsonl', tokenizer)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"结果统计:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"总样本数: {len(all_lengths)}\")\n",
    "    print(f\"最长训练文本长度: {max_length} tokens\")\n",
    "    print(f\"最长文本的样本索引: {max_index} (行号: {max_sample['line_num']})\")\n",
    "    print(f\"\\n长度统计:\")\n",
    "    print(f\"  平均长度: {sum(all_lengths) / len(all_lengths):.2f} tokens\")\n",
    "    print(f\"  最短长度: {min(all_lengths)} tokens\")\n",
    "    print(f\"  最长长度: {max(all_lengths)} tokens\")\n",
    "    print(f\"  中位数长度: {sorted(all_lengths)[len(all_lengths)//2]} tokens\")\n",
    "    \n",
    "    # 显示最长样本的详细信息\n",
    "    if max_sample:\n",
    "        print(f\"\\n最长样本详情:\")\n",
    "        print(f\"  索引: {max_sample['index']}\")\n",
    "        print(f\"  行号: {max_sample['line_num']}\")\n",
    "        print(f\"  长度: {max_sample['length']} tokens\")\n",
    "        print(f\"  User内容预览: {max_sample['conversations'][0]['content'][:100]}...\")\n",
    "        print(f\"  Assistant内容: {max_sample['conversations'][1]['content']}\")\n",
    "        print(f\"  训练模板预览: {max_sample['prompt_preview']}\")\n",
    "    \n",
    "    # 长度分布统计\n",
    "    print(f\"\\n长度分布:\")\n",
    "    # 从0-2000每隔100设置一个区间\n",
    "    length_ranges = [(0,1024,'0-1024')]\n",
    "    for start in range(0, 2000, 100):\n",
    "        end = start + 100\n",
    "        label = f\"{start}-{end}\"\n",
    "        length_ranges.append((start, end, label))\n",
    "    # 添加最后一个区间：2000+\n",
    "    length_ranges.append((2000, float('inf'), \"2000+\"))\n",
    "    \n",
    "    for min_len, max_len, label in length_ranges:\n",
    "        count = sum(1 for l in all_lengths if min_len <= l < max_len)\n",
    "        percentage = count / len(all_lengths) * 100 if all_lengths else 0\n",
    "        print(f\"  {label:10s}: {count:4d} 条 ({percentage:5.2f}%)\")\n",
    "else:\n",
    "    print(\"⚠️  请先运行 Cell 10 初始化 tokenizer\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4b8ada",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py312pt291cu128",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
