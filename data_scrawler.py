#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
é€šè¿‡ Wikipedia API é€’å½’æŠ“å–ä½“è‚²ç›¸å…³æ¡ç›®ï¼ˆåŒ…å«å­åˆ†ç±»ï¼‰ï¼Œ
è¾“å‡º sports_wiki.jsonl
æ¯è¡Œæ ¼å¼: {"text": "..."}
"""

import requests
import json
import time
from pathlib import Path
import re
from collections import deque

API_URL = "https://en.wikipedia.org/w/api.php"

HEADERS = {
    # å»ºè®®æŠŠé‚®ç®±æ”¹æˆä½ è‡ªå·±çš„ï¼Œæ–¹ä¾¿ Wikipedia é‚£è¾¹çœŸå‡ºäº‹äº†èƒ½è”ç³»åˆ°ä½ 
    "User-Agent": "SportsWikiDataCollector/0.2 (2284605489@qq.com)"
}

# èµ·å§‹ä½“è‚²ç›¸å…³åˆ†ç±»ï¼ˆå¯ä»¥ç»§ç»­åŠ ï¼‰
ROOT_CATEGORIES = [
    # --- General sports / organization / competitions ---
    "Category:Sports",
    "Category:Sports by type",
    "Category:Sports by country",
    "Category:Sports competitions",
    "Category:Sports leagues",
    "Category:Sports clubs and teams",
    "Category:Sports venues",
    "Category:Sports governing bodies",
    "Category:Sports people",
    "Category:Sport of athletics",

    # --- Major multi-sport events ---
    "Category:Olympic Games",
    "Category:Paralympic Games",
    "Category:World Games",
    "Category:Commonwealth Games",
    "Category:Asian Games",
    "Category:Pan American Games",
    "Category:African Games",

    # --- Football (soccer) ---
    "Category:Association football",
    "Category:Association football competitions",
    "Category:Association football clubs",
    "Category:Association football players",
    "Category:FIFA",
    "Category:FIFA World Cup",
    "Category:UEFA",
    "Category:UEFA Champions League",
    "Category:Premier League",
    "Category:La Liga",
    "Category:Serie A",
    "Category:Bundesliga",
    "Category:Ligue 1",

    # --- American football ---
    "Category:American football",
    "Category:National Football League",
    "Category:College football",

    # --- Basketball ---
    "Category:Basketball",
    "Category:National Basketball Association",
    "Category:Women's National Basketball Association",
    "Category:NCAA basketball",

    # --- Baseball / softball ---
    "Category:Baseball",
    "Category:Major League Baseball",
    "Category:Softball",

    # --- Ice hockey ---
    "Category:Ice hockey",
    "Category:National Hockey League",

    # --- Cricket / rugby ---
    "Category:Cricket",
    "Category:International Cricket Council",
    "Category:Rugby union",
    "Category:Rugby league",

    # --- Tennis / golf ---
    "Category:Tennis",
    "Category:Grand Slam (tennis)",
    "Category:Golf",
    "Category:PGA Tour",
    "Category:LPGA Tour",

    # --- Motorsport ---
    "Category:Motorsport",
    "Category:Formula One",
    "Category:NASCAR",
    "Category:IndyCar Series",
    "Category:Motorcycle racing",
    "Category:MotoGP",

    # --- Combat sports ---
    "Category:Boxing",
    "Category:Mixed martial arts",
    "Category:Ultimate Fighting Championship",
    "Category:Wrestling",
    "Category:Judo",
    "Category:Taekwondo",

    # --- Other popular sports ---
    "Category:Swimming",
    "Category:Cycling",
    "Category:Track cycling",
    "Category:Road cycling",
    "Category:Athletics (sport)",
    "Category:Marathon running",
    "Category:Gymnastics",
    "Category:Volleyball",
    "Category:Handball",
    "Category:Badminton",
    "Category:Table tennis",
    "Category:Field hockey",
    "Category:Lacrosse",
    "Category:Skateboarding",
    "Category:Surfing",
    "Category:Skiing",
    "Category:Snowboarding",
]

# ROOT_CATEGORIES = [
#     # æ ¸å¿ƒå¨±ä¹
#     "Category:Entertainment",
#     "Category:Popular_culture",

#     # ç”µå½± / ç”µè§†
#     "Category:Film",
#     "Category:Cinema_of_the_United_States",
#     "Category:Television",
#     "Category:Television_programs",
#     "Category:Animated_films",
#     "Category:Animation",
#     "Category:Anime",
#     "Category:Manga",

#     # éŸ³ä¹
#     "Category:Music",
#     "Category:Musical_groups",
#     "Category:Singers",
#     "Category:Albums",
#     "Category:Songs",
#     "Category:Music_awards",
#     "Category:Music_genres",

#     # åäººã€æ¼”å‘˜ã€å¨±ä¹äººç‰©
#     "Category:Entertainers",
#     "Category:Actors",
#     "Category:Actresses",
#     "Category:Film_directors",
#     "Category:Celebrities",

#     # ç”µå­æ¸¸æˆ
#     "Category:Video_games",
#     "Category:Video_game_industry",
#     "Category:Video_game_development",
#     "Category:Esports",

#     # å¨±ä¹äº§ä¸š & å¥–é¡¹
#     "Category:Entertainment_industry",
#     "Category:Film_awards",
#     "Category:Television_awards",
#     "Category:Music_awards",

#     # å–œå‰§ã€ç»¼è‰ºã€ç¤¾äº¤è¡Œä¸º
#     "Category:Comedy",
#     "Category:Stand-up_comedy",
#     "Category:Humor",

#     # æ–‡å­¦ï¼ˆå¨±ä¹æ€§è´¨ï¼‰
#     "Category:Fiction",
#     "Category:Novels",
#     "Category:Fantasy",
#     "Category:Science_fiction",

#     # æ–‡åŒ–æ´»åŠ¨
#     "Category:Festivals",
#     "Category:Entertainment_events",
# ]


# æœ€å¤§ä¸‹é’»æ·±åº¦ï¼ˆ0 è¡¨ç¤ºåªçœ‹ rootï¼›1 è¡¨ç¤º root çš„å­åˆ†ç±»ï¼›2 è¡¨ç¤ºå­å­åˆ†ç±»ï¼‰
MAX_DEPTH = 2

# è¦æ”¶é›†çš„æœ€å¤§æ¡ç›®æ•°ï¼Œå¤Ÿå¤šçš„è¯è®­ç»ƒå°æ¨¡å‹å·²ç»å¾ˆçˆ½äº†
TARGET_ARTICLE_COUNT = 10000

MIN_WORDS = 50  # å¤ªçŸ­å°±ä¸¢æ‰


def clean_text(text: str) -> str:
    """ç®€å•æ¸…æ´— Wikipedia extractï¼šå»ç©ºè¡Œã€å‹ç¼©ç©ºç™½ã€‚"""
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    text = " ".join(lines)
    text = re.sub(r"\s+", " ", text).strip()
    return text


def fetch_category_members(cat_title: str, namespaces="0|14"):
    """
    æ‹¿ä¸€ä¸ªåˆ†ç±»çš„æˆå‘˜ï¼š
    - ns=0: è¯æ¡é¡µé¢
    - ns=14: å­åˆ†ç±»
    æ–‡æ¡£: https://www.mediawiki.org/wiki/API:Categorymembers
    """
    session = requests.Session()
    members = []
    cmcontinue = None

    while True:
        params = {
            "action": "query",
            "list": "categorymembers",
            "cmtitle": cat_title,
            "cmlimit": "500",
            "cmnamespace": namespaces,
            "format": "json",
        }
        if cmcontinue:
            params["cmcontinue"] = cmcontinue

        resp = session.get(API_URL, headers=HEADERS, params=params, timeout=20)
        resp.raise_for_status()
        data = resp.json()

        part = data.get("query", {}).get("categorymembers", [])
        members.extend(part)

        cont = data.get("continue", {})
        cmcontinue = cont.get("cmcontinue")
        if not cmcontinue:
            break

        time.sleep(0.1)

    return members


def bfs_collect_page_ids(root_categories, max_depth, target_count):
    """
    ä»è‹¥å¹²æ ¹åˆ†ç±»å¼€å§‹åš BFSï¼š
    - æ”¶é›† ns=0 çš„ pageid ä½œä¸ºæ¡ç›®
    - å‘ç° ns=14 çš„å­åˆ†ç±»æ—¶ï¼Œå¦‚æœè¿˜æ²¡è¶…è¿‡ max_depthï¼Œåˆ™åŠ å…¥é˜Ÿåˆ—
    """
    visited_cats = set()
    page_ids = set()

    queue = deque()
    for cat in root_categories:
        # ç¡®ä¿ä¼ çš„æ˜¯ Category:XXX æ ¼å¼
        if not cat.startswith("Category:"):
            cat = "Category:" + cat
        queue.append((cat, 0))

    while queue and len(page_ids) < target_count:
        cat_title, depth = queue.popleft()
        if cat_title in visited_cats:
            continue
        visited_cats.add(cat_title)

        print(f"[BFS] Category: {cat_title}, depth={depth}")
        try:
            members = fetch_category_members(cat_title, namespaces="0|14")
        except Exception as e:
            print(f"  !! error fetching category {cat_title}: {e}")
            continue

        new_pages = 0
        new_cats = 0

        for m in members:
            ns = m.get("ns")
            title = m.get("title", "")
            if ns == 0:  # æ™®é€šæ¡ç›®
                title = m.get("title")

                if title not in page_ids:
                    page_ids.add(title)
                    new_pages += 1
            elif ns == 14 and depth < max_depth:  # å­åˆ†ç±»
                # title æœ¬èº«å°±åƒ "Category:Something"
                if title not in visited_cats:
                    queue.append((title, depth + 1))
                    new_cats += 1

        print(f"  + pages: {new_pages}, + subcats queued: {new_cats}, total pages so far: {len(page_ids)}")

    return list(page_ids)

def fetch_page_texts_by_title(page_titles, min_words=30):
    session = requests.Session()
    texts = []

    BATCH = 20
    titles = list(page_titles)

    for i in range(0, len(titles), BATCH):
        batch = titles[i:i + BATCH]
        titles_str = "|".join(batch)

        params = {
            "action": "query",
            "format": "json",

            # âœ… å¿…é¡»
            "redirects": "1",

            # âœ… å…³é”®ï¼šç”¨ titles è€Œä¸æ˜¯ pageids
            "titles": titles_str,

            # âœ… extracts è®¾ç½®
            "prop": "extracts|pageprops",
            "explaintext": "1",
            "exintro": "1",     # ğŸ”¥ å…³é”®ä¿®å¤ç‚¹
            "exlimit": "max",

            # ç”¨æ¥è¯†åˆ« disambiguation
            "ppprop": "disambiguation",
        }

        try:
            resp = session.get(API_URL, headers=HEADERS, params=params, timeout=30)
            resp.raise_for_status()
        except Exception as e:
            print(f"HTTP error: {e}")
            continue

        pages = resp.json().get("query", {}).get("pages", {})

        batch_valid = 0
        for _, p in pages.items():
            title = p.get("title", "")
            text = p.get("extract", "") or ""

            if not text.strip():
                continue

            if "pageprops" in p and "disambiguation" in p["pageprops"]:
                continue

            if title.lower().startswith("list of "):
                continue

            cleaned = clean_text(text)
            if len(cleaned.split()) < min_words:
                continue

            texts.append(cleaned)
            batch_valid += 1

        print(
            f"fetched batch {i//BATCH + 1}, "
            f"pages: {len(batch)}, valid texts: {batch_valid}, "
            f"total texts: {len(texts)}"
        )

        time.sleep(0.1)

    return texts



def main(output_file="sports_wiki.jsonl"):
    print("=== BFS collecting page ids from entertainment categories ===")
    page_ids = bfs_collect_page_ids(ROOT_CATEGORIES, MAX_DEPTH, TARGET_ARTICLE_COUNT)
    print(f"Collected {len(page_ids)} unique page ids.")

    print("=== Fetching page texts ===")
    texts = fetch_page_texts_by_title(page_ids)
    print(f"Got {len(texts)} valid articles with enough words.")

    out_path = Path(output_file)
    with out_path.open("w", encoding="utf-8") as f:
        for t in texts:
            rec = {"text": t}
            f.write(json.dumps(rec, ensure_ascii=False) + "\n")

    print(f"Done. Saved {len(texts)} articles to {out_path}")


if __name__ == "__main__":
    main()
